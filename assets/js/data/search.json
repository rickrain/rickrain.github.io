[ { "title": "Developer Laptop Setup on Ubuntu", "url": "/posts/ubuntu-dev-setup/", "categories": "", "tags": "", "date": "2021-12-06 09:00:00 -0600", "snippet": "This blog is really a crutch for me to remember how I like to setup my Ubuntu desktop for development work. I recently got a new laptop and had to go through the process again of setting it up the way I like. I decided to capture some of those notes here to hopefully benefit others as well. In this blog, I don’t cover things like installing SDK’s, runtimes (.NET, Java, Docker), etc. The online docs for those are very clear and consistent. This blog covers just some base installation and configuration I like. From there, I install the SDK’s and tools that I need. Everything below should work for Ubuntu 18.04 and newer. My most recent installation and setup of Ubuntu Desktop was done using Ubuntu 21.10.Install UbuntuThe first thing I like to do is install a fresh image of Ubuntu on my laptop. To do this, I download the latest Ubuntu Desktop ISO and then create a bootable USB stick from which I can boot my laptop with to install Ubuntu. I always choose the option to erase the disk when installing. I also select the minimal installation option and add tools/apps I decide I need later.Install and configure GitBelow are the steps I use to setup Git. More details and options are available in the online documentation.# Install Gitsudo apt install git# Configure Gitgit config --global user.name &quot;Rick Rainey&quot;git config --global user.email &quot;redacted@redacted.com&quot;git config --global init.defaultBranch &#39;main&#39;git config --global credential.helper storeInstall and configure zshZsh is my terminal shell of choice and I really like the enhancements that Oh My Zsh offers. Below are the steps I use to install and configure this.# Install Zshsudo apt install zshThen, navigate to Oh My Zsh and click the install button. After installing and rebooting, check that the default shell is zsh. Run echo $SHELL and verify the output is /usr/bin/zsh.Next, configure the theme.sudo apt install fonts-powerlineEdit ~/.zshrc and set ZSH_THEME to agnoster. Finally, reboot the system.Install and configure xRDPxRDP is a package that enables remote desktop connections into an Ubuntu Desktop from a Windows machine. You might be wondering why I would install this. Well, I do this for a couple of reasons: I prefer to keep my Ubuntu laptop as lightweight as possible and only use it for software development. So, I only install the tools I need to do my software development work - no office applications, etc. I have a Windows laptop at my desk that, for most days of the week, is the machine my hands are primarily on. It is attached to a docking station which is attached to 2 larger monitors. This Windows laptop has all my office tools (Outlook, Office, Teams, etc) that I use for meetings and communication during the day. It also has everything I need to connect back to the corporate network at Microsoft for internal line-of-business apps. So, what I’ve found works well for me is to have my Ubuntu laptop sitting next to my Windows laptop that is on the docking station. Then, I remote desktop into my Ubuntu laptop from the Windows laptop (docked), and dedicate one of my large monitors to that remote desktop session. When I’m doing my development work, I simply shift my attention to the monitor dedicated to the remote desktop session into my Ubuntu laptop. When I need to jump on a Teams call or respond to email, I shift my attention to one of the other monitors.So, that is my motivation for installing xRDP on my Ubuntu laptop. Now, simply installing the xrdp package is not sufficient to achieving a great user experience. After you install xrdp, you will have to apply some additional configuration changes to address things like, authentication challenges for color profiles, authentication challenges to refresh system repositories, and fixing theme inconsistencies experienced from remote sessions, and more. Over the years, I have relied on the great blogs from c-nergy.be to resolve these issues. While these manual configuration changes are awesome, these folks did one better and have created an xRDP installation script to do all of this for you! In my recent Ubuntu laptop build I used it and it was simply amazing. In just about 2 mins, I had everything setup without having to do any additional configuration.To setup xRDP, just run the setup script as described in this blog:xRDP – Easy install xRDP on Ubuntu 18.04,20.04,21.04,21.10 (Script Version 1.3)Install and configure VS CodeThe docs for VS Code have guidance on how to install VS Code on Linux/Ubuntu. However, I just install it using the Ubuntu Software app.A few settings and extensions I like to have setup right away are shown below.Preferences -&amp;gt; Settingswindow.zoomLevel: 1editor.renderWhitepsace: &quot;all&quot;git.enableSmartCommit: trueTelemetry.telemetryLevel: offExtensionsThe extensions below are just the base set of extensions I like to have. Other extensions for languages such as C#, Java, Terraform, Docker, etc. are not listed here. GitLens Material Icon Theme Visual Studio KeymapJekyllMy blog runs on GitHub pages and therefore I use Jekyll locally to develop and test my blog posts.For reference, instructions to install Jekyll are here.# Install Ruby and pre-requisitessudo apt-get install ruby-full build-essential zlib1g-dev# Configure gem installation pathecho &#39;# Install Ruby Gems to ~/gems&#39; &amp;gt;&amp;gt; ~/.bashrcecho &#39;export GEM_HOME=&quot;$HOME/gems&quot;&#39; &amp;gt;&amp;gt; ~/.bashrcecho &#39;export PATH=&quot;$HOME/gems/bin:$PATH&quot;&#39; &amp;gt;&amp;gt; ~/.bashrcsource ~/.bashrc# Install Jekyll and Bundlersudo gem install jekyll bundlerNext, clone the blog repo from GitHub and run the following command in the root of the repo.# Update gem dependencies for blogsudo bundle updateTo test the site locally run the following command:bundle exec jekyll servePostmanAdditional installation options are described here, but the command below is all that’s needed to install.sudo snap install postmanDrawiosudo snap install drawioNet Toolssudo apt install net-tools" }, { "title": "CKAD Exam Tips", "url": "/posts/ckad-tips/", "categories": "", "tags": "", "date": "2021-10-12 10:00:00 -0500", "snippet": "I recently took and passed the CKAD Exam and now the proud owner of the CKAD Certification issued by The Linux Foundation. Like so many others who have accomplished this, I decided to share some tips that I found most helpful when I took my exam. This is not a complete list and there are many great tips out there that you may find more valuable. These were just some top-of-mind thoughts I captured shortly after taking the exam. If you are preparing for the CKAD exam, I hope these tips help you be successful.The CKAD exam will test more than just your kubernetes knowledge. It will also test your ability to complete tasks fast. Speed and efficiency is critical to getting past this exam. Spend 1 minute (or less) setting up the following aliases and environment variable. The ROI on these 3 is very high. # Aliases alias k=kubectl alias kn=&#39;kubectl config set-context --current --namespace &#39; # Env Variable export dro=&#39;--dry-run=client -o yaml&#39; Below are a few examples using these: # Set the namespace in the current context to dev3201 kn dev3201 # Create a pod definition file k run frontend --image=nginx $dro &amp;gt; frontend.yaml If you search the internet, you will find some pretty nifty aliases that people suggest using. But, if you spend more than a minute at the start of the exam setting up various aliases, then it’s my opinon you probably won’t get the ROI in time savings. Make sure to run the command that they give you at the beginning of every question to set your context. Watch this video from The Linux Foundation demonstrating this. Then, if the question instructs you to work in a particular namespace, use your kn alias to set your namespace so you can skip the namespace parameter in your commands. If it doesn’t specify a namespace, run kn default to insure you’re in the default namespace. Get used to using abbreviations for resources. For example, po instead of pods, deploy instead of deployment, svc instead of service, cm instead of configmap. You can find all the abbreviations by running k api-resources. Get used to using shorthand for command switches/parameters. For example, -A instead of --all-namespaces, -l instead of --selector. Get really good at editing in vi(m) or nano so you don’t lose valuable time on trivial editor stuff. When you take your practice tests ( you get 2 ), try to identify situation where you are not efficient in the editor. Then, get really good at being efficient in those situations. Get really good at using grep to find stuff in files, command help, etc. Switches that I used a lot were the -i, -A, and -B. See next tip for an example. When you need schema for a resource, use k explain with the --recursive switch and then pipe it into grep with the -B and -A switches. For example, suppose you’re asked to configure a rolling update strategy for a deployment and instructed to set the maximum surge property to a given value. You may not recall the exact schema you need to add to your deployment, but you recall there is a property that contains the word “surge” in it. You can find the property and schema it is part of using the following command: k explain deploy.spec --recursive | grep -i -B5 -A3 surge This command will return the following output. For the search string “surge”, it found the maxSurge property and returned 5 lines before and 3 lines after this instance. This is enough to give you the full schema you need as highlighted in the box. Now, you can cut/paste this into your deployment configuration and fill out the values. Don’t waste time formatting yaml. You don’t get points for making your code pretty! Just make sure it’s syntactically correct. Below are equivalent and valid pod definitions. Notice the differences at pod.spec.containers.volumeMounts and pod.spec.volumes. apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: myfrontend image: nginx volumeMounts: - mountPath: &quot;/var/www/html&quot; name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaim apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: myfrontend image: nginx volumeMounts: - mountPath: &quot;/var/www/html&quot; name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaim It is my belief that if you are comfortable with your yaml editing, then you can ignore tips relating to configuring the vi(m) editor. I did a time check at about an hour into the exam. Took a minute to explore the weight of the remaining questions. Then, prioritized the order of remaining questions based on their weight. Don’t rathole on a question. If you’re not making progress or stuck on a question for more than 5 mins, flag it and come back to it later. " }, { "title": "Add a custom device to the Azure IoT Suite Remote Monitoring solution", "url": "/2016/06/22/add-a-custom-device-to-the-azure-iot-suite-remote-monitoring-solution/", "categories": "", "tags": "", "date": "2016-06-22 16:06:04 -0500", "snippet": "Today, Azure IoT Suite provides two end-to-end IoT solutions; Predictive Maintenance and Remote Monitoring. These solutions can be deployed into your Azure subscription and running in just a few minutes, enabling you to get hands-on with all the services that comprise a typical IoT solution. If you have never seen the solutions in IoT Suite, I encourage you to check them out. They will accelerate your learnings on how the plethora of services in Azure can be used to implement an IoT solution that provides telemetry ingestion at massive scale, data storage, analytics, and presentation of data. I lean on these solutions a lot when talking to customers about IoT on Azure as well as the Azure IoT Reference Architecture. And best of all, these solutions are open sourced on GitHub so you can fork them and use them as a starting point to build your own IoT solutions.In this blog post, I’m going to be using the Remote Monitoring solution to show how you can register a custom device with IoT Hub to send real telemetry (temperature and humidity) data to IoT Hub. I’m not going to go deep into the solution itself. You can do that at your own convenience. This post is all about taking an Azure IoT Starter Kit and connecting it to the solution so you can explore the possibilities of devices in a working solution.The Microsoft Azure IoT Starter Kits provide a variety of boards, sensors, and controls that you can use to get started with IoT in Azure. The one I’m using for this post is the Adafruit Feather M0 Starter Kit shown here from the Azure IoT Starter Kits page. As you can see, it is packed full of gadgets and can take your mind far beyond the scenario I’ll discuss in this post. BTW, this is not an endorsement of any kind. It is the starter kit that was available to me. If you decide to purchase a kit, explore them all and pick the one that best meets your needs. They are all certified for Azure!The getting started page for the Adafruit Feather M0 WiFi starter kit is a good place to start and if you are persistent you will find everything you need to be successful. However, one of the reasons I chose to write this blog is to consolidate all the steps into a single document. I spent hours on what I thought would be a rather trivial process and made some mistakes along the way. Also, because the device manufacturer, the IDE, and the Azure platform span different organizations, the documentation tends to be a bit fragmented as you dive deeper into things. So, my goal here is to document a clear and concise path to connecting a custom device to the remote monitoring solution. Specifically, in this post I will show how to build a device that can transmit temperature and humidity data to Azure IoT Hub, configure the Arduino IDE used to compile and upload code to the device, deploy the remote monitoring solution from the Azure IoT Suite, register the device in IoT Hub, compile and upload the remote monitoring device code to the device, observe the telemetry data from the device in the remote monitoring solution.Build the deviceThe first step in this process was to build the device to connect to the remote monitoring solution. For this, you need the breadboard, some wires, the BME280 sensor (barometric pressure, temperature, and humidity), and the main Adafruit Feather M0 WiFi module. This was a lot of fun for me personally. The last time I did anything involving a breadboard was back in 19xx when I was in college. It was a rewarding experience back then and a lot of fun xx years later in an internet-of-things (IoT) world. I used this opportunity to expose my youngest son to circuitry and IoT. In fact, he built the device referenced in this section of the post.First up, install the Adafruit module and BME280 sensor on the board. Placement on the board is not important. You just need to give yourself some room to work.Wire up the power (3Vo) and ground (GND) as shown here.Next, wire up the BME280 sensor. I tried a couple of wiring configurations (I2C and SPI) and eventually settled on the Serial Peripheral Interface (SPI) wiring configuration. What is shown here is the clock (SCK), data out (SDO), data in (SDI), and chip select (CS) from the BME280 connected to pins 13, 12, 11, and 10 on the main module.Later, we will test this out, but first, we need to setup the development environment so we can program the device.Setup the Arduino IDETo program the device, you need the Arduino IDE. Specifically, for the Azure IoT Suite Remote Monitoring Solution, it is advised that you use version 1.6.8 or newer. For this post, I used the Windows version 1.6.9. Note: I also tested things out on the Mac version and it worked equally as well.After installing the Arduino IDE, run the IDE and select File -&amp;gt; Preferences. Set the Additional Boards Manager URLS to https://adafruit.github.io/arduino-board-index/package_adafruit_index.json and click OK. This is used by the IDE’s Board Manager to locate new/updated Adafruit boards.Next, using the Boards Manager, install the Arduino SAMD Boards and Adafruit SAMD Boards as shown here. To open the Boards Manager, select Tools -&amp;gt; Boards Manager.Close and re-open the Arduino Software IDE.If you are running on Windows, as I was while writing this post, then you will want to also install the Adafruit Board Drivers. A link to the installer is available here (near the middle of the page). I used the default settings to install the drivers on my machine.Finally, select Tools -&amp;gt; Board, and then select the Adafruit Feather M0 board as shown here.Using the Library Manager, install the Adafruit BME280, Adafruit Unified Sensor, and RTCZero libraries as shown here. To open the Library Manager, select Sketch -&amp;gt; Include Library -&amp;gt; Manage Libraries.There are two more libraries you need for this solution that you have to download and copy to your Arduino Library folder manually. Clone (or download) the Adafruit WINC1500 library. Clone (or download) the Azure IoT Hub for Arduino Library.Copy both libraries to the Documents\\Arduino\\libraries folder. The Arduino libraries folder now contains the five libraries installed above as shown here.Test the deviceAt this stage you are ready to test out some basic functions on the board, such as the blink sample sketch. To do this, plug the device into a USB port and wait about 30 seconds.In the Arduino Software IDE, select Tools -&amp;gt; Ports -&amp;gt; [port for the device] as shown here (your COM port may be different).Next, compile (CTRL + R) and then upload (CTRL + U) the blink sketch to the device. After a few seconds the red light on the device will start blinking on/off per the code in the sketch. Note: The device needs to be reset before you can upload a new sketch to it. To do this, press the reset button 2 times, reconfigure your port setting in the Arduino IDE, and then upload the sketch.Deploy the Azure IoT Suite Remote Monitoring SolutionTo deploy the Remote Monitoring Solution from the Azure IoT Suite, open a browser and navigate to https://azureiotsuite.com. Sign-in using the same credentials you use to sign-in to the Azure portal.Click the link to Create a new solution.Select the Remote Monitoring Solution.Enter a solution name, select a region, select your subscription you want the solution deployed in, and then click Create Solution.The solution takes about 10 minutes to deploy. After it does, launch the dashboard (ie: web app) for the solution by clicking Launch on the solution as shown here.Create a custom device Id and key in the remote monitoring solutionThe solutions in the Azure IoT Suite uses Azure IoT Hub for bi-directional communication with devices. When you add a device to your IoT Hub, you need to generate a Device ID and Device Key. These are unique to each device and are used by the device to authenticate to IoT Hub before it can send and receive messages. Note: By having a unique ID and key for each device, you are able to ensure that only devices that have been registered with IoT Hub can authenticate to it. This also enables you to revoke access for an individual device if you ever need to.In the dashboard of the Remote Monitoring solution, click the Add a Device button in the lower left corner of the screen.Next, click the Add New button in the panel labeled Custom Device.Next, select the option to define your own Device ID. I named my device Adafruit-01. Click the Check ID button to verify the device ID is available in your instance of Azure IoT Hub and then click Create.Copy the Device ID, Device Key, and the IoT Hub Hostname to notepad or another text editor so you can retrieve them later.Upload the remote monitoring sketch to the deviceIn this section, we will configure the remote monitoring solution code to use the SPI wiring configuration for the device and provide it the Device ID and Key it will need to authenticate to IoT Hub.Clone (or download) the device code from GitHub or clone (or download) my forked copy that has these changes already applied.Open the .\\remote_monitoring\\remote_monitoring.ino sketch using the Arduino IDE.In the remote_monitoring.ino file, include the AzureIoTHubClient.h and initialize an instance of AzureIoTHubClient. Also, set the SSID and Secret for the WIFI network you want the device to use.In the bme280.cpp file, change the code to initialize the BME280 sensor using the SPI wiring configuration. The original code assumed the I2C wiring configuration for the device.In the remote_monitoring.c file, add the Device ID, Device Key and IoT Hub Name that was created previously.It’s time to compile this thing and try it out! Press Ctrl-U to compile and upload the sketch to the device. Note: You will have to reset the device (see above) and subsequently the port in the Arduino IDE before uploading the sketch to the device.A successful build and upload to the device will result in output similar to what is shown here in the output window of the Arduino IDE.After about 20-30 seconds the device will connect to the WiFi network, initialize the BME280 censor, and then start transmitting data to your IoT Hub instance.Observe telemetry data sent to IoT HubIn the Arduino IDE, you can monitor the recordings from the device using the Serial Monitor as shown here. To open the Serial Monitor, select Tools -&amp;gt; Serial Monitor. Note: It is not necessary at this point to have the device connected to the USB port on your computer. As long as you can provide a power source and be in range of your WiFi network, the device can be placed anywhere and the telemetry data can be observed in the dashboard of the remote monitoring solution.In the Devices page of the remote monitoring solution, the status of the custom device will be set to Running as shown here.Go back to the Dashboard page and change the device to view setting to the custom device ID to see the telemetry data coming from the device. In the image below, I exhaled on the BME280 device and then touched it with my finger to change the temperature and humidity readings.SummarySo there you have it. An end-to-end remote monitoring IoT solution with a real device! In this post I showed you how I built a custom device to send temperature and humidity data to the remote monitoring solution from Azure IoT Suite. I walked you through the steps to setup the Arduino IDE so you can compile and upload sketches to the device. Then, I deployed the remote monitoring solution from the IoT Suite and showed how to create a unique Device ID and Key from the Devices page of the remote monitoring portal. We looked at the code changes needed to support the SPI wiring configuration I chose when building the device. And finally, put it all together by uploading the sketch to the device and observing the telemetry data in the remote monitoring solution dashboard.One final comment about the solution. After you have a physical device(s) connected to your IoT Hub, you may want to shut down the simulated devices to reduce unnecessary costs while exploring the possibilities of your custom device. The simulated devices are fantastic to get you started and to run data through the system, but there are costs associated with this.To shut down the simulator, perform the following steps: Sign-in to the Azure portal and open the web app blade titled [solution name] jobhost. Click on Settings -&amp;gt; WebJobs. Highlight the DeviceSimulator-WebJob and click on Stop.IoT in Azure is a fun and very exciting space. If you want to learn more about IoT and Azure, check out the learning path available here. Hopefully this post helped to accelerate some of your own learning so you can explore the possibilities of IoT on Microsoft Azure.Thanks for reading.Cheers!Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Deploy an Azure Resource Manager Template", "url": "/2016/03/21/deploy-an-azure-resource-manage-template/", "categories": "", "tags": "", "date": "2016-03-21 16:06:04 -0500", "snippet": "In my previous post I showed how to use Visual Studio 2015 to author a simple Azure Resource Manager (ARM) template from scratch. In this post, I will discuss a few options for deploying that template. Specifically, I will show how to deploy using Azure PowerShell deploy using Azure Command-line Interface (CLI) deploy using Visual Studio deploy using the Azure PortalIf you want to get hands-on with the techniques described in this post then use the solution I walked through in the last post. A copy of that solution is available on my GitHub account.Deploy using Azure PowerShellThis is the most obvious and preferred method if you or the person deploying your template is a Microsoft Windows user. All you need on your machine is PowerShell 3.0 or newer and a couple Azure “RM” PowerShell Cmdlets. For this post, I have the following versions installed. PowerShell v5.0.10586.122 AzureRM.Resources v1.05If you already have PowerShell and the Azure “RM” PowerShell cmdlets installed, then feel free to skip the next section. If you do not or are not sure then continue.Setup Azure PowerShellIf you don’t have Azure PowerShell installed, then you can install the latest from here.To see what version of PowerShell you have installed, open a PowerShell console prompt and type $PSVersoinTable.PSVersion.PS C:\\&amp;gt; $PSVersionTable.PSVersionMajor Minor Build Revision—– —– —– ——–5 0 10586 122To see what version of Azure “RM” PowerShell Modules (cmdlets) you have installed use Get-Module as shown below.PS C:\\&amp;gt; Get-Module -ListAvailable | Where-Object { $_.Name -like &quot;AzureRM*&quot; } | Select Name, Version Name Version---- -------AzureRM.ApiManagement 1.0.5 AzureRM.Automation 1.0.5 AzureRM.AzureStackAdmin 0.9.2 &amp;lt;... abbreviated ... &amp;gt;AzureRM.Resources 1.0.5 AzureRM.SiteRecovery 1.1.4 &amp;lt;... abbreviated ... &amp;gt;AzureRM.Websites 1.0.5You will also need to configure PowerShell’s execution policy on your machine to allow you to execute scripts. To do that, use the Set-ExecutionPolicy command as shown here.PS C:\\&amp;gt; Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Force Note: To set the execution policy you will need to be running your PowerShell command prompt (or PowerShell ISE) as an administrator. Setting the execution policy is a one-time step and does not need to be repeated.Deploy the ARM TemplateRecall from the last post that Visual Studio created a deployment script for me titled Deploy-AzureResourceGroup.ps1. It is located in the Scripts folder of the project as shown here.So, the easiest method to deploy the template is to simply just invoke this script to do the work. However, before you can invoke the script you must first authenticate to your Azure subscription.I’m going to use PowerShell ISE (powershell_ise.exe) for this and open Deploy-AzureResourceGroup.ps1. The upper window in PowerShell ISE is the script pane and the lower (blue) window is the console pane. In the console pane invoke the Login-AzureRmAccount command to authenticate to your Azure subscription. After successfully authenticating click the play button in the toolbar (or press F5) to run the script in the script pane as shown here.When you run this script you will be prompted for mandatory parameters of the script, in this case, the ResourceGroupLocation parameter for which I supplied the value “West US” (without quotes). You will also be prompted (in a dialog window) for the virtual machine admin password you want to use when connecting to the VM using RDP. Note: The admin password is a parameter that the template expects. The ResourceGroupLocation parameter is a parameter that the Deploy-AzureResourceGroup.ps1 script expects.After providing the necessary parameters the resources will proceed to be provisioned as shown below.After a few minutes the deployment will be completed. A successful deployment will look similar to what is shown below where the ProvisioningState is reported as Succeeded. Also notice that in the Outputs section the FQDN of the VM, which can be used to connect to the VM using RDP.Deploy using the Azure Command-Line Interface (CLI)The Azure CLI tools provide cross-platform support for managing your Azure resources. So, if you or the person deploying your template is running Max (OS X) or Linux then Azure CLI can be used to complete the task. The Azure CLI tools also are available on the Windows platform.If you already have the Azure CLI installed, then feel free to skip the next section. If you do not or are not sure then continue.Setup Azure CLIIf you don’t have the Azure CLI installed, then you can install it using the Windows Installer, OS X Installer or Linux Installer.To see what version of Azure CLI you have installed, open a command prompt, type azure, and press ENTER.Deploy the ARM TemplateThe deployment experience using Azure CLI is very similar to the deployment experience using PowerShell as described above. However, there are a few subtle differences that I will call out in this section.To deploy the template using Azure CLI, start by opening a command prompt (if you’re on windows). Or a terminal session on Mac or Linux.Just as before in the PowerShell scenario, you must authenticate to Azure first before you can deploy your template. To authenticate using Azure CLI, type azure login and press ENTER. The authentication experience will require you to open a browser to https://aks.ms/devicelogin and enter a code (provided by the CLI) as shown below. Afterwards, you will be able to authenticate to Azure using your normal Azure subscription credentials.Next, you must switch your CLI session to use Azure Resource Manager. At the time of this writing, it defaults to the Azure Service Management (ASM) mode. To switch to ARM mode, type azure config mode arm and press ENTER as shown here.Azure CLI does not prompt for missing/mandatory parameters like PowerShell does so you have to specify all your parameters when you invoke the command to start the deployment. You can provide all the parameters inline on the command line or using a parameters file. However, you cannot use a combination of these. It must be one or the other. Since we already have a parameters file in this scenario, we will go with the latter. However, the azuredeploy.parameters.json file does not provide the admin password for the VM that the template expects. So, before proceeding, update the azuredeploy.parameters.json file to include this parameter value as shown here.{ &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;parameters&quot;: { &quot;MyVMName&quot;: { &quot;value&quot;: &quot;myVM&quot; }, &quot;MyVMAdminUserName&quot;: { &quot;value&quot;: &quot;adminuser&quot; }, &quot;MyPublicIPDnsName&quot;: { &quot;value&quot;: &quot;rickrainey-demovm&quot; }, &quot;MyStorageType&quot;: { &quot;value&quot;: &quot;Standard_LRS&quot; }, &quot;MyVMAdminPassword&quot;: { &quot;value&quot;: &quot;P@ssword1&quot; } }}With that change in place, the parameters file provides all the parameters necessary to deploy the template using Azure CLI.To create the resource group called “ARMBasics” and deploy the template using the parameter values in the parameters file, type azure group create -n “ARMBasics” -l “West US” -f “&amp;lt;path to template file&amp;gt;\\azuredeploy.json” -e “&amp;lt;path to template parameters file&amp;gt;\\azuredeploy.parameters.json” and press ENTER as shown here. After a few minutes the resources described in the template file will be provisioned in the resource group. And just like in the PowerShell scenario, a successful deployment will show Succeeded for the ProvisioningState.Deploy using Visual StudioIf you used Visual Studio to author your template, then it is convenient to just deploy directly from Visual Studio. I do this when I am testing things out. However, once I am finished I prefer to just use the PowerShell and CLI methods as described above.When you deploy using Visual Studio, it simply invokes the same script, .Scripts\\Deploy-AzureResourceGroup.ps1, which was shown in the Deploy using PowerShell section. So, there’s nothing particularly magical that Visual Studio is doing for you here except giving you a nice UI experience to invoke the script and provide some parameter values. I’ll come back to this later in this section.To use Visual Studio to deploy the ARM template, open the “ARMBasics” solution created using Visual Studio in the last post. From the main menu, select Project &amp;gt; Deploy &amp;gt; New Deployment. In the dialog that opens, select &amp;lt;create new&amp;gt; for the Resource group field, which will cause another window to open where you can specify the name and location of your resource group as shown below. When finished click Create.At this stage, an empty resource group is created in the selected region using the given name and you are returned back to the previous dialog as shown below. The Deployment template and Template parameters file fields are defaulted to the only values available. However, in more complex templates, you may have multiple deployment templates and you most certainly would have multiple template parameter files to feed different parameter values to different deployment environments (ie: DevTest, Staging, Production, etc.).The dialog also gives you an opportunity to edit the template parameter values directly by pressing the Edit Parameters button. Doing so will open a new dialog where you can specify the values for each parameter of your template as shown here.After setting the parameter values to your liking, click the Save button to save the values. The parameter values you specify are stored in the .\\Templates\\azuredeploy.parameters.json file. Note: Generally, you don’t want to store passwords in your template parameters file which is why it was left null in the image above. If you do want to save your password to the parameters file, then type it in and make sure you check the Save passwords checkbox. Otherwise, Visual Studio will prompt you for the password again when you deploy the template.After setting your parameter values you are returned back to the previous dialog. Click the Deploy button to start the deployment. At this time, you will be prompted to enter the password if you didn’t do so in the Edit Parameters dialog shown above.The deployment progress is displayed in the Output window as shown below. If you don’t see it your environment, then from the main menu select View &amp;gt; Output. Coming back to the statement earlier in this section about Visual Studio just invoking the Deploy-AzureResourceGroup.ps1 script to deploy the template, this is evident in the Output window of the deployment as shown here.The rest of the deployment is shown in the Output window as the deployment progresses as shown here.The deployment will take a few minutes to fully complete.Deploy using the Azure portalYou can deploy your template using the Azure Portal. In fact, you can even edit your template directly in the portal. This is handy in situations where you want to test something out with just a browser. In this scenario, all you need is the template file. In the portal click +New and search for “Template deployment” as shown here.In the search results blade select Template deployment. In the Template deployment blade click Create. This will open the Custom deployment blade where you can edit the template, provide parameters, specify the resource group name, and location. Click on the Edit template option as shown here.In the Edit template blade replace the text with the text from the * .\\Scripts\\azuredeploy.json file and click Save as shown here.In the Custom deployment blade, click the Edit parameters option as shown here.In the Parameters blade specify the values for each of the parameters the deployment template expects and then click OK as shown here.In the Custom deployment blade set Resource group to +New and specify a name for the resource group. Next, select a location for the deployment, review the legal terms and click Create as shown here.After a few minutes the resources described in the template will be provisioned using the parameter values specified above and the Resource group blade in the portal will show all the resources in the group.SummaryThis post demonstrated techniques for deploying an ARM template. The first technique used Azure PowerShell to invoke the deployment script, Deploy-AzureResourceGroup.ps1, that Visual Studio created when the resource group project was created in the last post. Next, I showed how to use the Azure Command-Line Interface (CLI) tools to deploy the template file, azuredeploy.json, using parameter values from the template parameters file, azuredeploy.parameters.json. In the Azure CLI case, the deployment script is not used – just the template and template parameters file. Next, I showed how to deploy directly from Visual Studio and pointed out that doing so performs the exact same steps as shown in the Azure PowerShell case, but with some added UI features from Visual Studio to capture parameter values. Finally, I showed you how you can deploy a template using only your browser by leveraging the Template deployment feature in the Azure portal.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Author an Azure Resource Manager Template using Visual Studio", "url": "/2016/02/22/author-azure-resource-manager-template-using-visual-studio/", "categories": "", "tags": "", "date": "2016-02-22 15:06:04 -0600", "snippet": "Updated 3/19/2016Last month I wrote an introduction to Azure Resource Manager (ARM) that talked about how ARM is different from the traditional Azure service management API’s (ASM) for deploying Azure resources. In that post I discussed some benefits of ARM and mentioned tools you can use to author your own ARM templates. In this post I will discuss how to use Visual Studio to author a very simple ARM template from scratch. If you’re an IT Professional and think that because I just said “Visual Studio” that this won’t apply to you – please do not stop reading. I believe you will find that Visual Studio with the Azure Tools installed delivers the ultimate experience for authoring ARM templates.For this post I’m using the following software versions: Visual Studio 2015 w/Update 1 Azure Tools SDK v2.8.2Create an Azure Resource Group ProjectWhen you first create an Azure Resource Group Project in Visual Studio you are presented with a list of common templates you can use to start defining your desired infrastructure. For example, if you wanted a set of Virtual Machines that could be used to run a scalable web server workload, then you could start with the Windows Server Virtual Machines with Load Balancer template to describe the initial infrastructure resources such as the Virtual Network, Storage Account, Load Balancer, Availability Set, Network Interfaces, and Virtual Machines. If you have ever provisioned an environment like this on-premises or even in Azure using the old Azure Service Management (ASM) API’s, you will quickly appreciate the power a template like this delivers.While this template is extremely useful for what it can do, it’s probably not a good choice if you’re just starting to learn how to author ARM templates, which is what this post is about. So instead, I’m going to choose the Blank Template (shown below) and incrementally add the resources needed to describe a single Virtual Machine.I believe this approach makes it easier to understand the basic concepts of ARM templates. Afterwards, you will be able to parse your way through and edit any template such as the one I referenced above.Explore the Project StructureChoosing the Blank Template results in a project that contains everything you need to begin authoring and eventually deploy your template. As shown below, the project structure contains three folders; Scripts, Templates, and Tools.The Scripts folder contains the PowerShell script that is used to deploy the environment you describe in the project. This is a fully functional script that essentially requires no editing. It just works! However, if you have a need to modify the script to support some advanced deployment scenarios you can do so.The Templates folder contains the JSON files that describe the environment you want to create. The azuredeploy.json file is where you describe the resources you want in your environment, such as the storage account, virtual machine, and other required resources. The azuredeploy.parameters.json file is where you can provide parameter values that you want passed into the template during deployment, such as the type of storage account, size of the virtual machine, credentials to use to sign into the virtual machine, and so on.The Tools folder contains AzCopy.exe and is used in scenarios where your deployment template needs to copy files to a storage account used for deploying application and configuration files into a resource. For a virtual machine, this typically would be Desired State Configuration (DSC) and DSC resources that you want pushed into the virtual machine after the virtual machine is provisioned to do things like configure Active Directory or IIS Web Server roles in the virtual machine. The script in the Scripts folder uses AzCopy in the Tools folder to copy these kinds of artifacts to a storage account so that ARM can then copy them from storage into the virtual machine after the instance is fully provisioned.Resource Deployment TemplateSince I chose the Blank Template, the azuredeploy.json file contains a skeleton JSON document. Notice the parameters, variables, resources, and outputs objects. This is where the bulk of your authoring will occur as you start adding resources to the template.{ &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;parameters&quot;: { }, &quot;variables&quot;: { }, &quot;resources&quot;: [ ], &quot;outputs&quot;: { }}The parameters object is where you define parameters for your resources to be used during deployment of the template. For example, if you define a Storage Account resource in your template, then you may want to parameterize the type (Standard or Premium) of storage account.The variables object is where you define variables that are referenced in your resource definitions. Using the Storage Account as an example again, you should have a variable for the name of the storage account that you can reference throughout the template rather than hard-coding the same value throughout the template.The resources array is where you describe all the resources for your environment such as a Storage Account Resource, Virtual Network Resource, Virtual Machine Resource, and so on.The outputs object is where you can optionally return values from your deployment. For example, you could return the public IP address of a virtual machine after it has been provisioned.JSON Outline WindowWhen you open the azuredeploy.json file, the JSON Outline Window will open to the left of your editor window as shown here. If it is not open in your environment, you can open it manually by selecting View -&amp;gt; Other Windows -&amp;gt; JSON Outline.The JSON Outline window provides a visual representation of the parameters, variables, resources, and outputs objects defined in azuredeploy.json. This is extremely helpful as you add resources to the file and provides a way to easily navigate the JSON file rather than scrolling through the JSON editor. As you are about to see, it also provides a nice UI experience to add resources to your template.Add a Storage Account ResourceThe first resource I will add is a Storage Account resource. The reason I chose this is because it is small and is needed to store the VHD’s (virtual hard disks) of the virtual machine. To add a Storage Account resource, right-click on resources in the JSON Outline Windows and select Add Resource. In the Add Resource window that opens, click on the Storage Account resource, provide a Name for the storage account, and click Add as shown here.The Storage Account resource is added to the azuredeploy.json file in the resources section. Additionally, a parameter for the type of storage account was added, and a variable for the name of the storage account is added. Notice also that the JSON Outline window gets updated. You can click on the parameter, variable, or resource in the JSON Outline window to quickly navigate to that element in the JSON file.The name property uses the variables() function to reference the value of the variable named MyStorageName. If you click on MyStorageName in the variables section of the JSON Outline window you will see the value of the variable is the name you gave the resource when adding it to the template concatenated with a uniquely generated string as shown here.“MyStorageName”: “[concat(‘mystorage’, uniqueString(resourceGroup().id))]” The value of the MyStorageName variable is generated using three ARM template functions; concat(), uniqueString(), and resourceGroup(). The resourceGroup() function is a function that returns a few properties about the resource group during deployment, such as the id, location, and name of the resource group. The id property is a long path (string) that includes your Azure subscription ID and resource group name. The uniqueString() function takes the value from resourceGroup().id (a string) and generates a 64-bit hash of it to generate a string that is unique to the resource group. The reason the template does this is to avoid DNS name conflicts for the storage account. Storage Accounts are publicly addressable at a URL unique to the storage account. Therefore, the name of the storage account needs to be unique to avoid DNS name conflicts. Finally, the concat() function takes the results of the uniqueString() function and concatenates it with the constant string “mystorage” to generate a string that looks something like “mystoragetj2fcadfgy6jw”.The type property identifies the type of the resource which in this case is a Storage Account. The value for the type follows a naming convention of /. When this template is deployed, ARM will use this value to invoke the resource provider at “Microsoft.Storage” to create the resource type “storageAccounts”.The location property tells ARM where to provision this resource. Generally speaking, your resources will be deployed in the same location as the resource group they will be contained in. So, the template uses the resourceGroup() function to return the location property of the resource group. So, for example, if at deployment time you specify “West US” for the region of your resource group, then your resources will also be deployed in “West US” because they will all be using the resourceGroup() function (by default).The apiVersion property tells ARM which version of the resource provider to use. The resource provider version changes as new features and resource types are introduced. Hence the reason for needing to specify this value.The properties object is where resource-specific properties are set. For a Storage Account, the accountType is required by the resource provider to identify the type of storage account to provision. The template takes this value as a parameter and then uses the parameters() function to retrieve the value specified during deployment.Add a Virtual Network ResourceA Virtual Machine in Azure requires a Virtual Network for the instance to be deployed in. You typically would probably be referencing an existing virtual network. However, for this post we’ll create a new one so we can further explore the basics of ARM templates.Following the same process as we did to add a Storage Account resource, add a Virtual Network resource as shown here.The Virtual Network resource is added to the azuredeploy.json file in the resources section and some variables are also added that are referenced in the virtual network resource definition. Notice the type property is set to “Microsoft.Network/virtualNetworks”, which is telling ARM which resource provider and resource type to provision. Also, notice that the properties object has virtual network-specific properties defined, such as addressSpace (the address space of the virtual network) and a collection of subnets describing how to carve up the address space.Add a Virtual Machine ResourceNow that we have met the minimum resource requirements for a Virtual Machine (ie: Storage Account and Virtual Network), we can add a Virtual Machine resource to the template. Following the same process as before, add a Windows Virtual Machine resource as shown below. The dialog will prompt you to choose a Storage Account and Virtual Network (and subnet) for the Virtual Machine.The Virtual Machine resource is added to the azuredeploy.json file in the resources section. You will also see that a virtual Network Interface Card (NIC) resource is also added. Selecting the NIC resource in the JSON Outline Window will navigate you directly to the resource as shown here.Notice that the resource type “networkInterfaces” is a type known by the same resource provider responsible for the Virtual Network resource, which is at the namespace “Microsoft.Network”. Also notice that for this resource the dependsOn property has a reference to another resource, which is the Virtual Network resource. When this template is deployed, this tells ARM that before it can create the virtual NIC resource, it needs to first create the Virtual Network resource. The reason this dependency exists is because in the ipConfiguration properties, this NIC binds a resource (the VM – as you will see soon) to a subnet in the Virtual Network resource.Moving down to the Virtual Machine resource, notice the resource provider and resource type is “Microsoft.Compute/virtualMachines”. Also notice that this resource dependsOn the Storage Account resource (to store it’s VHD’s) and the virtual NIC resource (to bind it to the VNET) as shown here.The Virtual Machine resource is added to the azuredeploy.json file in the resources section. You will also see that a virtual Network Interface Card (NIC) resource is also added. Selecting the NIC resource in the JSON Outline Window will navigate you directly to the resource as shown here.Notice that the resource type “networkInterfaces” is a type known by the same resource provider responsible for the Virtual Network resource, which is at the namespace “Microsoft.Network”. Also notice that for this resource the dependsOn property has a reference to another resource, which is the Virtual Network resource. When this template is deployed, this tells ARM that before it can create the virtual NIC resource, it needs to first create the Virtual Network resource. The reason this dependency exists is because in the ipConfiguration properties, this NIC binds a resource (the VM – as you will see soon) to a subnet in the Virtual Network resource.Moving down to the Virtual Machine resource, notice the resource provider and resource type is “Microsoft.Compute/virtualMachines”. Also notice that this resource dependsOn the Storage Account resource (to store it’s VHD’s) and the virtual NIC resource (to bind it to the VNET) as shown here.Add a Public IP Address ResourceTechnically, we’ve added all the resources necessary to deploy a virtual machine using this template. If you want to connect to the virtual machine after this template is deployed, then you will have to configure a VPN connection to the virtual network so you can communicate to it. That’s actually a best practice for most production scenarios. However, when you’re just learning, sometimes we take shorter paths to our destination. For this post, I’ll add the Public IP Address resource which will enable you to connect to the virtual machine using Remote Desktop Protocol (RDP) over a public IP Address.Following the same process, add a Public IP Address resource as shown here.Just like before, the Public IP Address resource is added to the azuredeploy.json file in the resources section as shown below.As you may have expected, the resource type “publicIPAddresses” is a type known by the resource provider at namespace “Microsoft.Network”. Hopefully you are recognizing that a resource provider can (and usually does) support multiple resource types. You probably also expected the virtual NIC to be in the dependsOn section for this resource. After all, the Add Resource dialog indicated it was a required resource and had you select the NIC resource when you added it. So, why then is it not there?!?!? The answer is that the Public IP Address resource is added as a dependent resource for the virtual NIC as shown below.If you look down a little further in the NIC properties, you will see the Public IP Address resource referenced in the publicIPAddress property of the ipConfigurations object.Add some OutputsWhen you deploy a template you may want to output values from the deployment. This is optional. For a deployment such as this (a VM) you may want to output the fully qualified DNS name for the virtual machine so you can RDP into it. For the template described in this post the outputs section could look like this.&quot;outputs&quot;: { &quot;dnsName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;value&quot;: &quot;[reference(variables(&#39;MyPublicIPName&#39;)).dnsSettings.fqdn]&quot; }}For the output I’m using the reference() function to reference the Public IP Address resource that gets deployed. When this template is deployed, the reference() function can be used to output the fully qualified domain name from the dnsSettings property of the resource.SummaryIn this post I walked through the process of creating a new Azure Resource Group Project using the Blank Template. Next, I added resources individually to support a single virtual machine deployment. Along the way we looked at the components of a Resource Group Project, how ARM templates are structured, and how resource dependencies are defined. We also saw examples of some of the many ARM Template Functions that you can use when authoring templates and how to output information from a deployment when an ARM template is deployed. The completed solution created for this blog post is available on my GitHub account.Next time I’ll show a few ways you can deploy this ARM template.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "An Introduction to the Azure Resource Manager (ARM)", "url": "/2016/01/19/an-introduction-to-the-azure-resource-manager-arm/", "categories": "", "tags": "", "date": "2016-01-19 15:06:04 -0600", "snippet": "The Azure Resource Manager (ARM) is the service used to provision resources in your Azure subscription. It was first announced at Build 2014 when the new Azure portal (portal.azure.com) was announced and provides a new set of API’s that are used to provision resources. Prior to ARM, developers and IT professionals used the Azure Service Management API’s and the old portal (manage.windowsazure.com) to provision resources. Today, both portals and sets of API’s are supported. However, going forward you should be using ARM, the new API’s, and the new Azure portal to provision and manage your Azure resources. Note: At the time of this writing there are still a few resources that are not yet available in the new Azure portal and therefore must be managed in the old portal.To understand how ARM works it is essential that you have an understanding of what a resource and a resource group is in Azure. So, I’ll talk about these concepts first and then highlight some benefits ARM brings to the Azure platform. In the last section of this post I will discuss tools you can use to work with ARM.Resources and Resource GroupsA resource is something you provision in your Azure subscription and always belongs to a resource group. For example, a SQL Database, Web App, Storage Account, Redis Cache, Virtual Machine, IoT Hub, and HDInsight are all examples of a resource.A resource group is a logical grouping of resources that support a particular application or workload. For example, you may have a line-of-business (LOB) application that is comprised of a Web App, Storage Account, and a SQL Database. Another example could be an IaaS workload that is comprised of a pair of virtual machines contained in a virtual network with a MySQL database for data. In the latter example, a storage account is also required to store the virtual hard disks (VHD’s) for the virtual machines. A visualization of what a resource group would look like for each of these scenarios is shown here.The resources you include in a resource group generally should be those that have the same management lifecycle. Using the LOB App as an example, suppose you have a web development team that manages the Web App and the Storage Account. And suppose that a separate DBA team manages the SQL Database and the underlying server. In this scenario it might make sense for the SQL Database (and the server) to be in a separate resource group that that the DBA team manages. In which case, the resources for the LOB App would span multiple resource groups as shown here.Resource ProvidersThe resources in your resource groups are created and managed by resource providers. Each resource has a resource provider that knows how to manage and configure the resource. Architecturally, this is a big change in Azure because it means that new resource types can be developed and exposed through a single management pane (ie: ARM) rather than having resource specific API’s which is the case for the old service management API’s.Bring it all togetherSo far I’ve talked about ARM, the ARM API’s, Resource Providers, Resource Groups, and Resources. The Resource Groups and Resources exist in your Azure Subscription. ARM and the Resource Providers used to provision resources are an integral part of the Azure platform. After you authenticate to your Azure subscription, you can invoke the API’s of ARM. A loose architecture of how all this comes together is shown here.Benefits of Azure Resource ManagerThere are many benefits of ARM that the original Service Management API’s (and old portal) could not deliver. Some of the benefits that are top-of-mind for me are described below.Declaratively provision resourcesARM provides us a way to describe resources in a resource group using JSON documents. Using the LOB App resource group example from above, with ARM, I declare in a JSON document the three resources (Web App, Storage Account, SQL Database) that I want ARM to create in the resource group. In the JSON document I also describe the properties for each of the resources such as the type of storage account, size of the SQL Database, and settings for the Web App, to name just a few. The advantage here is that I am able to describe the environment I want and send that to ARM to make it happen.Smarter and faster provisioning of resourcesBefore ARM, you had to provision resources independently to support your application and it was up to you to figure dependencies between resources and accommodate for these in your deployment scripts. For example, to provision a virtual machine, you had to know to create the storage account first and then create the virtual machine because the virtual machine needs the storage account to store it’s VHD’s. ARM, on the other hand, is able to inspect the resources you are telling it to provision and figure out what the resource dependencies are and provision the resources in the order required to ensure all resources are provisioned successfully.ARM is also able to figure out when it can provision resources simultaneously. For example, to create a virtual machine requires that a virtual network and storage account exist first. However, ARM is able to figure out that there is no direct dependency between the storage account and the virtual network and therefore will provision those two resources simultaneously before provisioning the virtual machine. This means faster provisioning of all the resources described in the resource group.Resource Groups as a unit of managementBefore ARM, the relationship between resources (ie; a web app and a database) was something you had to manage yourself. This included trying to compute costs across multiple resources to determine the all-up costs for an application. In the ARM era, since the resource group containing the resources for your application are a single unit of deployment, the resource group also becomes a unit of management. This enables you to do things like determine costs for the entire resource group (and all he resources within), making accounting and chargebacks easier to manage.Before ARM, if you wanted to enable a user or group of users to manage the resources for a particular application, then you had to make them a co-administrator on your Azure Subscription. This meant that those users had full capability to add, delete, and modify any resource in the subscription, not just the resources for that application. With ARM, you are able to configure Role Based Access Control (RBAC) for resources and resource groups, enabling you to assign management permissions to only the resource group for only the users that need access to manage it. When those users sign-in to Azure they will be able to see the resource group you gave them access to but not the rest of the resources in your subscription. You can even assign RBAC permissions to individual resources if you needed to.Idempotent provisioning of resourcesBefore ARM, automating the provisioning of resources meant that you had to account for situations where some, but not all, of your resources would be successfully provisioned. Using the virtual machine example again, if your storage account was provisioned successfully but your virtual network failed to provision correctly, then you had to write compensating code to handle that situation or handle it manually, neither of which is ideal. With ARM, when you send it the JSON document describing your environment, ARM knows which resources already exist and which ones do not and will provision only the resources missing to complete the resource group.ToolsThe Azure portal is a great way to get started using Azure Resource Manager with nothing more than your browser. When you create resources using the Azure portal (portal.azure.com) you are effectively using ARM. This is fantastic and visually appealing. However, it is not scalable. Eventually you will need to write a deployment template that describes all the resources you want ARM to provision. And to be scalable you will need to automate the deployment. You could start with something from the Azure Quick start Templates but at some point you will likely need (or want) to build your deployment template from scratch. For this, Visual Studio and PowerShell are the two tools I strongly recommend and I explain why in this section.Visual StudioThe ultimate tool in my opinion is Visual Studio with the Azure Tools installed. When it is time to write your ARM deployment templates, you will find that Visual Studio and the Azure Tools delivers an amazing experience that includes JSON intellisense in the editor and a JSON outline tool to visualize your resources. Writing ARM deployment templates is not a trivial task. Just go take a look at some of the quick start samples on GitHub and you will see how big and complex these can be. By using Visual Studio and the Azure Tools, you will be able to create deployment templates from scratch in a matter of minutes.If you don’t have Visual Studio you can download the free version here. After you have Visual Studio installed, download and install the Azure Tools. A version of the Azure Tools SDK is available for VS 2013 and VS 2015. The Azure Tools provides an Azure Resource Group project template to get you started as shown below. In subsequent posts I’ll be demonstrating how to use this project template.PowerShellThe project template I mentioned in the previous section generates a PowerShell deployment script that can be used to send your deployment to ARM in an automated manner. The script uses the latest Azure “RM” Cmdlets that you can download and install from here. Most of the code in the deployment script handles uploading artifacts that ARM may need to deploy your environment. For example, if your deployment template describes a virtual machine that needs a DSC extension to add additional configuration to the VM after Azure has created it, then the DSC package (zip file) would need to be generated and uploaded to a storage account for ARM to use while it is provisioning the VM. But, if you scroll down past all that to the bottom of the script file you will see two commands in the script (shown below).The first command, New-AzureRmResourceGroup, simply creates the resource group using a resource group name that you provide.The second command, New-AzureRmResourceGroupDeployment, pushes your deployment template and parameters for the deployment template to ARM. After ARM receives the files, it starts provisioning the resources described in your deployment template.A workflow diagram of how these tools are used to deploy an environment using Azure Resource Manager is shown here.SummaryIn this post I introduced Azure Resource Manager and highlighted some important advantages it brings to the platform. Then I concluded by introducing a couple of tools you should consider using to author your deployment templates. In future posts I will show how to use the tools to build environments using Azure Resource Manager.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Extending Azure AD using the Graph API", "url": "/2015/03/27/extending-azure-ad-using-the-graph-api/", "categories": "", "tags": "", "date": "2015-03-27 16:06:04 -0500", "snippet": "The Azure Active Directory Graph API enables some interesting scenarios that you can implement in your applications by enabling you to query and manipulate directory objects in Azure AD. In the last post I presented you with some common scenarios available via the Azure AD Graph API and showed how you can implement them using the Azure Active Directory Graph Client Library. In this post, I’m going to introduce you to another scenario made possible using Azure AD Graph API and then take you on a journey through its implementation.Applications often have requirements for user data that is beyond what we typically would get from the Claims collection for an authenticated user. In such cases, it is very common to store additional user data, for example profile data, in a database and then use it as needed in the application. This is generally the best practice most developers follow. However, if your additional user data requirements are small and your application is protected by Azure AD, then the Azure AD Graph API gives you another technique you can use whereby you can extend the directory schema in Azure AD to include the data your application needs. This feature is called Azure AD Graph API Directory Schema Extensions and can be used to store and retrieve extension properties (ie: custom data) for a variety of object types in Azure AD.This is really cool, but it does have some limitations so don’t think this should be your go-to solution for all scenarios like this. However, given the right situation, it can be a nifty and cost-effective solution for your application. The limitations of this feature are as follows: An extension property must be either a String type or Binary type. The size of the data that can be stored in the extension property cannot exceed 256 bytes. The directory objects that can be extended using extension properties are User, Group, TenantDetail, Device, Application, and ServicePrincipal. A directory (or Azure AD tenant) can have up to 100 extension properties registered.A Sample ScenarioNow that we know what our limitations are let’s look at a scenario. Assume that I’m building a line-of-business application to manage parking passes for employees. In addition to the employee information I can get from Azure AD I also need some basic vehicle information about the employee’s vehicle so building security knows which vehicles are authorized to be in the parking lot or garage. If this is all I need, then using schema extensions provides a perfect alternative to taking a traditional database approach. Let’s see how this can be done using the Azure AD Graph API Directory Schema Extensions and the Azure AD Graph Client Library.Register a New Application in Azure ADFor this post I’m using the same application I registered in the previous post to demonstrate the directory schema extensions feature. So, rather than repeat those steps here, see the Register a New Application in Azure AD section of the previous post.Create a New Console Application Using Visual StudioI am also using the same console application I used in the previous post. So, I won’t be repeating the nuances of instantiating an instance of the ActiveDirectoryClient in the Azure AD Graph Client Library. I’m assuming that when you see adClient in the forthcoming code that you know how it was created. If not, then please go back and review the previous post.The only change I did make in this version of the application is that I updated the NuGet package for the Active Directory Graph Client Library. It wasn’t necessary for this post, I just updated to get the latest package. So, for the sample code in this post, I’m using these two NuGet packages. Active Directory Authentication Library (ADAL), v2.14.201151115 Azure Active Directory Graph Client Library, v2.06Perfect! Now with those disclaimers out of the way, let’s dive right into directory schema extensions.Define a Model for Vehicle InformationIn this sample scenario, I added a simple class to hold essential vehicle information that my application will need as shown here. Notice that I’m not using properties but instead just member variables. This helps to reduce the size of the object when it is serialized as binary and therefore intentional in light of limitation #2 above.public class VehicleInfo{ public int Year; public string Make; public string Model; public string Color;}You may be thinking that defining a complex type like this violates limitation #1 above. In fact, you can represent an instance of a complex type using String or Binary data and I’ll show you both. If you want to see some examples of simple String extension properties, then take a look at the blog from the Azure AD Graph Team referenced at the bottom of this post.Now, for instances of VehicleInfo to be available as a property for users in my directory, I need to first register anextension property with Azure AD.Register an Extension Property in Azure ADExtension properties are defined using the ExtensionProperty class from the Azure AD Graph Client Library. To register an extension property you need to create one and set just a few required properties. This is a one-time operation. The extension property for my vehicle information data is shown here.// Create the extension propertystring extPropertyName = &quot;VehInfo&quot;;ExtensionProperty extensionProperty = new ExtensionProperty(){ Name = extPropertyName, DataType = &quot;String&quot;, TargetObjects = { &quot;User&quot; }};The Name is whatever I want it to be. The DataType is set to String (for now). Remember, this can be either String or Binary (limitation #1 above). The TargetObjects I have set to User because it applies to only users in my directory (limitation #2 above).An ExtensionProperty is always registered for an Application , which means you must first get a reference to the application registered in Azure AD. If you remember from previous posts, Azure AD generates a unique Client ID for an application when you register it in Azure AD. This Client ID can be used to locate the application (via its AppId) using a simple LINQ query as shown here.// Get a reference to our applicationMicrosoft.Azure.ActiveDirectory.GraphClient.Application app = (Microsoft.Azure.ActiveDirectory.GraphClient.Application)adClient.Applications.Where(a =&amp;gt; a.AppId == ClientID) .ExecuteSingleAsync().Result;if (app == null){ throw new ApplicationException(&quot;Unable to get a reference to application in Azure AD.&quot;);}After you have a reference to the Application that was registered in Azure AD, you can register the extension for the application as shown here.// Register the extension propertyapp.ExtensionProperties.Add(extensionProperty);app.UpdateAsync();Task.WaitAll(); // Apply the change to Azure ADapp.GetContext().SaveChanges(); The Add method adds the extension property to the ExtensionProperties collection for the application. The UpdateAsync method updates the client side Application object while the SaveChanges method of the application’s context is used to actually persist the change to Azure AD.To show what this extension property looks like in Azure AD, I used Postman to call the Azure AD Graph API to get the extension property registered from the code above. The image here shows how it is actually represented in Azure AD.Notice on line 10 that the name of the extension is not just the name “VehInfo” that I specified earlier. The actual naming convention in Azure AD for an extension property is extension_[AppID]_[ExtName], where [AppID] is the Client ID assigned to your application when you register it using the Azure Management Portal as shown below. The [ExtName] is the name that was specified in the ExtensionProperty.Name above. This naming convention is generally tr ansparent to you if you are using the Azure AD Graph Client Library. However, if you are using the REST API then this naming convention is important to follow so I’m making this point for those who may choose to use the REST API instead of the client library.Now that Azure AD knows about the extension property for my application, let’s see how you can set the value of the property for a user.Lookup a Previously Registered ExtensionTo lookup a previously registered extension you can query the ExensionProperties collection of the application as shown here.// Locate the vehicle info property extensionIEnumerable&amp;lt;IExtensionProperty&amp;gt; appExtProperties = app.ExtensionProperties;IExtensionProperty vehInfoExtProperty = appExtProperties.Where( prop =&amp;gt; prop.Name == extPropertyName).FirstOrDefault();Unfortunately, at the time of writing this post using v2.06 of the client library, this code doesn’t always work. The ExtensionProperties collection is always empty if your application instance is not the same instance that registered the extension property. Fortunately, you can work around this by calling the ActiveDirectoryClient.GetAvailableExtensionPropertiesAsync method to query for the extension property. This approach requires some knowledge of how the extension property name is actually represented in Azure AD which I showed previously. So, with that understanding of the naming convention, I can query for the single extension property I’m looking for as shown here.// Locate the vehicle info property extension (workaround)string extPropLookupName = string.Format(&quot;extension_{0}_{1}&quot;, ClientID.Replace(&quot;-&quot;, &quot;&quot;), extPropertyName);IEnumerable&amp;lt;IExtensionProperty&amp;gt; allExtProperties = adClient.GetAvailableExtensionPropertiesAsync(false).Result;IExtensionProperty vehInfoExtProperty = allExtProperties.Where( extProp =&amp;gt; extProp.Name == extPropLookupName).FirstOrDefault();Set the Extension Property Value for a UserTo set the extension property for a user you need to get a User reference. I’m going to stick with the “John Doe” user I’ve been using throughout this blog series and invoke a LINQ query against the ActiveDirectoryClient.Users property to get a reference to John Doe’s directory object as shown below. Next, I am creating an instance of the VehicleInfo class I defined previously and then used the User.SetExtendedProperty method to set the value for my user. Recall that our extension property can be either a String or Binary type – complex types are not supported (limitation #1 from above). To get around this, I’m using the Json.NET, v6.08 NuGet package to serialize my vehicleInfo object to a JSON string. Finally, I invoked UpdateAsync to update the client side object and then SaveChange on the user’s context to save the change in Azure AD.// Set the extension property value for a uservar upn = &quot;johndoe@cloudalloc.com&quot;;User user = (User)adClient.Users.Where(u =&amp;amp;amp;amp;amp;gt; u.UserPrincipalName.Equals( upn, StringComparison.CurrentCultureIgnoreCase)).ExecuteSingleAsync().Result; if (user != null){ var vehicleInfo = new VehicleInfo() { Make = &quot;Ford&quot;, Model = &quot;F150&quot;, Color = &quot;Silver&quot;, Year = 2014 }; user.SetExtendedProperty(vehInfoExtProperty.Name, JsonConvert.SerializeObject(vehicleInfo)); user.UpdateAsync(); Task.WaitAll(); // Save the extended property value to Azure AD. user.GetContext().SaveChanges();}The code above results in John Doe’s directory object being updated with the extension property value as shown below . On line 41 you can see the extension property and the JSON representation of the object.Retrieve the Extension Property Value for a UserTo retrieve the value of an extension property for a user you can query the collection returned from the User.GetExtendedProperties method as shown here. And since I’m serializing my complex type to a JSON string when I store it, I am de-serializing it back into a VehicleInfo object when I get it back.// Retrieve the extension property value for a userVehicleInfo vehInfo = null;var userExtProperty = user.GetExtendedProperties().Where( extProp =&amp;gt; extProp.Key.Equals( vehInfoExtProperty.Name, StringComparison.CurrentCultureIgnoreCase)).FirstOrDefault();if (userExtProperty.Value != null){ vehInfo = JsonConvert.DeserializeObject&amp;lt;VehicleInfo&amp;gt;((string)userExtProperty.Value);}Remove the Extension Property Value for a UserRemoving an extension property for a user is simply a matter of setting the value to null as shown here.// Remove the extended property for a useruser.SetExtendedProperty(vehInfoExtProperty.Name, null);user.UpdateAsync();Task.WaitAll();user.GetContext().SaveChanges();The code above removes the extension property (line 41 in the image above) from John Doe’s directory object in Azure AD.Unregister an Extension Property in Azure ADIf your application no longer needs an extension property you should unregister/remove it to free the space for potentially other extension properties (limitation #4 above). Unregistering an extension property may be accomplished using the Remove method of the Application.ExtensionProperties collection as shown here. Note: When you unregister an extension property, it also removes it from any directory objects ( ie: users ) that you may have set the extension property for.// Unregister the extension propertyapp.ExtensionProperties.Remove(extensionProperty);app.UpdateAsync();Task.WaitAll(); // Apply the change to Azure ADapp.GetContext().SaveChanges(); Unfortunately, at the time of this writing using v2.06 of the client library, this method does not actually remove it from Azure AD. As a workaround, the REST API to unregister an extension property does work so until this is corrected that is the only way to do this. The REST API is included in the References section at the end of this post.Storing Extension Properties as Binary DataAs I mentioned at the beginning of this post, extension property data can also be stored as binary data. You must be mindful of the 256 byte limit just as you had to be for string data. This can be a bit tricky as the binary representation of a small data type like the one I’m using is significantly larger than its JSON representation. During tests of storing my VehicleInfo object as binary data, I was coming in at about 205 bytes which was much higher than the 70 bytes used to store it as a JSON string. Still, it’s an interesting use case so in this section I’ll show how to create a binary extension property and then store and retrieve the data.To demonstrate this I’m going to use the BinaryFormatter to serialize my object to binary. The BinaryFormatter requires that my class be marked with the Serializable attribute as I’ve shown below. Otherwise, my class remains unchanged.[Serializable]public class VehicleInfo{ public int Year; public string Make; public string Model; public string Color;}Register a Binary Extension Property in Azure ADTo register a binary extension property with Azure AD you need to create a new ExtensionProperty and set the DataType to Binary as shown in the code below. It is exactly the same as before with just two changes that are highlighted: the name of the property and the data type being set to Binary.// Create the extension propertystring extPropertyName = &quot;VehInfoAsBinary&quot;;ExtensionProperty extensionProperty = new ExtensionProperty(){ Name = extPropertyName, DataType = &quot;Binary&quot;, TargetObjects = { &quot;User&quot; }}; // Get a reference to our application.Microsoft.Azure.ActiveDirectory.GraphClient.Application app = (Microsoft.Azure.ActiveDirectory.GraphClient.Application)adClient.Applications.Where( a =&amp;gt; a.AppId == ClientID).ExecuteSingleAsync().Result;if (app == null){ throw new ApplicationException(&quot;Unable to get a reference to application in Azure AD.&quot;);} // Register the extension propertyapp.ExtensionProperties.Add(extensionProperty);app.UpdateAsync();Task.WaitAll(); // Apply the change to Azure ADapp.GetContext().SaveChanges();The code above results in my directory now having two extension properties registered as shown below.Set the Binary Extension Property Value for a UserAn example of how this data can be set using the BinaryFormatter is shown below. As you can see, it is essentially the same code with just a few changes highlighted to serialize the data to binary.// Set the extension property value for a uservar upn = &quot;johndoe@cloudalloc.com&quot;;User user = (User)adClient.Users.Where(u =&amp;amp;amp;gt; u.UserPrincipalName.Equals( upn, StringComparison.CurrentCultureIgnoreCase)).ExecuteSingleAsync().Result; if (user != null){ var vehicleInfo = new VehicleInfo() { Make = &quot;Ford&quot;, Model = &quot;F150&quot;, Color = &quot;Silver&quot;, Year = 2014 }; BinaryFormatter binaryFormatter = new BinaryFormatter(); using (MemoryStream memoryStream = new MemoryStream()) { binaryFormatter.Serialize(memoryStream, vehicleInfo); user.SetExtendedProperty(vehInfoExtProperty.Name, memoryStream.GetBuffer()); user.UpdateAsync(); Task.WaitAll(); // Save the extended property value to Azure AD. user.GetContext().SaveChanges(); }}The code above results in John Doe’s directory object being updated with the extension property value as shown below. On line 41 is a property indicating that the extension property is a Binary extension and line 42 shows the actual value of the extension property (in binary).Retrieve the Binary Extension Property Value for a UserThe code below is an example of how you could read the binary extension property data and de-serialize it back into a VehicleInfo object. Again, the code that is different has been highlighted.// Retrieve the extension property value for a userVehicleInfo vehInfo = null;var userExtProperty = user.GetExtendedProperties().Where( extProp =&amp;gt; extProp.Key.Equals( vehInfoExtProperty.Name, StringComparison.CurrentCultureIgnoreCase)).FirstOrDefault();if (userExtProperty.Value != null){ using (MemoryStream memoryStream = new MemoryStream((byte[])userExtProperty.Value)) { vehInfo = (VehicleInfo)binaryFormatter.Deserialize(memoryStream); }}SummaryIn this pos t I showed you how you can used Azure AD Graph API Directory Schema Extensions to extend the schema in Azure AD. I discussed constraints that you must accept when using this feature and then showed you how you can use the Azure AD Graph Client Library to register extension properties, store, retrieve data using the property, and then remove the extension property. I showed how you can store complex types as a JSON string and also how it can be stored as binary data.For additional information on this feature I encourage you to look at the Azure AD Graph API Directory Schema Extensions documentation referenced in the References section below. Also, take a look at the post from the Azure AD Graph Team where they demonstrate using extension properties for simple string types.References Azure AD Graph API Directory Schema Extensions Azure AD Graph Team BlogPlease enable JavaScript to view the comments powered by Disqus." }, { "title": "Introducing the Azure AD Graph API", "url": "/2015/02/21/introducing-the-azure-ad-graph-api/", "categories": "", "tags": "", "date": "2015-02-21 15:06:04 -0600", "snippet": "At the end of the last post I closed by mentioning how the Azure AD Graph API and the IsMemberOf function could be used to determine a user’s membership in Azure AD Groups. However, as you saw in the last post, the group claims feature recently added to Azure AD made that task extremely simple without needing to use the Graph API. Still, there are many application scenarios where the Graph API is very useful and so it is the purpose of this post to formally introduce you to the Azure AD Graph API and introduce some handy client libraries you can use to access the Graph API.Introduction to the Azure Active Directory Graph APIAzure Active Directory provides a Graph API for every tenant that can be used to programmatically access the directory. So, if you have an Azure Subscription then the Azure AD Graph API is already there for you to use. Using the Graph API, you can do things such as query the directory to discover users, groups, and relationships between users. You can also use the Graph API to make changes to the directory such as adding, deleting, and updating users. In other words, the Graph API gives you CRUD capabilities when accessing the graph. And while my simple introduction is in the context of users and groups, don’t assume those as limitations for the Graph API. It is far more reaching than this and can be used to access virtually any entity in the directory.Like many other services in Azure, the Graph API is a REST API so programming against it is easy for any web developer using any language. For .NET developers, you could use the Microsoft Http Client Libraries to construct your REST calls to the Graph API. When available, I prefer to use client libraries that take care of invoking the REST API’s for me and Microsoft provides the .NET Azure AD Graph Client Library that does exactly that.The Graph API endpoint for your directory will be https://graph.windows.net/[YOUR TENANT ID]. If you have a custom domain configured for your directory then you could use that in place of your tenant ID. For example, either of the following endpoints are valid for my directory with a custom domain of cloudalloc.com. https://graph.windows.net/b3ebae33-54c5-4232-9693-01c76c13ee76 https://graph.windows.net/cloudalloc.comYou can find your Graph API endpoint using the Azure Management portal by clicking on the VIEW ENDPOINTS button at the bottom of the APPLICATIONS page for your directory as shown here.This will open a window showing all the application endpoints for your directory as shown below. Note: Even if you have a custom domain configured, this screen will show your endpoints using your tenant ID. But, you can exchange the tenant ID for your custom domain when accessing these endpoints in your code as you will see me do later in this post.At this point, we have a basic understanding of what the Graph API could be used for, where to access it, and some libraries that can be used to build applications using the Graph API. In the remainder of this post I will demonstrate using the .NET Azure AD Graph API Library and ADAL to implement some scenarios that are made possible via the graph.Develop a Client Application for the Azure AD Graph APIFor this post, I’m going to break away from the sample and scenarios of previous posts in this series and create a new application so you can see all the steps from beginning to end in one post. So, let’s get started.My application is going to be a very simple console application. Doesn’t get any simpler than that! The application is going to access the Graph API and perform a few simple operations.Register a New Application in Azure ADThe first step is to register an application in Azure AD. This is done in the APPLICATIONS page of your directory in the Azure Management Portal. Click on the ADD button at the bottom of the page to proceed through the new application wizard.The first page of the wizard as shown in Figure 3 needs a name for my application and the type of application it is. For this application, even though it is a console application, I am choosing the Web Application and/or Web API type. The reason I’m doing this is because I want my application to authenticate using application credentials rather than prompting the user running the application to authenticate with his/her username and password.Since I chose the Web Application/Web API application type, the next page in the wizard needs a valid sign-in URL and application ID URI. Since this is not really a web application any values will work as long as they are valid URL’s and URI’s as shown here.That completes the application registration part. Now I need to make a couple of configuration changes which I’ll do in the next section.Configure the Application to Access the Azure AD GraphThe configuration changes I need to make can be done in the CONFIGURE page for the application I registered using the Azure Management Portal. Shortly I’ll be getting into the code of the application and when I do there will be two pieces of information that I’m going to need. The first is the CLIENT ID which Azure AD generated for me when the application was registered. The other is a KEY (or secret) that I need to generate for my application so it can access the graph. Both of these are shown below. Pay attention to the message in the portal when you generate your key. As soon as you save the configuration change you will be able to see the key and copy it. After that, you will never be able to see it again so be sure to copy it before leaving the page. Otherwise, you will have to re-generate a new key. Also, notice that you can create multiple keys. This is to support key rollover scenarios. For example, as your key approaches expiration you can create a new key, update and deploy your code with the new key, and then remove the old key.A little further down in the permissions to other applications section, I am adding an Application Permission indicating this application can Read and write directory data as shown below. This will allow my application to query the directory and make changes to it such as adding new users. NOTE: It has been my experience that changes to permissions (application or delegated as shown above) generally take about 5 minutes to take effect. So for example, let’s assume I forgot to add the application permission above, built my application, and then realized my application didn’t have the required permissions to read the graph when I ran it. It’s a simple fix to come back here and add the permission. Just be aware that there is this delay. Otherwise, you may do like I did and start applying other unnecessary changes to try and get it working. Just be patient.Now, with these configuration changes saved, I’m ready to transition into the coding of this application.Create a new Console Application using Visual StudioAfter creating my new Console Application in Visual Studio, the first thing I want to do is bring in the client libraries that I’ll be using to access the graph in my directory. Both are available as NuGet packages and are as follows: Active Directory Authentication Library (ADAL), v2.14.201151115 Azure Active Directory Graph Client Library, v2.05Add Configuration SettingsThe first thing I’m going to do is bring in some settings from Azure AD and configuration from the application I just registered in my directory. I’ll need this information for authenticating to Azure AD and accessing the graph.// This is the URL the application will authenticate at.const string authString = &quot;https://login.windows.net/cloudalloc.com&quot;; // These are the credentials the application will present during authentication// and were retrieved from the Azure Management Portal.// *** Don&#39;t even try to use these - they have been deleted.const string clientID = &quot;07e33f8a-166f-4d79-a46d-f96dccf08b76&quot;;const string clientSecret = &quot;s9e6moGcFsRPP0rENcniLIi3q1hgmpGCUJUrpSIenvY=&quot;; // The Azure AD Graph API is the &quot;resource&quot; we&#39;re going to request access to.const string resAzureGraphAPI = &quot;https://graph.windows.net&quot;; // The Azure AD Graph API for my directory is available at this URL.const string serviceRootURL = &quot;https://graph.windows.net/cloudalloc.com&quot;;The settings in this code may look familiar. For example, the authString, resAzureGraphAPI and the serviceRootURL are simply from the App Endpoints dialog in Figure 2 above. The clientID and clientSecret were generated when registering the application as referenced in Figure 5 above and it is these two settings that will be used to generate a ClientCredential for authenticating to Azure AD later in this post. And while I’m talking about keys understand that it is not a best practice to store keys like this in code. I am showing it here out of convenience for this blog post. I suggest you be mindful of best practices for deploying passwords and other sensitive data to ASP.NET and Azure Websites for your own applications.Instantiate ActiveDirectoryClient from the Azure AD Graph Client LibraryThe Azure Active Directory Graph Client Library provides a class called the ActiveDirectoryClient that is used to access all the properties available to you in the graph, such as users, groups, contacts, the applications you have registered, and much more. As shown below, there are several properties hanging off of it to work with the various entities in your directory. The list of methods is brief and you will notice the IsMemberOfAsync that I talked about in previous posts as an alternative way to determine if a user is in a particular group.So, let’s begin this journey by looking at what it takes to instantiate an instance of this class. The constructor is shown below and I’m sure you will agree this it has a signature that is not your typical method signature. In particular, notice the 2nd parameter that I’ve highlighted. What you are required to provide here is a method that will acquire a token from Azure AD. Simple, right? Actually it is but at first glance this is a little intimidating.public ActiveDirectoryClient(Uri serviceRoot, Func&amp;lt;Task&amp;lt;string&amp;gt;&amp;gt; accessTokenGetter, IEnumerable&amp;lt;CustomTypeMapping&amp;gt; customTypeMappings = null);Here is how I am instantiating an instance of ActiveDirectoryClient in my application.// Instantiate an instance of ActiveDirectoryClient.Uri serviceRoot = new Uri(serviceRootURL);ActiveDirectoryClient adClient = new ActiveDirectoryClient( serviceRoot, async () =&amp;gt; await GetAppTokenAsync());The first parameter is pretty straight forward where serviceRoot is just the Graph API endpoint for my directory.The second parameter is using a lamda expression to wire up the function delegate to another method in my class called GetAppTokenAsync which I’ll get to shortly.I am intentionally leaving out the 3rd parameter as it is beyond the scope of what I want to do in this post and as you can see from the method signature, this is ok; it will just be initialized to null.The GetAppTokenAsync method is in my code and is tasked to do whatever is necessary to get an access token from Azure AD. This is where the Azure Active Directory Authentication Library (ADAL) comes into the picture. As you can see, it really is simple. I’m creating an authentication context for my directory, creating a credential to use to authenticate with, authenticating with Azure AD and getting an access token, and then returning the token which is just a long cryptic string.private static async Task&amp;lt;string&amp;gt; GetAppTokenAsync(){ // Instantiate an AuthenticationContext for my directory (see authString above). AuthenticationContext authenticationContext = new AuthenticationContext(authString, false); // Create a ClientCredential that will be used for authentication. // This is where the Client ID and Key/Secret from the Azure Management Portal is used. ClientCredential clientCred = new ClientCredential(clientID, clientSecret); // Acquire an access token from Azure AD to access the Azure AD Graph (the resource) // using the Client ID and Key/Secret as credentials. AuthenticationResult authenticationResult = await authenticationContext.AcquireTokenAsync(resAzureGraphAPI, clientCred); // Return the access token. return authenticationResult.AccessToken;}Now, for my simple application, since I’m using the Client ID and Key/Secret to create a credential for authenticating and acquiring an access token from Azure AD, I will not be prompted to authenticate as was the case in earlier posts in this series. If I wanted to access the graph using the identity of the user running my application, then I would have configured the application with a Delegated Permission to access the directory instead of an Application Permission as I showed earlier. In doing so, I would also use a different overloaded version of the AcquireTokenAsync method to perform the authentication and retrieve the access token.The point I want to be clear on here is that what I’m showing you is just one of many ways to acquire an access token from Azure AD. As an exercise, I encourage you to take a look at the many overloaded AcquireTokenAsync methods to get an idea of what is possible.Now that I’m able to connect to the graph I will show how we you perform a few operations on the graph.Lookup a User by their User Principal Name (UPN)I’ll start with a simple query of the directory. In this series I’ve used a fictitious user named John Doe to demonstrate various concepts. Below is how you could retrieve the User object for John Doe using his user principal name (UPN).// Look up a user in the directory by their UPN.var upn = &quot;johndoe@cloudalloc.com&quot;;var userLookupTask = adClient.Users.Where( user =&amp;gt; user.UserPrincipalName.Equals( upn, StringComparison.CurrentCultureIgnoreCase)).ExecuteSingleAsync(); User userJohnDoe = (User)await userLookupTask; // Show John Doe&#39;s NameConsole.WriteLine(userJohnDoe.DisplayName);Add a New User to the DirectoryTo add a new user to the directory you need to instantiate an instance of the User class and set some required properties as shown in the code below. There are many more properties that you can set for the user, but these are the minimum requirements for adding a user. The AddUserAsync method will write the user object into the directory.// Create a new user object.var newUser = new User(){ // Required settings DisplayName = &quot;Jay Hamlin&quot;, UserPrincipalName = &quot;jayhamlin@cloudalloc.com&quot;, PasswordProfile = new PasswordProfile() { Password = &quot;H@ckMeNow!&quot;, ForceChangePasswordNextLogin = false }, MailNickname = &quot;JayHamlin&quot;, AccountEnabled = true, // Some (not all) optional settings GivenName = &quot;Jay&quot;, Surname = &quot;Hamlin&quot;, JobTitle = &quot;Programmer&quot;, Department = &quot;Development&quot;, City = &quot;Dallas&quot;, State = &quot;TX&quot;, Mobile = &quot;214-123-1234&quot;,}; // Add the user to the directoryadClient.Users.AddUserAsync(newUser).Wait();Assign a User’s ManagerThe Manager property is defined in the base class DirectoryObject that User derives from. So, when setting the Manager property you need to cast the user back to a DirectoryObject as I’ve shown here. Also, notice that when I update the graph I do so by calling UpdateAsync on the user who is being assigned as the manager and not the user whose Manager property is being set.// Make John Doe this user&#39;s ManagernewUser.Manager = (DirectoryObject)userJohnDoe;userJohnDoe.UpdateAsync().Wait();Retrieve a List of Direct Reports for a UserIn the previous step I made Jay Hamlin’s manager John Doe. You can see all of John Doe’s direct reports as shown in the following code.// Get a list of John Doe&#39;s Direct ReportsIUserFetcher userFetcher = (IUserFetcher)userJohnDoe;var directReportsTask = userFetcher.DirectReports.ExecuteAsync();var directReports = await directReportsTask;do{ foreach (IDirectoryObject dirObj in directReports.CurrentPage.ToList()) { var directReport = (User)dirObj; Console.WriteLine(directReport.DisplayName); } } while (directReports.MorePagesAvailable);SummaryIn this post I introduced you to the Azure AD Graph API and laid the groundwork for how you can leverage the graph in your own applications. I showed how to add and configure an application in the Azure Management Portal to use the graph and then I introduced you to the Azure AD Graph Client Library. Next, I provided sample code to connect to the graph and perform a few basic operations.I barely scratched the surface of possibilities that the Azure AD Graph API presents. The Azure AD team has some excellent samples on GitHub that show many more scenarios for using the Graph API and the Graph Client Library in console and web applications. In addition to the samples, the Azure AD Graph Team has some nice blogs for you to learn more about the Graph API and the Azure AD Graph Client Library. Links to these resources are below in the References section. So, I encourage you to go explore the graph of possibilities.References Azure Active Directory Graph Team Blog Console Application Graph API Example Web Application Graph API Example Common QueriesPlease enable JavaScript to view the comments powered by Disqus." }, { "title": "Azure Redis Cache", "url": "/2015/02/19/azure-redis-cache/", "categories": "", "tags": "", "date": "2015-02-19 15:06:04 -0600", "snippet": "The Azure Redis Cache is a fully managed dedicated Redis cache that can be used to increase the performance of your cloud applications. It is offered in three tiers: Basic, Standard, and Premium with each tier offering various features and capacity you can choose from.The Azure Redis Cache offers far more capabilities than traditional key/value caches of past cache offerings. In addition to supporting key/value constructs extremely well, Redis Cache brings to the table features that today’s high performing cloud applications demand such as: Native support for complex structures like hashes, lists, sets, and ordered sets Transaction support for multiple operations against the cache Updating cache values without having to retrieve the item from the cache Keys with limited time-to-live Pub/Sub Messaging PatternsWith performance and features such as this it is no wonder why the Azure Redis Cache is the recommended cache for cloud applications running in Azure. Microsoft still supports some of the cache offerings of the past but the messaging is clear that Azure Redis Cache is the winner going forward.In this post I will demonstrate how you can use the Azure Redis Cache in your application with an emphasis on some of the features unique to Redis.Create a Redis CacheIn the Azure portal, search for and select the Redis Cache in the Azure Marketplace. In the New Redis Cache blade, provide a unique DNS name. Next, select the pricing tier (see Figure 1 above for tier choices, the resource group your cache will belong to, and the location. You should strive to select a location that is in the same region as the services that will use the cache for best performance. The DNS name for your cache will be {cache name}.redis.cache.windows.net.After the cache is created you will need two pieces of information from the Redis Cache blade to start using the cache in your code: the host name and an access key. You can access both of these in the Redis Cache blade as shown here.Create a client application to use the Redis cacheThe code I’m going to show in this section is running in a simple console application. This is intentional to keep the focus on using the Azure Redis Cache. However, the code can be used in any application of your choice after installing the Redis Client Library from Stack Exchange.Install Redis Client LibraryThe Redis Client Library I’m using is from Stack Exchange and is available as a NuGet package as shown here.Before I transition into the code to add/retrieve items to and from the cache, I want to mention another NuGet package that may be useful if you’re developing web applications, which is the RedisSessionStateProvider. This will provide easy support for caching ASP.NET sessions if you need it. However, it is not a topic I’m going to cover in this post.Connect to the Azure Redis CachThe ConnectionMultiplexer provides a Connect method used to connect to the cache. It expects a string containing the Redis Endpoint Uri and Access Key. The Endpoint Uri and Access Key were referenced previously using the Azure Management portal and I’ve stored these settings in the app.config file for my application as shown here.&amp;lt;configuration&amp;gt; &amp;lt;startup&amp;gt; &amp;lt;supportedRuntime version=&quot;v4.0&quot; sku=&quot;.NETFramework,Version=v4.5&quot; /&amp;gt; &amp;lt;/startup&amp;gt; &amp;lt;appSettings&amp;gt; &amp;lt;add key=&quot;RedisEndpoint&quot; value=&quot;cloudalloc.redis.cache.windows.net&quot;/&amp;gt; &amp;lt;add key=&quot;RedisAccessKey&quot; value=&quot;UydEP&amp;lt;…abbreviated…&amp;gt;IQR2c=&quot;/&amp;gt; &amp;lt;/appSettings&amp;gt;&amp;lt;/configuration&amp;gt;In my code, I can retrieve these settings to connect to the Redis Cache as shown here.// Connect to the Redis Cache EndpointredisConnection = ConnectionMultiplexer.Connect( string.Format(&quot;{0},ssl=true,password={1}&quot;, redisEndpoint, redisAccessKey)); // Get a reference to the cachevar cache = redisConnection.GetDatabase();Add items to the Azure Redis CacheNow that I have my connection established, I can start adding items to the cache. In the code below, I’m demonstrating four things: Adding simple key/value pairs to the cache. Incrementing an integer value in the cache. Adding a hash to the cache. Adding a list to the cache.// Add simple data to the cacheawait cache.StringSetAsync(&quot;BlogTitle&quot;, &quot;Azure Redis Cache&quot;);await cache.StringSetAsync(&quot;NumberOfHits&quot;, 500); // Update cache data (without retrieving it first)await cache.StringIncrementAsync(&quot;NumberOfHits&quot;, 125); // Add a Hash to the cacheHashEntry[] userCreds = new HashEntry[]{ new HashEntry(&quot;username&quot;, &quot;johndoe&quot;), new HashEntry(&quot;password&quot;, &quot;p@ssword&quot;)};await cache.HashSetAsync(&quot;User&quot;, userCreds); // Remove items from the cache (if they exist)string keyBasicTier = &quot;RedisCacheBasicTiers&quot;;await cache.KeyDeleteAsync(keyBasicTier); // Add a List to the cacheawait cache.ListLeftPushAsync(keyBasicTier, &quot;B0&quot;);RedisValue[] basicTiers1to5 = new RedisValue[] { &quot;B1&quot;, &quot;B2&quot;, &quot;B3&quot;, &quot;B4&quot;, &quot;B5&quot;};await cache.ListRightPushAsync(keyBasicTier, basicTiers1to5);await cache.ListRightPushAsync(keyBasicTier, &quot;B6&quot;);Retrieve items from the Azure Redis CacheIn this section I’ll demonstrate retrieving, and in one case removing, items in the cache. Note that in this section I’m also not using Asychronous API’s as I did in the previous section. This is just to demonstrate that the Redis Cache Client Library supports both synchronous and asynchronous API’s.// Retrieve simple data from cacheConsole.WriteLine(&quot;BlogTitle: {0}&quot;, cache.StringGet(&quot;BlogTitle&quot;));Console.WriteLine(&quot;NumberOfHits: {0}&quot;, cache.StringGet(&quot;NumberOfHits&quot;));Console.WriteLine(); // Retrieve Hash data from cacheConsole.WriteLine(&quot;John Doe Credentials:&quot;);Console.WriteLine(&quot;\\tUsername: {0}&quot;, cache.HashGet(&quot;User&quot;, &quot;username&quot;));Console.WriteLine(&quot;\\tPassword: {0}&quot;, cache.HashGet(&quot;User&quot;, &quot;password&quot;));Console.WriteLine(); // Retrieve List data from cachevar basicTierOptionsCount = cache.ListRange(keyBasicTier).Count();Console.WriteLine(&quot;Number of Basic Tiers: {0}&quot;, basicTierOptionsCount); RedisValue item = cache.ListLeftPop(keyBasicTier);while (item != RedisValue.Null){ Console.Write(&quot;{0} &quot;, item); item = cache.ListLeftPop(keyBasicTier);}Console.WriteLine();Observing the application outputThe output of the application is show here.SummaryIn this post I introduced some basic principles of the Azure Redis Cache. I showed how to create a cache using the Azure Management portal and then how to use the .NET Client Library from Stack Exchange to add, update, remove, and retrieve items in the cache.ReferencesAzure Redis Cache FAQPlease enable JavaScript to view the comments powered by Disqus." }, { "title": "Using Group Claims in Azure Active Directory", "url": "/2015/02/13/using-group-claims-in-azure-active-directory/", "categories": "", "tags": "", "date": "2015-02-13 15:06:04 -0600", "snippet": "In the post titled Developing Native Client Apps for Azure AD I showed how you can use the Active Directory Authentication Library (ADAL) to build a native client application that calls the CloudAlloc.WebAPI introduced in the post titled Building Web Apps for Azure AD. As part of that post, I demonstrated how the Web API could be exposed to other applications using ouath2Permissions that I defined in the manifest which enabled me to assign permissions to my native client application as shown here.Being able to assign read and write permissions in this way provided an easy and useful way to control an application’s ability to invoke various operations using the claims that were issued by Azure Active Directory for an authenticated user. However, the level of granularity for which authorization decisions can be made in the application code is pretty coarse with this approach. In my example, it was the simple read and write permissions I defined and if the native client application was configured to allow write permissions, then any authenticated user would be able to invoke such operations. While that may be sufficient for some applications, others may require additional information about the user before making such authorization decisions. For example, you may want to restrict write operations to users in a specific security group. As you can see below, the claims provided by Azure Active Directory for our authenticated user John Doe just don’t provide that level of detail about the user (yet). Other than some various identifier claims about John Doe, we’re currently limited to the values in the scope claim to make authorization decisions.A common need for many applications is the ability to make authorization decisions based on a user’s membership in a specific security group (or groups). In this post I’m going to show how you can take advantage of some recently released features of Azure Active Directory to do just that.Using group claims to drive authorization decisionsVery recently, the Azure Active Directory team announced the preview release of two new features: group claims and application roles. Before this, you had to use the Azure AD Graph API to determine a user’s membership in a group. Now, you can just look for it in the Claims collection for the authenticated user provided you enable the group claims feature for your application in Azure AD. Let’s see how this would work for the application we’ve been talking about and only allow users to invoke the POST method if they are in the “Dev/Test” security group.Enable group claims for the cloudalloc.webapi applicationEnabling the group claims feature currently requires that you update the manifest for the application in Azure AD. This is exactly the same process I showed you in the last post where I set the oauth2Permissions for the application.In the Azure Management portal I’ll begin by going to the CONFIGURE page for the CloudAlloc.WebAPI application in Azure AD. At the bottom of the page is a MANAGE MANIFEST button where I can select to download the manifest for the CloudAlloc.WebAPI application as shown here.The application manifest is just a JSON file that you can edit with the simplest of editors (ie: notepad.exe). By the way, if you’re curious what the GUID in the filename is about when you download the manifest, it is the Client ID that was assigned to the application when it was registered in Azure AD. Scrolling down the manifest file I will find the groupMembershipClaims property which will be set to null as shown here.I’m going to change this value to SecurityGroup and then save the changes as shown here.Your choices for setting the groupMembershipClaims property are null (the default), All or SecurityGroup. If you choose SecurityGroup you will get group claims in the JWT token for just security groups the user is a member of. If you choose All you will get group claims in the JWT token for security groups and distribution lists the user is a member of.All that remains now is to upload the modified manifest file which I did using the MANAGE MANIFEST button.With that change in place, I will now start getting group claims in the token for users of my application. As mentioned above, I’m interested in checking for the user’s existence in the Dev/Test security group. So, before I show you the code changes needed to do this, let’s review my security groups and where our John Doe user lands in these groups.Locate the Objec ID for the Dev/Test Security GroupWhen Azure AD adds applicable group claims to the token it issues for my users, the value for the group claim will be the Object ID of the security group and not the name of the security group. Remember, every entity in Azure AD has a unique Object ID associated with it. You may think it would be more intuitive to refer to the group by name instead of the Object ID. This is true. However, a group’s name can be changed in the directory so it is not a reliable identifier for the group. The Object ID will never change as long as the group exists. So, in this case, I need to find the Object ID for the Dev/Test security group. This can be found in the CONFIGURE page of for my Dev/Test security group as shown here.Now that I have the Object ID for the Dev/Test security group I am ready to make the necessary code changes.Update the POST method to look for the Dev/Test Group ClaimThe only code change needed is to look for a groups claim with the value of “244728b5-8b9e-4e2f-8703-9853366cd431” (yours will be different) in the authenticated user’s claims collection as shown here.// POST api/valuespublic HttpResponseMessage Post([FromBody]string value){ // Look for the scope claim containing the value &#39;Read_Write_CloudAlloc_WebAPI&#39; var principal = ClaimsPrincipal.Current; Claim writeValuesClaim = principal.Claims.FirstOrDefault( c =&amp;gt; c.Type == &quot;http://schemas.microsoft.com/identity/claims/scope&quot; &amp;amp;&amp;amp; c.Value.Contains(&quot;Read_Write_CloudAlloc_WebAPI&quot;)); // Look for the groups claim for the &#39;Dev/Test&#39; group. const string devTestGroup = &quot;244728b5-8b9e-4e2f-8703-9853366cd431&quot;; Claim groupDevTestClaim = principal.Claims.FirstOrDefault( c =&amp;gt; c.Type == &quot;groups&quot; &amp;amp;&amp;amp; c.Value.Equals(devTestGroup, StringComparison.CurrentCultureIgnoreCase)); // If the app has write permissions and the user is in the Dev/Test group... if ((null != writeValuesClaim) &amp;amp;&amp;amp; (null != groupDevTestClaim)) { // // Code to add the resource goes here. // return Request.CreateResponse(HttpStatusCode.Created); } else { return Request.CreateErrorResponse( HttpStatusCode.Unauthorized, &quot;You don&#39;t have permissions to write values.&quot;); }}Note: As I pointed out in my last post, it is a good idea to organize code like this into a custom AuthorizeAttribute class or some other common class so that your application logic and authorization logic are not mixed together. I’m intentionally leaving that as an exercise to you the reader though.That’s all there is to it! Now, let’s observe the results of these changes by examining the claims collection for the authenticated user (John Doe).Examining the Group ClaimsI ran the application under the debugger and set a breakpoint in the POST method so I could show you the new group claims in the claims collection. As shown below, you can see there are two groups claims present for John Doe.The value for the first groups claim should look familiar – it is the Object ID for the Dev/Test group I discussed earlier. The second one happens to be a Developer security group in my directory for which John Doe is a member. The figure here shows the membership for both security groups.Notice that John Doe is explicitly identified as a member of the Developer security group but not the Dev/Test group. Yet, in the token issued to John Doe, a group claim was present for both of these security groups. This is because the group claims feature in Azure AD is transitive. So, since John Doe is a member of the Developer group and the Developer group is a member of the Dev/Test group, John Doe is a member of both security groups.As you can imagine, in a real world production environment a user is likely to be a member of many different security groups. When you enable the group claims feature the tokens issued for users will contain the group claims for all of these groups which could greatly increase the size of the token. Therefore, there are limits on the number of group claims that Azure AD will place in a token as follows: JWT Tokens: Up to 200 group claims SAML Tokens: Up to 150 group claimsCurrently there is not a way to filter the group claims that Azure AD places in a token. So, if users in your directory could potentially exceed these limits you will need a different solution.Working with the Azure AD Group Claims LimitThe Azure AD Graph API is a REST API that Azure Active Directory makes available for each tenant. With it you can programmatically access the directory and query about users, groups, contacts, tenant details and more. In addition to querying the directory, the Azure AD Graph API can be used to create, update and even delete entities in the directory.For the scenario mentioned above, the Azure AD Graph API could be used to look up the security groups a user belongs to or to check if a user is in a specific security group. The latter is particularly applicable in our scenario here and could be achieved using the IsMemberOf function. To execute this query you need only the Object ID of the user and the Object ID of the security group. The query will execute in Azure AD (server side) and simply return true or false. Prior to the release of the group claims feature in Azure AD discussed in this post, this was the only way to check group membership for a user. In situations where the number of groups a user is in could potentially exceed the limits above, it is still available to you. In the next post I’ll talk more about the Azure AD Graph API.SummaryIn this post I showed how to take advantage of the new Azure Active Directory group claims feature and apply it to our scenario to determine a user’s membership in a particular security group. I also discussed the group claims limit as it applies to JWT and SAML tokens issued by Azure AD and how you can fall back on the Azure AD Graph API’s IsMemberOf function to query the directory and determine the same.In the next post, I will introduce you to the Azure AD Graph API and some handy client libraries you can use to access the graph in Azure AD.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Developing Native Client Apps for Azure Active Directory", "url": "/2014/12/08/developing-native-client-apps-for-azure-active-directory/", "categories": "", "tags": "", "date": "2014-12-08 15:06:04 -0600", "snippet": "In my post titled Building Web Apps for Azure AD, I discussed developing two types of applications protected by Azure Active Directory: web applications and web API’s. In that post I approached these applications from the perspective of a developer using Visual Studio 2013 and the project templates provided for creating these types of applications. The automation delivered by Visual Studio and the Azure SDK when creating these applications resulted in the two applications being registered in my Azure Active Directory as shown here.Testing the CloudAlloc.Website was a matter of simply pressing F5 in Visual Studio to launch a browser and access the application. Recall this application is protected using WS-Federation which redirects unauthenticated users to a sign-in page. After successfully authenticating, the user is redirected back to the CloudAlloc.Website URL.The CloudAlloc.WebAPI is protected using OAuth 2.0 and expects a valid token in the authorization header when its endpoint is accessed. In this post I am going to continue where I left off with the Web API application and discuss how you can develop a native client to acquire an access token that can be used to access the Web API.A native client application is one that is installed on a user’s computer or device. Examples of such an application include, but is not limited to, WinForms, WPF, Windows Store, Windows Phone, and iOS applications. And while these applications may not necessarily run in Azure as compared to the web application and web API applications discussed in the previous post, native client applications can be protected by Azure Active Directory and access other applications registered with Azure Active Directory.Using Claims in the Web API to drive applicaion behaviorBefore getting into the development of a native client, I want to first point out two changes I made to the Web API to facilitate the topics discussed in this post.For the Get method, I changed the default implementation provided by the Visual Studio project template to extract the claims from the authenticated user and look for a scope claim with a value of Read_CloudAlloc_WebAPI or Read_Write_CloudAlloc_WebAPI. If the scope claim is found and contains one of these values, then I return HTTP 200 (OK). Otherwise, I return an HTTP 401 (Unauthorized).You may be wondering where I got the scope claim and value. Azure Active Directory will add this claim and value to the claim set for authenticated users provided that my Web API has been configured to support this claim and my client (native client) has been granted the permissions to include the claim. I’ll come back to this shortly. For now, just understand that with this code, the Web API now requires that this claim and value be present for the user to be able to read these values.The updated Get method implementation is shown here.// GET api/valuespublic HttpResponseMessage Get(){ // Look for the scope claim containing the // value &#39;Read_CloudAlloc_WebAPI&#39; or &#39;Read_Write_CloudAlloc_WebAPI&#39; var principal = ClaimsPrincipal.Current; Claim readValuesClaim = principal.Claims.FirstOrDefault( c =&amp;gt; c.Type == &quot;http://schemas.microsoft.com/identity/claims/scope&quot; &amp;amp;&amp;amp; (c.Value.Contains(&quot;Read_CloudAlloc_WebAPI&quot;) || (c.Value.Contains(&quot;Read_Write_CloudAlloc_WebAPI&quot;)))); if (null != readValuesClaim) { // // Code to retrieve values to include in the response goes here. // return Request.CreateResponse(HttpStatusCode.OK); } else { return Request.CreateErrorResponse( HttpStatusCode.Unauthorized, &quot;You don&#39;t have permissions to read values.&quot;); }}For the Post method, I made similar changes but instead look for the scope claim containing the value Read_Write_CloudAlloc_WebAPI. If the scope claim is found and contains this value, then I return HTTP 201 (Created) to simulate the value being added to the server. Otherwise, I return an HTTP 401 (Unauthorized). The updated Post method implementation is shown here.// POST api/valuespublic HttpResponseMessage Post([FromBody]string value){ // Look for the scope claim containing the value &#39;Read_Write_CloudAlloc_WebAPI&#39; var principal = ClaimsPrincipal.Current; Claim writeValuesClaim = principal.Claims.FirstOrDefault( c =&amp;gt; c.Type == &quot;http://schemas.microsoft.com/identity/claims/scope&quot; &amp;amp;&amp;amp; c.Value.Contains(&quot;Read_Write_CloudAlloc_WebAPI&quot;)); if (null != writeValuesClaim) { // // Code to add the resource goes here. // return Request.CreateResponse(HttpStatusCode.Created); } else { return Request.CreateErrorResponse( HttpStatusCode.Unauthorized, &quot;You don&#39;t have permissions to write values.&quot;); }} Note: Generally it is recommended to organize code that tests for the presence of claims with this level of granularity into a custom AuthorizeAttribute class. By doing so you isolate the authorization code into one place, reducing the potential for error and improving maintainability of the code. However, in the interest of keeping the focus in this section on how claims can be used to make authorization decisions, I’m including it directly in the method for readability.Exposing the Web API to other applicationsTo make the Web API accessible to other applications in Azure Active Directory, permissions must be defined for the Web API that can be assigned to other applications. The Azure Management portal does not provide a user interface to do this. However, it does offer you access to the application’s manifest where you can configure settings for an application, including settings that the portal doesn’t provide a user interface for. Here are the steps to get to an application’s manifest using the Azure Management portal. Go to the APPLICATIONS page for the Azure Active Directory the application is registered in. Click on the name of the application whose manifest you’re interested in. For this post, it is the CloudAlloc.WebAPI application. At the bottom of the screen click on the MANAGE MANIFEST button and select the option to Download Manifest and save it to your local computer, as shown here.The manifest is a JSON formatted file that contains the Azure Active Directory configuration for an application registered in Azure Active Directory. If you scroll through the file you will see the settings you see on the CONFIGURE page for an application and much more. The configuration setting we’re interested in for this post is the oauth2Permissions setting. By default, this is an empty array, as shown here.For this application (the CloudAlloc.WebAPI), I am adding two permissions to this array; one that allows read access to the API and one that allows read/write access to the API. Notice the value that I indicated for each permission. This is where the values I’m looking for in the scope claim originate from in the Get and Post methods above.&quot;oauth2Permissions&quot;: [ { &quot;adminConsentDescription&quot;: &quot;Allow read access to the CloudAlloc WebAPI on behalf of the signed-in user&quot;, &quot;adminConsentDisplayName&quot;: &quot;Read access to CloudAlloc WebAPI&quot;, &quot;id&quot;: &quot;1835E3A9-C857-4F33-A357-751E620E558D&quot;, &quot;isEnabled&quot;: true, &quot;origin&quot;: &quot;Application&quot;, &quot;type&quot;: &quot;User&quot;, &quot;userConsentDescription&quot;: &quot;Allow read access to the CloudAlloc WebAPI on your behalf&quot;, &quot;userConsentDisplayName&quot;: &quot;Read access to CloudAlloc WebAPI&quot;, &quot;value&quot;: &quot;Read_CloudAlloc_WebAPI&quot; }, { &quot;adminConsentDescription&quot;: &quot;Allow read-write access to the CloudAlloc WebAPI on behalf of the signed-in user&quot;, &quot;adminConsentDisplayName&quot;: &quot;Read-Write access to CloudAlloc WebAPI&quot;, &quot;id&quot;: &quot;87A81936-E765-4678-B6DB-8E12197AAA7D&quot;, &quot;isEnabled&quot;: true, &quot;origin&quot;: &quot;Application&quot;, &quot;type&quot;: &quot;User&quot;, &quot;userConsentDescription&quot;: &quot;Allow read-write access to the CloudAlloc WebAPI on your behalf&quot;, &quot;userConsentDisplayName&quot;: &quot;Read-Write access to CloudAlloc WebAPI&quot;, &quot;value&quot;: &quot;Read_Write_CloudAlloc_WebAPI&quot; }],The schema for the oauth2Permissions can be found in the MSDN documentation for adding, updating, and removing an application in Azure Active Directory.After making this update to the manifest file all that is left is to upload it to Azure by clicking the MANAGE MANIFEST button and selecting the Upload Manifest option.Now the Web API application can be accessed from other applications using these permissions.Add a native client application to Azure Active DirectoryRegardless of the type of native client application you plan to build, the first step is to register it in Azure Active Directory. This is easily done using the Azure Management portal by clicking the ADD button in the APPLICATIONS page of the directory and selecting the option to add an application my organization is developing. This will open a wizard where you can specify the name and type of application as I’ve done here.The next page of the wizard will prompt you for the redirect URI associated with the native client application. This is a URI (not a URL), so any value will work as long as it is a valid URI and is unique to your directory as shown here.That is all that is required to register the application with Azure Active Directory. The application will appear in the APPLICATIONS page of the directory as a Native client application as shown here.Configure permissions to access the Web APISince this native client application is going to be accessing the CloudAlloc.WebAPI I need to configure the permissions for it. In the CONFIGURE page of the native client application is where permissions to other applications can be set. And since my Web API application is now accessible as a result of the manifest changes I made earlier, I can easily configure the native client with the permissions I want to give it. For now, I’m going to start by assigning the Read access to CloudAlloc WebAPI permission as shown here.As users of the native client application authenticate to Azure Active Directory, they will now get the scope claim with a value of Read_CloudAlloc_WebAPI in addition to other claims issued by Azure Active Directory.Develop the native client applicationFor this post I’m going to build a simple console application for the native client. However, the code in this application would work the same for other types.Recall that my Web API is protected by Azure AD and expects a security token when accessed from client applications. So, I need a way to authenticate users and acquire a security token (JWT) that can be used when calling the API. Thankfully, Microsoft provides a super-handy library called the Active Directory Authentication Library (ADAL) that simplifies this. Since my client application is a C# console application, I’ll be using ADAL for .NET (version 2.x). But, there are other flavors available such as ADAL for JavaScript, ADAL for Node.js, ADAL for Java, ADAL for Android, and ADAL for iOS and OSX making native client application development for Azure AD a breeze for the most popular languages and platforms used in the enterprise.To construct the HTTP requests to my Web API, I will use the Microsoft HTTP Client Libraries (version 2.x).Define settings used by the native client applicationThe native client application needs the Client ID and Redirect URI of the application registered with Azure AD. The Client ID is generated automatically when the application is registered and the Redirect URI is the URI entered in the new application wizard earlier.// Native client application settingsprivate string clientID = &quot;386985f0-b940-4713-a8e5-f7a49d39f368&quot;;private Uri redirectUri = new Uri(&quot;http://cloudalloc.webapi.client&quot;);Both of these settings can be found in the properties section of the CONFIGURE page for the native client application in the Azure Management portal as shown here.When a native client needs to get a token from Azure Active Directory, it needs to specify the resource it wants a token for. In this scenario the client application wants access to the Web API so the APP ID URI for the Web API is used as the resource name. After it has the token it also needs to know the URL where the resource can be accessed, in this case the address of the Web API.// Resource settings this application wants to accessprivate string resource = &quot;https://cloudalloc.com/CloudAlloc.WebAPI&quot;;private Uri WebAPIUri = new Uri(&quot;https://localhost:44313&quot;);Both of these settings can be found in the single sign-on section of the CONFIGURE page for the Web API application in the Azure Management portal as shown here. Note: The reason the reply URL is a localhost address is simply because in the previous post when these applications were created and registered in Azure Active Directory, I didn’t publish them to Azure Websites. If I had, then this reply URL would have a *.azurewebsites.net address instead. To see how publishing a web application and/or web API to Azure applies to applications registered in Azure Active Directory see my post on Authenticating with Organizational Accounts and Azure Active Directory.For ADAL to authenticate the user and acquire a token for the resource, an AuthenticationContext must first be instantiated by passing in the URL of your tenant in Azure Active Directory. The URL is of the form *https://login.windows.net/* where can be either the GUID assigned to your tenant in Azure AD or the domain if you have a custom domain configured.// Session to Azure ADprivate const string authority = &quot;https://login.windows.net/cloudalloc.com&quot;;private AuthenticationContext authContext = new AuthenticationContext(authority);The AuthenticationContext is like a connection to your Azure Active Directory and is ultimately used to acquire tokens from your directory.Call the Web API to get valuesThe code to issue an HTTP GET request to the Web API essentially breaks down to three tasks: Authenticate the user and get a token from Azure Active Directory. Here is where the AuthenticationContext instance is used to call the AcquireToken method. If the user is not already authenticated, then the ADAL library will launch a sign-in page for the user to sign-in with. After successfully authenticating, a security tokento access the resource using the native client application is issued. An HttpClient instance is instantiated and configured to include the security token in the Authorization header for HTTP calls to the resource (Web API). An HTTP GET is issued to the resource’s URL to retrieve the values.The full source code for the native client to call the Get method on the Web API is shown here.public async Task&amp;lt;bool&amp;gt; ReadValues(){ // Authenticate the user and get a token from Azure AD AuthenticationResult authResult = authContext.AcquireToken(resource, clientID, redirectUri); // Create an HTTP client and add the token to the Authorization header HttpClient httpClient = new HttpClient(); httpClient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue( authResult.AccessTokenType, authResult.AccessToken); // Call the Web API to get the values Uri requestURI = new Uri(WebAPIUri, &quot;api/values&quot;); Console.WriteLine(&quot;Reading values from &#39;{0}&#39;.&quot;, requestURI); HttpResponseMessage httpResponse = await httpClient.GetAsync(requestURI); Console.WriteLine(&quot;HTTP Status Code: &#39;{0}&#39;&quot;, httpResponse.StatusCode.ToString()); if (httpResponse.IsSuccessStatusCode) { // // Code to do something with the data returned goes here. // } return (httpResponse.IsSuccessStatusCode);}Call the Web API to post a new valueThe code to issue an HTTP POST request to the Web API is almost identical. Since this method expects a new value in the body of the POST, the content-type header needs to be set and the content needs to be passed along in the body. Otherwise, it’s the same code.The full source code for the native client to call the Post method on the Web API is shown here.public async Task&amp;lt;bool&amp;gt; WriteValue(string value){ // Authenticate the user and get a token from Azure AD AuthenticationResult authResult = authContext.AcquireToken(resource, clientID, redirectUri); // Create an HTTP client and add the token to the Authorization header HttpClient httpClient = new HttpClient(); httpClient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue( authResult.AccessTokenType, authResult.AccessToken); // Construct the payload for the HTTP post operation var newValue = new StringContent(value); newValue.Headers.ContentType = new MediaTypeHeaderValue(&quot;application/x-www-form-urlencoded&quot;); // Call the Web API to post the new values Uri requestURI = new Uri(WebAPIUri, &quot;api/values&quot;); Console.WriteLine(&quot;Writing value &#39;{0}&#39; to &#39;{1}&#39;.&quot;, value, requestURI); HttpResponseMessage httpResponse = await httpClient.PostAsync(requestURI, newValue); Console.WriteLine(&quot;HTTP Status Code: &#39;{0}&#39;&quot;, httpResponse.StatusCode.ToString()); return (httpResponse.IsSuccessStatusCode);}Running the native client applicationMy primitive console application calls the ReadValues method and then the WriteValue method shown above. When AcquireToken is called in the ReadValues method, I am immediately prompted to sign-in since a token doesn’t already exist for me.After successfully authenticating, a token is issued to me and the values are returned from the Get method of the Web API because the security token included the scope claim with the Read_CloudAlloc_WebAPI value.Next, the WriteValue method is called. This time I am not prompted to sign-in because ADAL has cached my token and can use it since it hasn’t expired (more on this shortly). However, because the Read_Write_CloudAlloc_WebAPI value is not present in the scope claim, the Post method in the Web API returned an HTTP 401 (Unauthorized).The full output of the console application is shown here.Using the Azure Management portal, I can change the permissions for my native client application to grant the read/write permission instead, as shown here.Running the native client application now results in a successful call to both the Get and Post methods as shown here.ADAL’S Token Cache and Refresh TokensPreviously I mentioned that ADAL cached my token. ADAL does this automatically without you having to write any code, resulting in a positive experience for the end-user. If the token hasn’t expired, ADAL will re-use it in subsequent calls to AcquireToken. However, even if the token has expired, the end-user may still avoid being prompted to sign-in if a refresh token was issued when the access token was issued.A refresh token, which may not always be present, can be used to acquire a new access token on behalf of the user if Azure AD allows it. As long as the user’s account in Azure AD hasn’t been deleted, disabled, or some other change in the directory that would invalidate the token, the refresh token can be exchanged for a new access token. Best of all, ADAL will use the refresh token to acquire a new access token if it can and do so without you having to write any code.The figure below shows an instance of the AuthenticationResult returned from the first call to AcquireToken. You can see the time the AccessToken expires in the ExpiresOn property and the RefreshToken ADAL may use to acquire a new AccessToken.And it’s not always just for a single resource that a refresh token can be used to acquire an access token. If the IsMultipleResourceRefreshToken property is set to true, the RefreshToken can also be used to acquire an AccessToken for other resources registered in my Azure AD. Currently, I’m only accessing the Web API, but if my native client accessed another resource, then this RefreshToken could be used to acquire the AccessToken for that resource too without me having to sign-in.As you can see, ADAL provides a lot of benefits to native client applications with just a couple of lines of code.SummaryIn this post I demonstrated how to use claims in a Web API application protected by Azure Active Directory to make authorization decisions based on the presence of a scope claim in the authenticated user’s claim set.Next, I discussed how to expose the Web API to other applications in Azure AD by defining permissions in the oauth2Permissions array that can be updated in the manifest file for the Web API. By defining permissions for the Web API, other applications can be configured to access the application with selected permissions using the Azure Management portal.After the Web API was updated and configured, I discussed the steps necessary to register a native client application in Azure Active Directory. Using values in Azure Active Directory for the registered native client application, I then showed how to develop a console application that uses the Active Directory Authentication Library (ADAL) and the Microsoft HTTP Client Library to securely call the Web API. Running the native client, we were able to observe the benefit of the token cache provided by ADAL and the use of a refresh token to avoid unnecessarily prompting the user for credentials.In the next post, I will show how you can extend this scenario and make authorizations decisions based on a user’s group membership.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Building Web Apps for Azure AD", "url": "/2014/11/05/building-web-apps-for-azure-ad/", "categories": "", "tags": "", "date": "2014-11-05 15:06:04 -0600", "snippet": "In my last post I introduced some basic concepts about Azure Active Directory and ended with a review of the protocols and application endpoints that are used to build applications protected by Azure AD. In this post I will continue by talking about developing Web Applications and Web API’s protected by Azure AD using Visual Studio 2013.Before I get into the developer experience of building a Web Application or Web API protected by Azure AD using Visual Studio, I want to quickly pick up where I left off on the previous post and talk about the portal experience of adding an application to your directory. The experience is extremely simple and prompts you to make only four choices as shown below, two of which require very little thought.Here is a quick description of the four prompts. The name of your application. This can be any name you want and is simply how you will identify the application in your Azure Active Directory. The type of application. Notice that Web Applications and Web API’s are considered the same type of application as far as Azure AD is concerned. Since this article is about both, that makes this an easy decision. A Sign-On URL. This is the URL where users will access your application. There is no verification of this URL so even if the application hasn’t been developed yet the portal will still let you add it. An App Id URI. Notice this is a URI and not a URL. This is what Azure AD will use to identify your application when authentication users requesting access to it. It can be practically anything you want as long as it is unique in your directory and a valid URI.Now, click the checkmark and you have registered an application in Azure AD. Is it really that simple? Well, not really. What you have is just enough information at your fingertips to go build the application you just added to your directory. And if you continue down this path I promise you will learn to appreciate the degree of automation that Visual Studio and the Azure SDK provides for you when building Web Applications and/or Web API’s that are protected by Azure AD.So, let’s develop that appreciation now. :)Add a web application to Azure AD using Visual StudioTo get started building a web application protected by Azure AD, you must navigate your way through a few new project wizard dialogs. If you do this correctly, you will have a new web application that is also added to your Azure AD and configured properly, resulting in the F5 (Run) experience we’ve become so accustomed to from Visual Studio.Assuming that you are starting from the ASP.NET Project template for an MVC application and will be hosting it in an Azure Website, the first thing you must do is change the authentication type to use Organizational Accounts (the same as Student or Work accounts mentioned in the last article) and enter the domain (Azure AD tenant) you are externalizing authentication to. In my sample shown here, the Azure AD Tenant is configured with a custom domain otherwise it would be &amp;lt;tenant-name&amp;gt;.onmicrosoft.com.The next change you must do is specify an existing Azure SQL Database Server to use with your application or create a new one. The reason why this is required is because the code that is added to your project by the ASP.NET project template when using Organizational Accounts requires it.These are the only two changes you would have to make to successfully create a web application and add it to your Azure Active Directory using Visual Studio. If you prefer a step-by-step experience that walks you through every dialog and more, then please see Authenticating with Organizational Accounts and Azure Active Directory. And if you are interested in further understanding the SQL Database dependency I mentioned above, then see Deep Dive: Azure Websites and Organizational Authentication using Azure AD.The application produced by the ASP.NET template uses Windows Identity Foundation (WIF) in the .NET Framework to take care of authenticating users accessing the application. WIF provides the implementation of protocols such as WS-Federation that an application like this uses to authenticate users. It handles redirecting unauthenticated users to the correct sign-in (and sign-out) URL for their realm, verifying the signature of the SAML token issued to the client after successfully authenticating, managing session tokens and cookies for the client, extracting claims about the user from the token, and putting these claims in the principal object of the current thread. Most of this functionality is implemented in two HTTP modules that WIF provides, which are the WSFederationAuthenticationModule and SessionAuthenticationModule. Visual Studio takes care of adding these in the web.config as shown here.&amp;lt;system.webServer&amp;gt; &amp;lt;modules&amp;gt; &amp;lt;add name=&quot;WSFederationAuthenticationModule&quot; type=&quot;System.IdentityModel.Services.WSFederationAuthenticationModule, System.IdentityModel.Services, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089&quot; preCondition=&quot;managedHandler&quot; /&amp;gt; &amp;lt;add name=&quot;SessionAuthenticationModule&quot; type=&quot;System.IdentityModel.Services.SessionAuthenticationModule, System.IdentityModel.Services, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089&quot; preCondition=&quot;managedHandler&quot; /&amp;gt; &amp;lt;/modules&amp;gt;&amp;lt;/system.webServer&amp;gt;Visual Studio also adds the configuration necessary to insure that session security tokens can be used in a web farm type of environment across multiple machines. This is absolutely critical in situations where you have an Azure Website, Cloud Service or Virtual Machine configured with multiple instances hosting your web application. Otherwise, users would have to re-authenticate every time they landed on a different instance or be bound to a single instance throughout the entire session – neither of which is good for scale. This powerful feature is achieved by removing the default token handler that uses the Data Protection API (DPAPI) and replacing it with a token handler that uses a common machine key available to all instances as shown here. And again, this is taken of for you by the template. If you want to know where that key is and how it comes into the equation see the “deep dive” link I mentioned earlier.&amp;lt;system.identityModel&amp;gt; &amp;lt;identityConfiguration&amp;gt; &amp;lt;issuerNameRegistry type=&quot;CloudAlloc.Website.Utils.DatabaseIssuerNameRegistry, CloudAlloc.Website&quot; /&amp;gt; &amp;lt;audienceUris&amp;gt; &amp;lt;add value=&quot;https://cloudalloc.com/CloudAlloc.Website&quot; /&amp;gt; &amp;lt;/audienceUris&amp;gt; &amp;lt;securityTokenHandlers&amp;gt; &amp;lt;add type=&quot;System.IdentityModel.Services.Tokens.MachineKeySessionSecurityTokenHandler, System.IdentityModel.Services, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089&quot; /&amp;gt; &amp;lt;remove type=&quot;System.IdentityModel.Tokens.SessionSecurityTokenHandler, System.IdentityModel, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089&quot; /&amp;gt; &amp;lt;/securityTokenHandlers&amp;gt; &amp;lt;certificateValidation certificateValidationMode=&quot;None&quot; /&amp;gt; &amp;lt;/identityConfiguration&amp;gt; &amp;lt;/system.identityModel&amp;gt;Another important element in this section of the configuration is the &amp;lt;issuerNameRegistry&amp;gt; element. This is the class that has been added to your project that WIF will use to validate the SAML tokens presented to the web application. You can find it in the Utils folder of your project.Stepping out of the WIF configuration for a moment, if you look in the &amp;lt;appSettings&amp;gt; section you will find some settings that should look familiar.&amp;lt;appSettings&amp;gt; &amp;lt;add key=&quot;webpages:Version&quot; value=&quot;3.0.0.0&quot; /&amp;gt; &amp;lt;add key=&quot;webpages:Enabled&quot; value=&quot;false&quot; /&amp;gt; &amp;lt;add key=&quot;ClientValidationEnabled&quot; value=&quot;true&quot; /&amp;gt; &amp;lt;add key=&quot;UnobtrusiveJavaScriptEnabled&quot; value=&quot;true&quot; /&amp;gt; &amp;lt;add key=&quot;ida:FederationMetadataLocation&quot; value=&quot;https://login.windows.net/cloudalloc.com/FederationMetadata/2007-06/FederationMetadata.xml&quot; /&amp;gt; &amp;lt;add key=&quot;ida:Realm&quot; value=&quot;https://cloudalloc.com/CloudAlloc.Website&quot; /&amp;gt; &amp;lt;add key=&quot;ida:AudienceUri&quot; value=&quot;https://cloudalloc.com/CloudAlloc.Website&quot; /&amp;gt; &amp;lt;/appSettings&amp;gt;The first is the ida:FederationMetadataLocation. Recall from the last post that this was the first endpoint listed that is unique to my Azure Active Directory. In this metadata document is a massive amount of information about my directory tenant in Azure AD, including the public key that is used to validate tokens. The ida:Realm is used when unauthenticated users are redirected to Azure AD to sign-in and the ida:AudienceUri is that unique identifier in Azure AD that I mentioned at the very beginning of this post. So, where do these appSetttings get used? Go look in App_Start\\IdentityConfig.cs – another class added to your project by the ASP.NET template.Are you starting to appreciate all the intricate WIF configuration necessary to make a web application secure with just a few clicks? There is so much more that I don’t have space here to get into. So let me end this with just one more element in the configuration you should at least be aware of, which is the &amp;lt;system.identityModel.Services&amp;gt; element as shown here.&amp;lt;system.identityModel.services&amp;gt; &amp;lt;federationConfiguration&amp;gt; &amp;lt;cookieHandler requireSsl=&quot;true&quot; /&amp;gt; &amp;lt;wsFederation passiveRedirectEnabled=&quot;true&quot; issuer=&quot;https://login.windows.net/cloudalloc.com/wsfed&quot; realm=&quot;https://cloudalloc.com/CloudAlloc.Website&quot; requireHttps=&quot;true&quot; /&amp;gt; &amp;lt;/federationConfiguration&amp;gt;&amp;lt;/system.identityModel.services&amp;gt;This configuration applies specifically to the WSFederationAuthenticationModule that I mentioned earlier. Notice the issuer property which should also look familiar. Recall from the last post that this was the second endpoint listed that is unique to my Azure Active Directory and is where users sign-in and sign-out of the web application.Add a Web API application to Azure AD using Visual StudioTo get started building a Web API application protected by Azure AD using Visual Studio, start with the same ASP.NET project template but this time choose Web API as shown here.The rest of the experience is the same as for web applications with one very important difference – the Web API project has no requirement for a SQL Database. Now, you may want to have a SQL Database linked to your Web API, but just understand that the project template in this case does not emit any code in your project that absolutely requires it. So, you’re free to choose the No database option if you like as shown here.The application produced by the ASP.NET template in this scenario uses Microsoft Open Web Interface for .NET (OWIN) Components to take care of authenticating users accessing the application. OWIN is still relatively new and is essentially a specification that defines a standard interface between .NET web servers and web applications. Microsoft’s OWIN Components (aka project “Katana”) provide an open source implementation of the OWIN specification with goals of being portable, modular, and lightweight.The application in this scenario uses OAuth 2.0 to secure the Web API’s (REST API’s) and therefore uses the OAuth 2.0 application endpoints in Azure AD that I mentioned in the first post in this series. The implementation also takes advantage of the JSON Web Token (JWT) format that is much lighter than the SAML token format we saw previously.Unlike WIF, OWIN is not a core component of the .NET Framework 4.5 stack. Instead, it is delivered to your project through a set of Nuget packages as shown below, consisting of the community owned and run Owin package (Owin.dll) and several Microsoft Owin Components that provide the implementation of the OWIN specification.&amp;lt;package id=&quot;Microsoft.Owin&quot; version=&quot;2.1.0&quot; targetFramework=&quot;net45&quot; /&amp;gt;&amp;lt;package id=&quot;Microsoft.Owin.Host.SystemWeb&quot; version=&quot;2.1.0&quot; targetFramework=&quot;net45&quot; /&amp;gt;&amp;lt;package id=&quot;Microsoft.Owin.Security&quot; version=&quot;2.1.0&quot; targetFramework=&quot;net45&quot; /&amp;gt;&amp;lt;package id=&quot;Microsoft.Owin.Security.ActiveDirectory&quot; version=&quot;2.1.0&quot; targetFramework=&quot;net45&quot; /&amp;gt;&amp;lt;package id=&quot;Microsoft.Owin.Security.Jwt&quot; version=&quot;2.1.0&quot; targetFramework=&quot;net45&quot; /&amp;gt;&amp;lt;package id=&quot;Microsoft.Owin.Security.OAuth&quot; version=&quot;2.1.0&quot; targetFramework=&quot;net45&quot; /&amp;gt;&amp;lt;package id=&quot;Owin&quot; version=&quot;1.0&quot; targetFramework=&quot;net45&quot; /&amp;gt;The Owin package contains the IAppBuilder interface which standardizes the startup process for the application. I’ll come back to this interface shortly.The web.config for this project is far less verbose and essentially comes down to just a couple of appSettings that link back to the application registration in Azure AD.&amp;lt;appSettings&amp;gt; &amp;lt;add key=&quot;webpages:Version&quot; value=&quot;3.0.0.0&quot; /&amp;gt; &amp;lt;add key=&quot;webpages:Enabled&quot; value=&quot;false&quot; /&amp;gt; &amp;lt;add key=&quot;ClientValidationEnabled&quot; value=&quot;true&quot; /&amp;gt; &amp;lt;add key=&quot;UnobtrusiveJavaScriptEnabled&quot; value=&quot;true&quot; /&amp;gt; &amp;lt;add key=&quot;ida:Tenant&quot; value=&quot;cloudalloc.com&quot; /&amp;gt; &amp;lt;add key=&quot;ida:Audience&quot; value=&quot;https://cloudalloc.com/CloudAlloc.WebAPI&quot; /&amp;gt; &amp;lt;add key=&quot;ida:ClientID&quot; value=&quot;024e16d8-639b-4de3-b57c-e4066473e7c9&quot; /&amp;gt;&amp;lt;/appSettings&amp;gt;The ida:Tenant and ida:Audience have the same meaning as previously discussed. The ida:ClientID is a unique identifier for the application in Azure AD and is used if/when the Web API accesses other applications in Azure AD. We will see this used a lot in the next couple of posts in this series. That’s basically it for the configuration!Let’s turn attention now to the code, which essentially comes down to two files being added in your project: Startup.cs and Startup.Auth.cs as shown here.Startup.cs provides the class name and method signature that “Katana” applications look for to configure the Microsoft OWIN components. The Configuration method takes an IAppBuilder interface which if you recall, is the interface defined in the Owin.dll.using Owin; namespace CloudAlloc.WebAPI{ public partial class Startup { public void Configuration(IAppBuilder app) { ConfigureAuth(app); } }}The Configuration method simply calls the ConfigureAuth method in the Startup.Auth.cs file that was added to the project that in turn adds the Azure Active Directory JWT Bearer Token middleware to the application’s HTTP request pipeline as shown here. Notice that it is here where the two appSettings in web.config are pulled in the configuration.using System;using System.Collections.Generic;using System.Configuration;using System.Linq;using Microsoft.Owin.Security;using Microsoft.Owin.Security.ActiveDirectory;using Owin; namespace CloudAlloc.WebAPI{ public partial class Startup { public void ConfigureAuth(IAppBuilder app) { app.UseWindowsAzureActiveDirectoryBearerAuthentication( new WindowsAzureActiveDirectoryBearerAuthenticationOptions { Audience = ConfigurationManager.AppSettings[&quot;ida:Audience&quot;], Tenant = ConfigurationManager.AppSettings[&quot;ida:Tenant&quot;] }); } }}The bearer token middleware simply means that any party in possession of a token (a “bearer”) can use it get access to the resource (the Web API endpoints in this case) as defined in the OAuth 2.0 Authorization Framework specification.Just like in the Web Application scenario, this Web API project has all of these details implemented for you and the application is automatically registered in Azure Active Directory. Unlike the Web Application scenario, you cannot just press F5 to run it in your browser. This is a Web API project that expects you to be the bearer of a JWT token to access the endpoints of the application. I’ll show you how to get that token and use it to access the Web API endpoints in the next post.Identity in the .NET FrameworkWhether authentication of users is accomplished using the WS-Federation or OAuth 2.0 endpoints in your Azure Active Directory, and whether a SAML or JWT token was presented to your application, once your application is invoked you can access all the claims that Azure AD (or the user’s identity provider) issued when the user was authenticated. This is possible because your application is claims-aware and is the case for any .NET application targeting .NET Framework 4.5 or newer.In a claims-aware application the client’s identity is presented as a collection of claims to the application. And as you know, these claims are delivered via the authentication token that the client received from their identity provider (ie: Azure AD) after successfully authenticating. A claim is a statement about the user that the identity provider can corroborate. For example, my Azure AD can undeniably state that my email is rick@cloudalloc.com. A claims-aware application is one that relies on the claims presented to drive application behavior for the user because it trusts the identity provider it has externalized authentication to. If you have ever heard the term “relying party” or “RP” to describe a claims-aware or claims-based application, that is why – because it relies on the claims presented by the client provided the claims are issued by an identity provider it trusts.Every thread in a .NET application has a ClaimsPrincipal object that can be used to discover the identity of the client, the identity provider that authenticated the client, and the claims the identity provider included in the authentication token. If you are already familiar with the traditional IPrincipal interface in .NET, then you already know about the principal object on the thread. And as you can see in the diagram below, the ClaimsPrincipal class implements the IPrincipal interface making it compatible with existing code. The ClaimsPrincipal also has a Claims property that you can use to access an individual Claim for a client.These classes are core components of the .NET Framework starting with version 4.5. You don’t need to download a Nuget package or install an SDK. These classes are as common as System.String and are defined in the mscorlib assembly. So, if you’re targeting .NET Framework 4.5 or newer, then your application is a claims-aware application.The ClaimsPrincipal class has a static property called Current that will return the ClaimsPrinicipal object for the thread. It is also equipped with some handy methods you can used to find and retrieve the values of claims for a clientThe Claim class is used to describe statements about the Subject (or client) that the Issuer can prove. Each claim as a Type that describes the statement, such as the Subject’s Name, Email, PostalCode or any other custom claim that the issuer (or you) has added to the Claims collection. There are 50+ predefined ClaimTypes that are used for the more common claims. However, these ClaimTypes are simply URI’s so you are free to create custom claims for any situation. For example, you may store a profile for a user that includes the make of the car he or she drives and add a claim for that to the Claims collection in a ClaimsAuthenticationManager implementation.An example of how you can retrieve the Surname claim for a Subject is shown here.var principal = ClaimsPrincipal.Current;var surnameClaim = principal.FindFirst(ClaimTypes.Surname);var surname = (surnameClaim != null) ? surnameClaim.Value : &quot;&quot;;Or, if you prefer to use LINQ you could do something like this.var principal = ClaimsPrincipal.Current;var surnameClaim = principal.Claims.Where(c =&amp;amp;gt; c.Type == ClaimTypes.Surname).FirstOrDefault();var surname = (surnameClaim != null) ? surnameClaim.Value : &quot;&quot;;As you can see the object model is extremely simple to navigate to discover and retrieve values of claims.SummaryIn this post talked about the developer experience of building Web Applications and Web API applications that are protected by Azure AD. To recap each of these, a Web Application generated using Visual Studio 2013 is protected by WIF’s implementation of the WS-Federation protocol which is configured via settings in web.config, code that is added to your project, and a SQL Database that stores cryptographic keys. The web application receives a SAML token from authenticated users and WIF validates the token and extracts the claims about the client from the token.Web API applications generated using Visual Studio 2013 are protected by Microsoft’s OWIN middleware components using OAuth 2.0 and JWT token format. The OWIN components perform similar token validation and also extract the claims from the token.Finally, I wrapped up with a brief overview of the ClaimsPrincipal and Claim classes and demonstrated how you can retrieve the claims to drive behaviors in your application code.In the next post, I will cover the Active Directory Authentication Library (ADAL) and show you how you can use it to call Web API’s protected by Azure AD and build native client (non-browser based) applications for Azure AD.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Azure Active Directory: An Introduction", "url": "/2014/10/16/azure-active-directory-an-introduction/", "categories": "", "tags": "", "date": "2014-10-16 16:06:04 -0500", "snippet": "What Azure Active Directory is (and is not)Azure Active Directory (aka Azure AD) is a fully managed multi-tenant service from Microsoft that offers identity and access capabilities for applications running in Microsoft Azure and for applications running in an on-premises environment. Its name leads some to make incorrect conclusions about what Azure AD really is. Therefore, to avoid any confusion with Windows Server Active Directory that you may already be familiar with in an on-premises environment, understand that Azure AD is not Windows Server Active Directory running on Virtual Machines in Microsoft Azure.Azure AD is not a replacement for Windows Server Active Directory. If you already have an on-premises directory, it can be extended to the cloud using the directory integration capabilities of Azure AD. In these scenarios, users and groups in the on-premises directory are synced to Azure AD using a tool such as Azure Active Directory Sync (AAD Sync). This has the benefit of users being able to authenticate against Windows Server Active Directory when accessing on-premises applications and resources, and authenticating against Azure AD when accessing cloud applications. The user can authenticate using the same credentials in both scenarios.Azure AD can also be an organization’s only directory service. For example, many startups today don’t have an on-premises Windows Server Active Directory. In these scenarios, an organization may simply rely on Office 365 and other SaaS applications to conduct its business and manage its user’s identity and access to SaaS applications, all online.As a developer of cloud applications, you can use Azure AD to accomplish things such as single sign-on (SSO) for your cloud applications, query the directory for user and group information, and even write to the directory provided your application has the permissions to do so. Of course, you can accomplish similar ideas on-premises and build applications using Microsoft proprietary technologies such as Windows NTLM, Kerberos, LDAP and so on. However, as more applications are developed with the intent of running in the cloud, and access to those applications cross organizational boundaries, and are accessed from a growing number of devices across a variety of platforms, organizations need an enterprise ready directory service that can handle the authentication needs of the organization and the applications it depends on. The graphic below illustrates how Azure Active Directory could be used in a company to address some common enterprise needs.Throughout this series, the focus will be on the developer, with careful attention given to the tools, libraries and features of Azure AD that can be used to build cloud applications that are protected by Azure AD. This post lays the groundwork for some key concepts in Azure AD. Future posts in this series will go deep into the developer experience for different types of cloud applications.Users and GroupsIn Azure AD there are two entities that we are most concerned with during application development; the user and perhaps the group (or groups) the user is a part of. We use the user’s claims (statements about the user from Azure AD) to drive application behavior such as personalizing screens or making authorization decisions about what the user can do. By externalizing the authentication of users to Azure AD we’re able to rely on the claims provided in the authentication token for the user to drive these application experiences.A user in a directory can be sourced from either a directory in Azure AD, which is referred to as a Work or Student Account (previously called an Organizational Account). Or, the user can be sourced from a Microsoft Account. An example of how this may look in the Azure AD Users page of the Azure Management Portal is shown here.Work or Student AccountsUsers are generally added to a directory in Azure AD as a Work or Student Account user (formerly know as Organizational Accounts). A user’s user name and email address will take the form of &amp;lt;someuser&amp;gt;@&amp;lt;someorg&amp;gt;.onmicrosoft.com and the account typically exists for as long as the user is part of the organization and until an Administrator removes the account. If you have ever had an account in an on-premises Windows Server Active Directory (for example, at work) then this is essentially the same concept.A user from a different directory in Azure AD can also be added to a directory (aka external user). This is particularly useful in situations where users in different directories need access to the same cloud applications protected by Azure AD. This is a very powerful scenario and one that is increasing popular as more and more organizations partner with one another. Consider, for example, a user “John Doe” from CompanyA (john.doe@companya.onmicrosoft.com) is added as a user to the directory for CompanyB. This would enable “John Doe” to access CompanyB’s cloud applications protected by CompanyB’s Azure AD. Now, when “John Doe” authenticates, he will do so in the realm of CompanyA’s directory, not CompanyB’s directory. So, his profile data, password, policies, etc. are all managed by the administrators at CompanyA. As it should be – CompanyA knows him best and is the authority that can properly authenticate him. Given that CompanyA and CompanyB have established trust between the two organizations, CompanyB can add “John Doe” to their directory so he can collaborate with users and applications in CompanyB. If CompanyB decides later to remove “John Doe” from the directory, then he will no longer be able to access CompanyB’s resources. However, suppose “John Doe” quits CompanyA or is removed from the organization for other reasons. What is the first thing IT generally does in this case? They delete “John Doe” from the directory (or at least disable the account). They probably don’t call all their partners such as CompanyB to tell them that “John Doe” should be removed. But that’s ok, because as I pointed out earlier, “John Doe” will never authenticate in the realm of CompanyB’s directory. He will authenticate against CompanyA’s directory. And since his account will have been either removed or disabled, he won’t be able to successfully authenticate, and therefore won’t be able to access any CompanyB resources even though his account technically still exists in CompanyB’s directory. If you have been in this industry for at least a few short years you can probably appreciate the immense flexibility and level of security this brings to an organization.During authentication, Work or Student Account users may notice the blue badge icon shown above. This icon is a visual hint to the user signing in using a Work or Student Account.Microsoft AccountsUsers can also be added to a directory in Azure AD as a Microsoft Account user. In this scenario, the user name and email address will likely take the form of &amp;lt;someuser&amp;gt;@hotmail.com, &amp;lt;someuser&amp;gt;@outlook.com, or &amp;lt;someuser&amp;gt;@live.com. A Microsoft Account is an individual account that a user has created to access consumer services such as Xbox LIVE, Messenger, Outlook/Hotmail, etc. Unlike the Organizational Account, these accounts don’t get deleted when they are removed from a directory in Azure AD. The account belongs to the person in this case, not an organization. However, Azure AD enables organizations to add users to their directory using a person’s Microsoft Account and these users are also considered external users of the directory.One scenario where you might add a user to a directory in Azure AD using a person’s Microsoft Account is in a “freelance” situation where you need an external user (someone that is not part of the organization) to have access to cloud applications for a particular project or service agreement. This prevents the user from having to keep up with a separate set of user credentials just to access the necessary application resources for the project. When the project term ends the user can simply be removed from the directory.During authentication Microsoft Account users may notice the Windows icon shown above. This icon is a visual hint to the user signing in using a Microsoft Account.Adding users and groups to Azure ADUsers and Groups can be added to a directory in a variety of ways. In no particular order here are some common methods: By syncing from an on-premises Windows Server Active Directory using AAD Sync. This is how most enterprise customers will get their users added to the directory and requires some additional server configuration on-premises to setup. Manually using the Azure Management Portal. The portal experience is very easy and intuitive. Organizations that don’t have an on-premises directory may use this approach for its simplicity, provided the number of users is relatively small. This is also very useful during development and test phases of application development. Scripted using PowerShell and the Azure Active Directory cmdlets. PowerShell makes automating this task very useful, particularly for large user bases. This too can be very useful during development and testing. Programmatically using the Azure AD Graph API. This is an extremely powerful option that essentially gives you full control of how users are added to the directory. We will see examples of this later in the series.Custom DomainsEvery directory in Azure AD gets a unique DNS name on the shared domain *.onmicrosoft.com. So, as an example, for a directory named “cloudalloc”, the DNS name would be cloudalloc.onmicrosoft.com. A user in the directory would therefore have a user name such as john.doe@cloudalloc.onmicrosoft.com.By using a custom domain you are able to associate a domain you own with a directory in Azure AD. This is not required, but often preferred by customers who own their own domain name. Continuing with the same example, if you owned the cloudalloc.com domain a user name would take the form john.doe@cloudalloc.com instead of john.doe@cloudalloc.onmicrosoft.com.Configuring a custom domain and associating it with a directory in Azure AD is a relatively simple process. First, you need to own the domain name you want to use with your directory. Next, you need to go through a domain verification step in Azure AD, which basically involves updating the DNS records with your domain registrar to include a TXT record and value (provided by Azure AD) to prove you own the domain. The details for configuring a custom domain are covered in full detail in this Verify a domain article on TechNet.Protocols supported by Azure ADAs a developer building applications protected by Azure AD you will find that Azure AD provides support for all the common protocols that can be used to secure your applications. Some of these protocols have been around for a really long time and as a result are widely used in the industry today. Others are still emerging as a new (and preferred) way to protect access to cloud applications. The protocols supported are shown here: WS-Federation – This is arguably one of the most well-known and used protocol today for authenticating users of web applications. Microsoft uses this when authenticating users for some of their own cloud applications, such as the Microsoft Azure Management portal, Office 365, Dynamics CRM Online, and more. There is fantastic tooling support in Visual Studio 2010, 2012, and 2013 for this protocol making it very easy for developers to protect their applications using Azure AD. The token format used in this protocol is SAML. SAML-P – This is also a widely adopted protocol and follows a very similar authentication pattern to WS-Federation. However, it does not get the same level of tooling support that WS-Federation gets. The token format used in this protocol is also SAML. OAuth 2.0 – This is an authorization protocol that has been quickly and widely adopted in the industry as a way to sign-in users using their credentials at popular web applications such as Facebook, Twitter, and other “social” applications. Some of the benefits of this protocol is its smaller token format, JSON Web Token (JWT), and application scenarios it simplifies such as accessing Web API’s from a native client with an access token. To do the latter with WS-Federation or SAML-P involves a lot of intricate code and configuration. OpenID Connect – This is a protocol that adds an authentication layer on top of the existing OAuth 2.0 protocol. Because it is layered on OAuth 2.0, it benefits from the highly efficient JWT token format that OAuth 2.0 uses.The OAuth 2.0 and OpenID Connect just recently became Generally Available (GA, or fully supported and out of preview in September of 2014) on Azure AD and there is a great amount of work going into libraries like Active Directory Authentication Library (ADAL) and OWIN middleware components to light up scenarios these protocols enable for developers. This series will cover these libraries in great detail in later posts. The type of application you build and the requirements for your application will largely determine which of these protocols is used to protect the application when registering it with Azure AD. For now, it is enough to know they are supported by Azure AD.From protocols to application endpointsThe support for these protocols is surfaced in Azure AD through a set of Application Endpoints. These endpoints are unique for each directory (or tenant) in Azure AD. The table below shows application endpoints and URL for each of the supported protocols. It is these endpoints that your application uses to leverage the various protocols when authenticating users. Notice that every endpoint starts with https://login.windows.net/&amp;lt;tenant&amp;gt;/, where &amp;lt;tenant&amp;gt; is a unique identifier for the directory. Application Endpoint URL Federation Metadata Document https://login.windows.net/&amp;lt;tenant&amp;gt;/federationmetadata/2007-06/federationmetadata.xml WS-Federation https://login.windows.net/&amp;lt;tenant&amp;gt;/wsfed SAML-P https://login.windows.net/&amp;lt;tenant&amp;gt;/saml2 OAuth 2.0 Token https://login.windows.net/&amp;lt;tenant&amp;gt;/oauth2/token OAuth 2.0 Authorization https://login.windows.net/&amp;lt;tenant&amp;gt;/oauth2/authorize In each of these endpoints, &amp;lt;tenant&amp;gt; can be either the Guid that is assigned to the directory, or the hostname of the directory. In other words, for an Azure Active Directory named “cloudalloc” with a tenant id of “530c3a3b-e508-4826-997a-38fb543bc87f”, the following two URL’s for the WS-Federation endpoint would be equivalent. https://login.windows.net/cloudalloc.onmicrosoft.com/wsfed https://login.windows.net/530c3a3b-e508-4826-997a-38fb543bc87f/wsfedIf a custom domain was configured for the directory, then the domain name could also be substituted for &amp;lt;tenant&amp;gt;.As this series progresses we will see how these endpoints are leveraged for various types of applications.Adding applications to Azure ADAdding applications to an Azure AD tenant (or directory) is necessary when you want users in your organization to be authenticated against your directory before accessing the application. By adding the application to the directory (or registering it), you are adding configuration that Azure AD will need to identify your application as an application so that it can issue authentication tokens when authenticating users.Applications you develop can be registered with a directory in Azure AD by using tools such as Visual Studio, the Azure Management Portal and other command line tools. The Azure Management Portal provides an easy wizard experience to get the process started as shown here when you click on the add application button in the portal.When you choose the option to add an application my organization is developing, you must next indicate the type of application you are going to add. The reason the type is important is because it is here where necessary protocol artifacts will be collected leading you eventually to the use of either WS-Federation or OAuth / OpenID Connect. The two choices are: Web Application and/or Web API – Think of browser-based web applications or services that are accessed using a browser and/or protocols of the web. Native Client Application – Think of client applications that will run on a desktop computer, laptop, or other smart device.SummaryIn this post I introduced Azure AD and some important concepts regarding users. I talked briefly about custom domains and then wrapped up by covering the protocols supported by Azure AD and how they are surfaced through application endpoints unique to the directory. In the next post, I’ll dive into the developer experience and talk about building on the first of the two choices mentioned in the last section which is Web Applications and/or Web API’s.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Deep Dive: Azure Websites and Organizational Authentication using Azure AD", "url": "/2014/08/19/deep-dive-azure-websites-and-organizational-authentication-using-azure-ad/", "categories": "", "tags": "", "date": "2014-08-19 16:06:04 -0500", "snippet": "In my previous post I showed how to create an Azure Website that uses Organizational Accounts for authenticating users in Azure Active Directory. In this post, I’m going to go deeper and explore how Visual Studio, the Azure SDK, and Azure Active Directory collectively make building secure LOB web applications for the enterprise a first class experience from the very beginning of the application development process.The last post was more practical and demonstrated the developer experience when creating a Website that uses Organizational Authentication. So, if you just want to know the how-to, then check out that post. However, if you want to know more about what is happening behind the scenes, then by all means please read on.The SQL Database DependencyRecall from my previous post that as I created my Azure Website, I changed the authentication type to use Organizational Accounts. As a result of that decision, I picked up a SQL Database dependency too. Now, having this SQL Database created for me is actually pretty handy. After all, my Website will need a database of some kind, and since an Azure Website targeting the enterprise will be using Organizational Accounts, it is a logical choice for a database. But, why was this necessary? That’s the gist of this post.First, let’s just revisit what was created in the last post, which was the Azure Website, called AzureADSampleWebsite. And, as part of that process, a SQL Database was created for me too, called AzureADSampleWebsite_db.In this database, there were three tables created. Two of these that are critical to supporting the Organizational Authentication type are the IssuingAuthorityKeys and the Tenants tables.Now, let’s take a deeper look into these tables, what they contain, and how they are used.The Tenants TableThe Tenants table has just one row with one Id column, which is just a GUID. Each Azure Active Directory tenant has an ID, and for my Azure AD Tenant, it is this.This ID shows up in a number of places. For example, it is a part of the URL for various endpoints hanging off of my Azure Active Directory, such as the Federation Metadata Document location, the WS-Federation Sign-on Endpoint, the OAuth 2.0 Endpoints, and others that Azure AD supports. You can see these using the Azure Management Portal by clicking on the View Endpoints button at the bottom of the Applications page of your Azure AD.Each of these endpoints shown is unique. However, notice that each one of them has the Azure AD Tenant ID in the URL. Which of these endpoints you would use in your code depends on the type of application you are building and the authentication requirements you are targeting. For my sample MVC Website, the WS-Federation Sign-on Endpoint (2nd shown below) is what is used to sign users in and out.The Federation Metadata Document (1st shown above) is of particular interest for this discussion. Every Azure AD tenant has a federation metadata document that provides certain metadata describing the STS endpoints and other metadata needed by client applications that have externalized authentication to the Azure AD tenant. For example, my sample Website has externalized authentication of users to my Azure AD tenant. This means that after a user has been authenticated, they (the user’s browser) will present a token to my application that Windows Identity Foundation (WIF) will validate and process. One of the validation steps it performs is to insure that the token received is from an STS that I trust. In this case, my Azure AD tenant. This is accomplished by checking the token’s signature using a certificate which can be found in the Federation Metadata Document (shown below).Fortunately I don’t have to write any code to do this token validation stuff. As demonstrated in my last post, this is all handled for me through some nifty tooling automation and WIF, which leads me to the other table that was created in the SQL Database, the IssuingAuthorityKeys table.The IssuingAuthorityKeys TableThe IssuingAuthorityKeys table also has just one row with one Id column. The value of this Id is the thumbprint of the signing certificate found in the Federation Metadata Document.You can verify this by grabbing the full certificate value from the Federation Metadata Document and saving it to a .cer file and examining the certificate. That is precisely what I did here. I pasted this value into notepad and then saved it as a .cer file.Then, from Windows Explorer, I right-clicked on the certificate and selected Open to view the certificate. In the Details tab, I scrolled down to find the Thumbprint for the certificate. Does this value look familiar? It should. It is the Id in the IssuingAuthorityKeys table.By having the signing certificate thumbprint stored in a SQL Database, it is a relatively trivial task now to retrieve it and pass it on to the WIF modules in my Website so that WIF has what it needs to validate and process tokens presented to my application. Fortunately, I don’t have to write any code to do this either! This code is provided automatically by the Visual Studio ASP.NET Web Application template when Organizational Accounts is selected for the authentication type.The CodeChoosing Organizational Accounts for authentication using the ASP.NET Web Application template will inject the necessary configuration and code into your project, making this experience simply a pleasure to work with. First, is the web.config file, where you will find all the configuration necessary to support this type of authentication. Second, is the code that is added to your project to read the web.config and retrieve the signing certificate thumbprint from the SQL Database so that WIF can be properly configured at application start-up.The files shown here are added to the Web Application project to do all the heavy lifting for you. It is not necessary to change a thing. I’m just pointing it out because now that you know what is going on behind the scenes, the code in these files will make sense.TenantRegistrationModels.csThis file is the class definitions (models) for the two tables in the SQL Database.TenantDbContext.csThis file defines the TenantDbContext class, derived from DbContext. This class is used when querying the tables in the SQL Database. If you have used Entity Framework, then you already know how this works.DatabaseIssuerNameRegistry.csThis file contains a helper class derived from ValidatingIssuerNameRegistry that uses the TenantDbContext (from above) to retrieve the Azure AD tenant Id and signing certificate thumbprint values. There is code in here to do a few other things, but the two methods that get used in the call to IsThumbprintValid are shown here.IdentityConfig.csThis is where everything above gets kicked off. In particular is the ConfigureIdentity method, which is called from the Application_Start method in the Global.asax file to bootstrap the configuration at start-up. Notice that RefreshValidationSettings references the Federation Metadata Document endpoint (from web.config) and then calls DatabaseIssuerNameRegistry.RefreshKeys to add or update the keys (thumbprint) for the Azure AD Tenant. This is where the Tenants table and the IssuingAuthorityKeys table get the values I discussed earlier.SummaryIn this post, I really just scratched the surface on the amount of automation that the tooling provides in this scenario. There is more that comes into the picture when publishing the website to Microsoft Azure. I briefly mentioned it in my last post. Personally, I appreciate the experience provided by the platform and tools because it wasn’t that long ago when some of this had to be done manually. And having an understanding of what is going on is important should you ever want to change things up. Hopefully this post has provided that.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Authenticating with Organizational Accounts and Azure Active Directory", "url": "/2014/07/28/authenticating-with-organizational-accounts-and-azure-active-directory/", "categories": "", "tags": "", "date": "2014-07-28 16:06:04 -0500", "snippet": "If you’re an enterprise developer targeting Microsoft Azure for a new Line-of-Business (LOB) application, then you will most likely be building your application to authenticate users using Azure Active Directory. Azure Active Directory (Azure AD) is Microsoft’s service that provides identity and access capabilities in the cloud. It can be used to authenticate users of cloud applications or users running modern LOB applications on-premises that may be leveraging Azure services behind the scenes.In this post, I’m going to demonstrate how to use Azure Active Directory and Visual Studio 2013 to build a web application that authenticates users using their Organizational Account (an account that has been created by an organization’s administrator for a user in the organization). Along the way, I will draw attention to some important details you need to be aware of to be successful in this scenario. I will also provide some background on how Visual Studio does some of its magic to make this a rather low-friction experience.To start, I’m going to use an Azure Subscription that is sourced from a Microsoft Account (rather than an Organization Account). I’m choosing this because it’s the situation I find many developers (including myself) starting from that just want to learn how to build an application that will use Organization Accounts for authentication, without actually using an Azure organizational subscription that you may not have access to.Adding users to Azure Active DirectoryFirst, I added two users to my Azure Active Directory.App User is just your typical user in the directory and doesn’t require any explanation.Dev User is a user that would be representative of typical developer in an organization. That would be you (if you’re reading this blog). What makes this user unique are two things:Dev User is a Global Administrator in the Azure Active Directory. This is because Dev User will be building an application that will be registered in Azure AD and therefore will need rights to configure the directory for the application.Dev User is a Co-Administrator on the Azure Subscription. This is because Dev User will be building an application that will be using Azure and therefore will need rights to provision services (Azure Websites, SQL Databases, etc.) in the Azure Subscription.The 3rd user (myself) is the Microsoft Account associated with my Azure Subscription. This user is added automatically and cannot be removed.Create an MCC Web ApplicationThe point of this blog is not to create a fancy web site, so I’m going to just use the MVC default template to get something up and working. From the main menu, simply select FILE -&amp;gt; New -&amp;gt; Project and select the ASP.NET Web Application template.In the New ASP.NET Project wizard is where you will find your opportunity to change the authentication type for your application. It defaults to Individual User Accounts (think Facebook, Twitter, Username/Password, etc.). By clicking on the Change Authentication button, you can choose other options for authentication.In this case, I’m going with Organizational Accounts. When you choose this option, you also have to indicate the domain (your Azure AD) for your application.Of course, just providing the name of the domain is not sufficient. Visual Studio will challenge you to sign-in so it can verify you have access to this domain and rights to configure it. Why? Because it’s about to provision the application for you in your Azure AD. Signing in as Dev User (a Global Administrator in my directory) will satisfy this challenge.After successfully authenticating, you’re returned back to the New ASP.NET Project wizard screen with the authentication type changed to Organizational Auth. Clicking on the OK button here completes the steps needed to create the web application and then transitions into the next part – configuring the Azure Website in Microsoft Azure.Create the Azure Website and SQL DatabaseThe next dialog is all about configuring the Azure Website in Microsoft Azure. So, selecting a globally unique name (for DNS) and indicating the region you want to run in are part of this experience. Notice also that I indicated to create a new database server (SQL Database). This is a very important step in this process. It’s not necessarily because I want a database for my web site. No, this is actually needed to support the Organizational Account authentication type I selected previously. Without a database, the startup code for the web site won’t be able to locate the information it needs to initialize the Windows Identity Foundation (WIF) modules so WIF can properly validate authentication tokens issued from my Azure AD tenant. The information (a signature) to properly identify the Azure AD tenant this application trusts for authentication is put into the SQL Database as an automation step for you. I’ll talk a lot more about this database dependency in my next blog post. For now, just know that for this application to work when publishing it to Azure, the MVC project template injects code in your solution that is expecting a database to retrieve these values from.At this point, Visual Studio has its marching orders and is ready to go into action as soon as you click the OK button. When you do, here is what you will get in return:The Microsoft Azure WebsiteVisual Studio created the Microsoft Azure Website in Azure. Note, it did not publish the site, it just created it. So, if you tried to browse to its URL you would just get the standard empty web site page.The Microsoft Azure SQL DatabaseThe SQL Database (and server in this case) was created and linked to the Azure Website above. While the SQL Database is initially created to support my decision for Organization Account authentication, there’s nothing to stop me from using this database for other data needs in my application. Having this created as a linked resource for my Website is a nice benefit.The application (for localhost) registered in Azure ADAn application was added to my Azure AD. There is quite a bit of automation that took place here and if you have a general understanding of the WS-Federation protocol and how Windows Identity Foundation works, then you can really start to appreciate what is going on here to setup authentication for organizational accounts. Looking in the APPLICATIONS tab for your Azure AD, you will see what I call the “local” version of the application. Meaning, this application and all of its configuration is in place to support you running your MVC application on localhost while developing the app.Press F5 to run your application locally and sign-in using the credentials of an organizational user (such as App User in my example), and the page loads as expected. Beautiful!Publish the application to AzureNow, let’s fast-forward and assume you’re ready to publish your application to your Azure Website and run this in the cloud. That is a simple process to kick off. Simply right-click on your project in Visual Studio and select Publish. Visual Studio will launch the Publish Web dialog as you see here and this first screen can typically be left alone. Click on the Next button to get to the important stuff.On the Settings page of the dialog, you must check the checkbox to Enable Organizational Authentication. When you do this, you will then be prompted to enter your domain (Azure AD) just as before. The other thing to note on this screen is the Databases section. Notice it is referencing the SQL Database in Azure that was created previously. The connection string in the web.config will be updated in Azure (not locally) to read the settings I mentioned earlier from the SQL Database. Again, more on this in my next post.When you click the Publish button, the MVC application will be published to your Azure Website as expected. Also, an additional application is registered in your Azure Active Directory during this step. Let’s review.The application (for .azurewebsites.net) registered in Azure ADThe first time you publish the application to Azure Websites, Visual Studio will register what I call the cloud version of the application. Meaning, this application and all of its configuration is in place to support running your MVC application on Microsoft Azure Websites (*.azurewebsites.net). It’s the same MVC application. However, from the perspective of Azure AD, the version you run on localhost during development is an entirely different application than the one that runs in Azure. That is why you will have two applications in Azure AD after publishing.If you were to FTP into the website or use Site Control Management (&amp;lt;yoursitename&amp;gt;.scm.azurewebsites.net) to look at the web.config file for the application in Azure, you would find that Visual Studio took care of all the details necessary to connect to the SQL Database in Azure to get the information it needs to identity your Azure AD tenant, configure Windows Identity Foundation (WIF) to validate incoming tokens, and handle the details of the underlying WS-Federation protocol.Run the application in AzureNow that the application is published, I can open a browser and navigate to the URL (https://azureadsamplewebsite.azurewebsites.net in this case). I’m immediately challenged to sign-in just as before when I was running on localhost. I’ll sign in as App User which is just a typical user in my organization’s active directory.As expected, I’m signed in and identified as App User in my application.Wrapping upThis blog post was strictly about the developer experience and understanding the basics of what is required to be successful when creating cloud applications that will authenticate users using Organizational Accounts in Azure Active Directory. In subsequent posts, I’ll go deeper into the automation that is happening in this scenario and talk more about the database dependency and even how you can remove it (if you want).Cheers!Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Web Site Affinity with Windows Azure Traffic Manager", "url": "/2014/03/25/web-site-affinity-with-windows-azure-traffic-manager/", "categories": "", "tags": "", "date": "2014-03-25 16:06:04 -0500", "snippet": "In my previous post I talked about using Windows Azure Web Sites and Traffic Manager together to achieve load balancing across web site deployments in different regions. Having Traffic Manager support for Windows Azure Web Sites is a great addition to the Windows Azure platform that was recently announced by Microsoft.In this post, I’m going to dig into this a little deeper and share some observations and recommendations when using Traffic Manager with Windows Azure Web Sites.ARR Affinity and Web SitesWindows Azure Web Sites, by default, use an ARRAffinity cookie to insure subsequent requests from a user are routed back to the web site instance that the user initially connected to. In other words, Windows Azure Web Sites assumes your web site is not stateless. If you were to scale your web site deployment to multiple instances, the ARR Server processes this ARR Affinity cookie and sends you back to the instance that processed your initial request (the request that originated the cookie).If you’re not already familiar with this, check out this blog for an in-depth discussion on ARR Affinity for Web Sites and how to disable ARR Affinity if your web site is stateless.ARR Affinity and Traffic ManagerNow, assuming the default ARR Affinity behavior for web sites, let’s look at this in a scenario where Traffic Manager is providing ##Round Robin Load Balancing## across web site deployments in multiple regions. For this post, I’m continuing with the 3 web site deployments from my previous post, which are in the West, North Central, and East regions.You can see this ARRAffinity cookie using the developer tools feature (press F12) in Internet Explorer. Below is a new browser session where I browsed to contoso-ws-trafficmanager.net and as part of the response, I got an ARRAffinty cookie back.Now, in subsequent requests, this cookie is provided in the request headers so ARR can properly route the request back to the instance (not region) that serviced the first request.Assuming my DNS entries on my client don’t change, I’ll get back to the West region on a subsequent request and the ARR Server will use this cookie to make sure I get back to the right instance in that region.Now, when you add Traffic Manager (Round Robin or Performance) for your web site deployments, there is a risk that you will not get back to the same instance because Traffic Manager may resolve you to a different region entirely.To demonstrate, notice above that my ARR Affinity cookie (…0112) was issued in my initial request which was serviced by the West deployment. In that same browser session, after some time had passed, I refreshed my browser. As expected, the cookie is passed in the request headers. But, this time my request resolved to the North deployment.Clearly this can be problematic if my web site is not stateless, which as I said at the beginning, is an assumption made for Windows Azure Web Sites and the reason for the ARR Affinity cookie.Now, looking at the cookies for this last request I can see that the instance in my North deployment issued a new ARRAffinity cookie (…5069).It did this because it does not have a mapping for the first cookie (…0012) to an instance of my web site in the North. The ARR server in the West knows about this cookie, but not the one in the North. So, a new cookie was issued.Waiting again for a few minutes, I just happened to get routed back to the West deployment. On this request, the 2nd cookie (…5069) that I got from the North was sent in the request. Of course, the instance in the West didn’t recognize it as valid so it issued a new cookie, which happens to be the first cookie (…0112) because I only have the one instance in that region.So, I have two deployments, West and North, servicing one user. If my web site is not stateless this has the potential of creating some big problems for me.Traffic Manager and Web Sites with state/sessionIf your web site is not stateless, then your use of Traffic Manager should be limited to the Failover load balancing method. Otherwise, you risk having session data for a single user in multiple regions as I’ve demonstrated above.Traffic Manager and stateless Web SitesIf your web site is indeed stateless, then you don’t have any risk other than your load balancing at each region not being as optimal as it could be. So, disabling ARR Affinity for stateless web sites is a good idea and this blog shows a couple of ways this can be done. For my sample here, I chose this approach.Hopefully this helps you get the most out of using Traffic Manager with Microsoft Azure Web Sites.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Windows Azure Web Sites and Traffic Manager", "url": "/2014/03/22/windows-azure-web-sites-and-traffic-manager/", "categories": "", "tags": "", "date": "2014-03-22 16:06:04 -0500", "snippet": "Last week Microsoft announced Traffic Manager support for Windows Azure Web Sites. This is a fantastic feature that enables you to deploy your web site to multiple data center regions, resulting in improved availability, performance (for users of your web site), maintenance capabilities, and failover support for your web site. In this post, I’ll demonstrate how to setup and configure Traffic Manager for Windows Azure Web Sites.If you’re not already familiar with what Traffic Manager is, then check out the Traffic Manager Overview before proceeding. It’s a really simple, yet very powerful feature that will take just a few minutes to understand. If I had to sum it up in one sentence, I’d describe it as simply a global load balancer for your web site or cloud service with some built-in configuration for common load balancing methods.WebSite DeploymentsTo start with, I created three web sites. One in West US, North Central US, and East US.Traffic Manager requires that the web sites be in Standard mode so each of these were configured as such.To simplify my sample, I also added a “Region” application setting to each web site that I reference in code (later) to show which region Traffic Manager resolved me to.Using Visual Studio, I created a very simple ASP.NET MVC Web Application (default template) and changed the Index view to just print out the value of the “Region” application setting.Next, I downloaded the PublishSettings file for each web site and published the MVC Web Application to each of the 3 web sites. Here is the sample output for contoso-ws-west.azurewebsites.net.Adding Traffic ManagerThe first step here is creating a Traffic Manager profile.When doing so, you will need to come up with DNS prefix that is globally unique. Keeping with my web site naming convention, I chose contoso-ws.trafficmanager.net. I left the LOAD BALANCING METHOD at it’s default (Performance) which I’ll cover in more detail shortly.Adding the endpoints to the Traffic Manager profileAdding the web site endpoints is just a matter of a few clicks. In the ENDPOINTS section of the Traffic Manager profile, click on the Add Endpoints link and you get a window where you can specify the SERVICE TYPE (Web Site or Cloud Service) and click the check box next to the endpoints you want added to the Traffic Manager profile.Within a few seconds the ENDPOINTS page indicates the 3 web sites and their status. A status of Online means that Traffic Manager got a successful response (HTTP 200) the last time it issued an HTTP GET to the site, which it will do frequently from this point on. To understand the details of how Traffic Manager monitors the endpoints, check out this article.With the endpoints online, I opened a browser and navigated to contoso-ws-trafficmanger.net and as you can see by the output, it resolved me to the East region. Why East? Because currently my Traffic Manager profile is set to load balance for Performance and since I’m in Dallas, TX at the time I’m writing this, Traffic Manager has determined that this region will be faster for me than going to the West or North Central region. This could change, and in fact it did. A couple of times I got a response from North Central.Disabling and Enabling EndpointsOn the ENDPOINTS page in the portal, an endpoint can be disabled by simply highlighting it and clicking the DISABLE link at the bottom of the screen. Notice the STATUS changes to Disabled.When I open a new browser window and navigate to contoso-ws-trafficmanager.net, Traffic Manager routes me to the North Central deployment since East has been disabled. NOTE: East is still running. Traffic Manager is just not taking it into consideration when resolving the request.When I click the ENABLE link for the East location and open a new browser window, my requests start resolving back to East (most of the time).Shutting down and starting a Web SiteIf I completely shut down one of the web sites (East for example), Traffic Manager will detect this the next time it sends it’s health probe request. Notice in this case the STATUS changes to Stopped.When I start the web site back up, Traffic Manager will update East to Online again because it will be getting HTTP 200 (Ok) back on it’s health probe requests.Configuring Traffic ManagerIn the CONFIGURE page, there are basically 3 groups of settings; general, load balancing method, and monitoring.The DNS NAME is the Traffic Manager Domain and is what you would point your company domain to using a CNAME resource record. For example, in this case, if I owned contoso.com, I would add a CNAME record pointing to contoso-ws.trafficmanager.net and any requests to contoso.com would then resolve to one of my 3 web site endpoints via Traffic Manager.The only setting you can change in this section is the DNS TTL. It defaults to 5 minutes and this defines how frequently a client will query Traffic Manager for updated DNS entries. Updates could be an endpoint being disabled, stopped, or just not functioning. Unless the client opens a new browser instance, it will continue to use the DNS entries for this period of time. Generally, you probably will want to leave this setting at 5 minutes.Load Balancing MethodThe Load Balancing Method can be either Performance, Round Robin, or Failover.Performance Load Balancing is what I demonstrated above and simply means that depending on where a request is coming from, Traffic Manager is going to resolve to a region closest to the user (generally). So, if you’re in CA, then you would resolve to contoso-ws-west. If you’re in NC, then you would resolve to contoso-ws-east. These are general rules. As I mentioned earlier, sometimes my requests from TX resolved to contoso-ws-north, but most of the time my requests resolved to contoso-ws-east.Round Robin Load Balancing is simply going to route requests equally across all 3 web sites. The idea here is to distribute load equally across all the deployments.Failover Load Balancing gives you a way to specify the priority for all traffic. For example, below I have North Central first in the list (by using the up/down arrows to the right) . What this means is that the North Central deployment is my primary endpoint and all traffic will go to this endpoint. If / when that site is stopped, disabled, or not functioning, then Traffic Manager will send all traffic to the East deployment, and so on. When North Central comes back online, Traffic Manager will start resolving all traffic back to North Central.MonitoringIn the monitoring settings, you are able to customize how Traffic Manager probes your web site endpoints to determine it’s availability for load balancing.The protocol setting allows you to specify HTTP or HTTPS. This could be useful if you have a custom probe page that you use to determine the health of your web site that may return some confidential information. Telling Traffic Manager to issue the requests over HTTPS will insure that content is encrypted on the wire.The port setting is useful in cases where you want to keep your Traffic Manager health probes separate from regular web site traffic.The relative path and file name setting defaults to the default page for your web site. Generally this is probably not a good idea. A good practice here is instead to have a page that properly checks the health of your web site by verifying database connections, web service endpoints your web site depends on, perhaps even checking some performance counters (standard or custom). In other words, just because the default page returns successfully may not mean your web site is fully working. In which case, you don’t want Traffic Manager sending traffic to an unhealthy instance.SummaryIn this post I showed you how you can use Windows Azure Traffic Manager with Web Sites to improve availability for your site. In my next post, I’ll provide some additional guidance you should be aware of when using Traffic Manager with Windows Azure Web Sites.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Continuous Deployment (GitHub) with Azure Web Sites and Staged Publishing", "url": "/2014/01/21/continuous-deployment-github-with-azure-web-sites-and-staged-publishing/", "categories": "", "tags": "", "date": "2014-01-21 15:06:04 -0600", "snippet": "With Windows Azure Web Sites you can setup continuous deployment (CD) to publish your site directly from source control as changes are checked in. This is a fantastic automation feature that can be leveraged from a range of source control tools such as Visual Studio Online and others (GitHub, BitBucket, DropBox, CodePlex, or Mercurial) .With the recent preview release of Staged Publishing for Windows Azure Web Sites, you can now configure CD to deploy directly to a staging environment where you can do final testing before swapping to your production environment. If you’re familiar with Cloud Services, then this is similar to the staging and production environments of that execution model.Together, continuous deployment and staged publishing with Windows Azure Web Sites provides a powerful solution for getting quality changes and new features to market quickly. In this post I’m going to walk through setting up continuous deployment from a GitHub repository to a Web Site with Staged Publishing enabled.Create an Azure Web SiteThere are many ways to do this but I will start from Visual Studio’s Server Explorer by right-clicking on Web Sites and selecting Add New Site .Next, I’ll give my site a name and click Create.Set Web Site Mode to StandardBefore you can enable Staged Publishing for a web site, you first need to set your web site to Standard mode (previously known as Reserved) if it is not already. Staged Publishing is only available in Standard mode. This can be set in the Windows Azure Portal on the SCALE tab for the web site.Enable Staged PublishingTo enable Staged Publishing for the web site, click on the Enable staged publishing link in the quick glance section of the DASHBOARD for the site.When you do this, Windows Azure essentially creates a new web site that will serve as the staging environment. You will see the staging site under the web site with the same name, but with “(staging)” appended. Notice also the URL of the staging site. This is the URL you will use to test your web site prior to swapping it to production.Setup Publishing from Source ControlFor this step, I previously created a new repository in my GitHub account. The repository is empty (for now), but it has been created.Next, I went to the DASHBOARD for the staging environment and clicked on the link to Set up deployment from source control .I selected GitHub from the list of source control providers and then selected my repository I previously created. Note: Windows Azure has to authenticate you to access your repository so you may be challenged for your GitHub credentials at this step if it cannot authenticate you.The completes the steps necessary to setup CD to a web site’s staged environment. In a moment, I’ll create a web site to demonstrate these features working together. However, before I do, I need to step through a short little work around that at the time of this writing is necessary when working with the preview bits for Staged Publishing.Staged Publishing Preview WorkaroundThis section is a work-around for an issue with continuous deployment to web sites that have Staged Publishing (preview) enabled. A big THANK YOU to David Ebbo for this work-around.I went to the DASHBOARD for the production environment and clicked the link to Set up deployment from source control . Yes, it’s the exact same step as before, but this time for the production environment .Next, I selected GitHub from the list of source control providers and then selected my repository I had previously created (it’s the exact same repository).Remove the GitHub Service Hook Pointing at the Production EnvironmentFor this, I navigated my browser to my GitHub repository and clicked on the Settings icon on the lower-right corner of the page.Next, on the upper-left corner of the page, I clicked on Service Hooks. Notice there are two WebHook URL’s indicated. That’s because I setup publishing from source control on both the staging and production environments.Click on the WebHook URLs (2) link to see the two URL’s. Locate the production URL (the one that is not the staging URL) and click the remove link.Finally, click the Update settings button to save the changes.You may be asking yourself, why setup publishing from source control to the production environment if all you do is then delete the service hook for it in GitHub? I wondered that too. By doing this, there apparently are some settings in Windows Azure Web Sites that gets triggered/applied as a result of initially setting up publishing from source control. I don’t know the details. However, I can tell you what happens if you don’t do this.If you don’t apply this work around, then this is the experience you would have. Assume that you’ve deployed v1.0 to staging and that production is empty. Now, swap the two environments. After the swap, staging is empty and production has v1.0. Push v1.1 to GitHub. The CD from GitHub won’t occur. Why? Because the service hook is referencing staging and you swapped the staging and production environments. So, whatever setting is needed for CD to occur is currently in the production environment. Now, swap the two environments again (in other words – back to their original state) so that staging is back to v1.0 and production is empty. Push v1.2 to GitHub. This time CD from GitHub will occur.So, basically, you have to swap the environments back to their original state for CD to work. Of course, that defeats the purpose of Staged Publishing. Hence, the reason for the work around.I look forward to the day when I can delete this section from the blog. Until then, doing these few extra steps will deliver the CD experience you would expect when your site has Staged Publishing enabled.Let the Continuous Deployment Cycle BeginNow that I’ve got CD all wired up to my GitHub repository and Staged Publishing enabled on my web site, it’s time to take this for a spin.In Visual Studio, I cloned my GitHub repository so that I have it locally on my machine. Next, I created an ASP.NET Web Application project using the default project that the Visual Studio template creates and specified the path of my local repository to store the project in. If this is not something you’re familiar with, check out my post Visual Studio and GitHub: The Basics of Working with Existing Repositories to see essentially the same steps. The only change I made to the project is a small update to the index.cshtml file to show a version number for the purpose of this blog.Push v1.0 to GitHubIn Team Explorer, I added a description (just the version number) and clicked the Commit button to commit the change to my local copy of the repository.Next, I pushed the change to my remote GitHub repository. It’s this step that kicks off the continuous deployment from my GitHub repository to my Azure Web Site.In the Windows Azure Portal, I navigated to the DEPLOYMENTS page for the staging environment and I can see that v1.0 was successfully deployed.Clicking on the BROWSE button at the bottom of this page, I can see v1.0 of the application running in the browser. Notice the URL is the staging URL.Navigating to the production URL, I see the standard empty site page.Swap the Staging and Production EnvironmentsIn the Windows Azure Portal, the DASHBOARD for either staging or production now has a SWAP button at the bottom of the screen. Clicking this raises a dialog to confirm I want to perform the swap and some helpful information about what settings change and what stay the same.After the swap is complete, which takes just a second or two, I refreshed my browser where my application is running. Now v1.0 is in the production environment.And as expected, the empty site is now in the staging environment.Push v1.1 to GitHubTo further illustrate these great features and to point out a couple of observations (which I’ll get to later), I changed the version on the index.cshtml page to 1.1 and committed the change. Just like before.And then pushed the change to my remote GitHub repository which again kicks off the continuous deployment from my GitHub repository to my Azure Web Site.In the Windows Azure Portal, I navigated to the DEPLOYMENTS page for the staging environment and I can see that v1.1 was successfully deployed. Notice that v1.0 is not in the deployment history.However, if I go over and check the DEPLOYMENTS page for the production environment I see that v1.0 is in the deployment history.So, an observation to note here is that the deployment history follows the environment . At least for now. Personally, I would like to see the full deployment history regardless of which environment I’m in. But, hey, this is preview so maybe that’s coming. SmileRefreshing my browser where my application is running in the staged environment, I see v1.1 is deployed as expected.And v1.0 is still running in production, just as before.Swap the Staging and Production EnvironmentsFinally, I performed one last swap and as result, v1.1 is now running in the production environment.And as expected, v1.0 is now running in the staging environment.Some Final ThoughtsI wanted to briefly mention a previous blog from the Windows Azure Team titled Windows Azure Web Sites: How Application Strings and Connection Strings Work . The reason why is connection strings and application settings are common things to change as you transition from one environment to another. With Staged Publishing now available, the value of these features, while cool before, just became even more important in my opinion because you can define connection strings and application settings appropriate for each environment. If you’re not using these features today I would suggest you do – especially if you want to get maximum benefit from continuous deployment and staged publishing.In this post I demonstrated how to setup continuous deployment from a GitHub repository to a Windows Azure Web Site with Staged Publishing enabled. I also showed the steps necessary to work around a little issue currently present in the preview bits for Staged Publishing.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Auto Scaling Cloud Services on CPU Percentage with the Windows Azure Monitoring Services Management Library", "url": "/2013/12/15/auto-scaling-cloud-services-on-cpu-percentage-with-the-windows-azure-monitoring-services-management-library/", "categories": "", "tags": "", "date": "2013-12-15 15:06:04 -0600", "snippet": "The auto scaling feature in Windows Azure is a fantastic feature that enables you to scale your services dynamically based on a set of rules. For example, in the case of Cloud Cervices, if CPU exceeds a defined threshold in your rule, auto scaling will add additional instances to handle the increased load. When CPU drops below a defined threshold, auto scaling will remove those extra instances. It’s this kind of elasticity that makes cloud computing so compelling.Auto scale (currently in preview) is available for Cloud Services, Websites, and Virtual Machines. The metric for scaling on these different compute options can be based on CPU or Queue Depth as shown here.Auto scale is also available for Mobile Services with some different metrics (perhaps a topic for a future post).How to Scale an Application covers some common scenarios for configuring auto scale using the Windows Azure Management Portal. However, in this post, I’ll show how you can achieve auto scaling using the Windows Azure Monitoring Services Management Library (also in preview) instead. For the sake of time, I’m only covering Cloud Services and auto scaling based on CPU. Otherwise, this would be a massively long post. This is important because some of what I talk about does not apply to the other compute options.Overview of my Cloud Service ApplicationTo set the stage for the application I’ll be configuring for auto scale, it is a cloud service consisting of two Web Roles configured for just 1 instance. An MVC Web Front End (the UI). This role defines the single public endpoint for the cloud service that is accessed through the browser. It simulates, in a very elementary manner, job processing. You can do two things with the UI – see status of jobs and submit a new job. When a job is submitted, it looks for available Web API instances running and round-robin posts to them through an internal endpoint. An MVC Web API. This is where the job processing is done and it consumes a lot of CPU cycles. Therefore, it is this role that I will define auto scaling rules for.Introduction to Auto Scale ProfilesWhen you define auto scale settings, you’re essentially defining what’s called an AutoScaleProfile. An AutoScaleProfile contains rules, each of which has a MetricTrigger and a ScaleAction. The MetricTrigger identifies a resource (CPU for example) and some parameters for how the metric is measured and evaluated. The ScaleAction defines what to do when the conditions for the metric have been met (scale up/down for example). Basically, you have something like this.This is by no means complete . It does, however, show the relationship of some basic constructs that are used in the API’s.Benefits of using Windows Azure Monitoring Services Management LibraryBy using the API’s, you can take advantage of features that may not otherwise be accessible through the portal. Here’s a list of features I came up with that I found useful. The ability to create multiple profiles. Up to 20. So, for example, I may define a profile to scale to higher capacity during one time period and another profile that scales to lower capacity during a different time window. The recent cyber-Monday comes to mind. Or, perhaps one profile during the week and another for weekends. The ability to define multiple profiles allows for some interesting possibilities. The ability to set the TimeWindow property to a value other than the default of 45 minutes. The TimeWindow is a property in the MetricTrigger I mentioned above that specifies the range of time in which instance data is collected and evaluated, meaning that, by default, 45 minutes of monitoring data will have to have been collected before a MetricTrigger will result in a ScaleAction being performed. Now, you can set this value as small as 5 minutes. However, I would strongly caution against it for reasons I’ll address later. What I’ve found to be a good value for this is 25 minutes based on my testing. The ability to change how the statistics for a metric are evaluated across role instances. By default, they are averaged, but I could choose to use minimum or maximum values instead.There are other properties that can be set using the API’s that are not surfaced in the Azure portal. These are just a few I found interesting.How to use the library to Create/Update, Read and Delete Auto Scale SettingsThe library is available via Nuget here . If you’re looking for it in Visual Studio’s Nuget Package Manager, make sure to “Include Prerelease” when searching for “Microsoft.WindowsAzure.Management.Monitoring”.Instantiate an Instance of AutoscaleClientThe AutoscaleClient is the class you make use to create, read, or delete auto scale settings. To instantiate it, you need to provide appropriate credentials which can be generated from the usual suspects – Subscription Id and Certificate. This code uses a helper class to get the management certificate from the certificate store. It also sets some variables to identify the cloud service and role I’ll apply the auto scale settings to. The latter is known as the resourceId in subsequent code.// Cloud Service and Role to be auto scaled.var cloudServiceName = &quot;[YOUR CLOUD SERVICE NAME]&quot;;var isProduction = true;var roleName = &quot;[THE ROLE NAME TO SCALE]&quot;;// Azure Subscription ID and Management Certificate Thumbprint.string subscriptionId = &quot;[YOUR SUBSCRIPTION ID]&quot;;string certThumbprint = &quot;‎[YOUR CERTIFICATE THUMBPRINT]&quot;;// Get the certificate from the local store.X509Certificate2 cert = CertificateHelper.GetCertificate( StoreName.My, StoreLocation.CurrentUser, certThumbprint);// Genereate a resource Id for the cloud service and role.var resourceId = AutoscaleResourceIdBuilder.BuildCloudServiceResourceId( cloudServiceName, roleName, isProduction);// Create the autoscale client.AutoscaleClient autoscaleClient = new AutoscaleClient(new CertificateCloudCredentials(subscriptionId, cert));Create an Autoscale ProfileUsing the psuedo-code above as reference, the code here shows how to create a profile. Notice there are some other settings here like ScaleCapacity where you can define minimum, maximum, and default capacity limits for scaling.AutoscaleSettingCreateOrUpdateParameters createParams = new AutoscaleSettingCreateOrUpdateParameters() { Setting = new AutoscaleSetting() { Enabled = true, Profiles = new List&amp;lt;AutoscaleProfile&amp;gt;() { new AutoscaleProfile() { Name = &quot;Scale.WebApi.Cpu&quot;, Capacity = new ScaleCapacity() { Default = &quot;1&quot;, Minimum = &quot;1&quot;, Maximum = &quot;4&quot; }, Rules = new List&amp;lt;ScaleRule&amp;gt;() } } } };Create a ScaleRule to Scale UpAgain, using the psuedo-code above as a reference, this code creates the first of two ScaleRule instances. The rule contains a MetricTrigger and a ScaleAction to scale up the instances (WebApi in this case) when the average CPU exceeds 40% across all instances.A few things to point out about this code are… The MetricName and MetricNamespace are not values I just made up. These have to be precise. You can get these values from the MetricsClient API and there is some sample code in this link to show how to get the values. The TimeWindow is set to 25 minutes. As I indicated earlier, the default for this is 45 minutes. My experience has been intermittent with a value of 20 minutes or less. So, I recommend at least 25 minutes for this value. I’ll come back to this later.var cpuScaleUpRule = new ScaleRule(){ // Define the MetricTrigger Properties MetricTrigger = new MetricTrigger() { MetricName = &quot;Percentage CPU&quot;, MetricNamespace = &quot;&quot;, MetricSource = AutoscaleMetricSourceBuilder.BuildCloudServiceMetricSource(cloudServiceName, roleName, isProduction), TimeGrain = TimeSpan.FromMinutes(5), TimeWindow = TimeSpan.FromMinutes(25), TimeAggregation = TimeAggregationType.Average, Statistic = MetricStatisticType.Average, Operator = ComparisonOperationType.GreaterThan, Threshold = 40.0 }, // Define the ScaleAction Properties ScaleAction = new ScaleAction() { Direction = ScaleDirection.Increase, Type = ScaleType.ChangeCount, Value = &quot;1&quot;, Cooldown = TimeSpan.FromMinutes(10) }};Create a ScaleRule to Scale DownSimilar to the previous rule, this rule contains a MetricTrigger and ScaleAction to scale down the instances when the average CPU falls below 25% across all instances.var cpuScaleDownRule = new ScaleRule(){ // Define the MetricTrigger Properties MetricTrigger = new MetricTrigger() { MetricName = &quot;Percentage CPU&quot;, MetricNamespace = &quot;&quot;, MetricSource = AutoscaleMetricSourceBuilder.BuildCloudServiceMetricSource(cloudServiceName, roleName, isProduction), TimeGrain = TimeSpan.FromMinutes(5), TimeWindow = TimeSpan.FromMinutes(25), TimeAggregation = TimeAggregationType.Average, Statistic = MetricStatisticType.Average, Operator = ComparisonOperationType.LessThan, Threshold = 25.0 }, // Define the ScaleAction Properties ScaleAction = new ScaleAction() { Direction = ScaleDirection.Decrease, Type = ScaleType.ChangeCount, Value = &quot;1&quot;, Cooldown = TimeSpan.FromMinutes(10) }};Create or Update Auto Scale SettingsFinally, to create (or update existing settings), all that is left is to add the rules to the profile and call the CreateOrUpdate method to apply the settings for the resource.// Add the rules to the profilecreateParams.Setting.Profiles[0].Rules.Add(cpuScaleUpRule);createParams.Setting.Profiles[0].Rules.Add(cpuScaleDownRule);// Apply the settings in Azure to this resourceautoscaleClient.Settings.CreateOrUpdate(resourceId, createParams);Get Auto Scale SettingsTo retrieve existing auto scale settings for a particular resource, you just need to call Get, passing in the resourceId. The AutoscaleSettingGetResponse will have all the goodness in it that we just went through to create the the settings.AutoscaleSettingGetResponse setting = autoscaleClient.Settings.Get(resourceId);Delete Auto Scale SettingsTo delete existing auto scale settings just call Delete, passing in the resourceId.var deleteResponse = autoscaleClient.Settings.Delete(resourceId);Observations of a Running Application Configured for Auto ScaleWith these auto scale settings in place, I decided to take my MVC and CPU consuming Web Api for a test drive to see this in action.I started by going to my MVC page and submitting a couple of “jobs” over the span of about 10 minutes. And as you can see here, my CPU is certainly climbing and quickly I’m above my 40% threshold.As expected, after 25 minutes of average CPU above 40%, I saw my WebApi role spin up a 2nd instance.Once the instance was ready, I threw a couple more jobs at it. The home page for my MVC front end displays the results of all the jobs and also which instance serviced (or is servicing them). My MVC front end is dispatching these jobs in round-robin fashion to the available WebApi instances.Continuing with this pattern, I saw another instance get created about 15 minutes later.And I continued to throw a few more jobs at the system. So, now I have 5 jobs being worked across 3 instances.The timeline for the scale up actions is below and shows when my instances were created and the effect they had on the average CPU for WebApi while I continued to throw jobs at the system.What I didn’t show is the scale down behavior that occurred after I stopped loading the system up with work. Over the course of another 20 minutes or so, the WebApi instances dropped back down to the single instance I started with.Caution About Setting the Time WindowWhen I first learned about the TimeWindow property and the default value of 45 minutes, I immediately elected to use the API’s to set this to as small a number as allowed by the API (which is 5 minutes). After all, if my service is getting hammered, I would prefer that it scale right away instead of wait for 45 minutes to do so. Unfortunately, when I did this I started seeing messages like this in the Azure Portal with no scale actions being invoked.What I’ve learned is that the gap between the current time and the last data point that you can see in the MONITORING page is part of that TimeWindow. So, consider for example the scenario below where it may be 8:43 but the last data point available was at 8:30. That’s a 13 minute gap in time. Defining a TimeWindow of 5 minutes would result in the warning above because there was no monitoring data points in the last 5 minutes ( between 8:38 and 8:43 ) when auto scale was trying to evaluate the MetricTrigger.This lag in monitoring data and current time is common and the lag time is unpredictable. It might be 10 minutes during part of the day and 20 minutes at another. In my testing, I’ve found that 25 minutes is a safe value for TimeWindow.So, experiment with this setting to find the value that works for you if you choose to deviate from the default of 45 minutes.Examining the Operating LogsAnytime you change auto scale settings an AutoscaleAction is invoked. You can see the details of such changes in the Azure Portal. Click on the MANAGEMENT SERVICES section. Next, click on a particular log and click on the DETAILS link at the bottom to see the changes. I’ve found this to be very helpful when testing out various profile settings.ConclusionThere is so much more to auto scaling that I’ve not even mentioned. As long as this post is, it’s hard to believe I’ve just barely scratched the surface. Hopefully this has helped you understand a little more about auto scale for cloud services, the Windows Azure Monitoring Services Management Library, and the benefits of using it to automate your auto scale settings.The Service Management REST API for Autoscaling provides documentation that at the time of this writing wasn’t available in the .NET API’s that I wrote about. Even if you don’t use the REST API’s, referring to them for documentation is a good idea.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "It works fine in the Windows Azure Emulator but fails when I publish to Windows Azure", "url": "/2013/12/11/it-works-fine-in-the-windows-azure-emulator-but-fails-when-i-publish-to-windows-azure/", "categories": "", "tags": "", "date": "2013-12-11 15:06:04 -0600", "snippet": "Today I ran into one of those situations where my cloud service worked just fine running locally in the Windows Azure Emulator but failed when I published to Windows Azure, resulting in this infamous page being rendered in my browser.It’s tempting to jump right into the various techniques for debugging cloud services. Especially the latest Remote Debugging feature. While effective, these techniques can be somewhat heavy handed approaches requiring you to publish a debug build instead of a release build or preconfigure some diagnostic settings. When I am doing basic acceptance testing, I prefer to do so using a release build. After all, that’s what I will ultimately deploy to production. I also tend not to enable any diagnostic settings by default. Again, not something I would do in production unless necessary.When troubleshooting an issue it’s generally a good idea to start with the least invasive approach and bring out the big debugging tools only if necessary. This is especially true if you are ever debugging a production environment.In this post, I’ll walk through the simple process I took to determine root cause. The issue is not really important. Well, maybe it is for some MVC developers out there. For this post though, it’s primarily about the process.So, I published my cloud service, consisting of an MVC Web Role and a Web API Web Role, and got the Runtime Error above.Enable Remote Desktop on the MVC RoleThanks to the service management extensions in Windows Azure, I went directly to the Windows Azure Portal. Then, I went to the CONFIGURE tab for my cloud service and clicked the REMOTE button at the bottom of the screen.In the Configure Cloud Service window, I enabled Remote Desktop (RDP) for just my MVC role.Remote Desktop into the MVC InstanceAfter about 3-4 minutes RDP was ready to go. Next, I went to the INSTANCES tab for my cloud service and clicked on the CONNECT button at the bottom of the screen.I logged in using my user name and password (from above) and went directly to the Windows Event Viewer where I quickly found the ASP.NET event I was looking for.The details for this event were all I needed to see.So that’s it – I’ve determined root cause in a matter of about 5-6 minutes. Just to re-deploy a debug build would have taken longer.The FixSo, it turns out that Microsoft.Web.Infrastructure is in the Global Assembly Cache (GAC) and that’s where it was loading from when I ran it locally. This was put there during my installation of Visual Studio.For this to run in Windows Azure, the fix is simple. Just add the Microsoft.Web.Infrastructure package using the Nuget Package Manager in Visual Studio.Build and re-publish the service and I’m done.Post-Mortem AnalysisI’ve published many MVC applications to Windows Azure successfully. Why was this one different? Well, I did create this application a little differently than past applications. For this one, I started with the Empty template and checked the option for MVC folders and core references.Ironically, the Microsoft.Web.Infrastructure reference is not included in the core references in this scenario even though the Microsoft.AspNet.WebPages depends on it.However, if I choose the MVC template (which most people do most of the time), then that package reference is there by default.ConclusionThe Windows Azure Platform has a rich set of features and tools for troubleshooting applications. However, start with the least invasive techniques first and build up. Sometimes you will get lucky and find your problem in just a few minutes like I did here.Cheers!Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Calling Azure Service Management REST API&#39;s from C#", "url": "/2013/10/17/calling-azure-service-management-rest-apis-from-c/", "categories": "", "tags": "", "date": "2013-10-17 16:06:04 -0500", "snippet": "IntroductionThe Management Portal for Windows Azure provides a nice user experience for many service management operations such as creating cloud services, storage accounts, virtual machines, virtual networks, and so on. If you want to automate such operations, then the Azure PowerShell Cmdlets are a great choice and you can find a plethora of examples on how to use them on my close friend Michael Washam’s blog. On the other hand, if PowerShell is not your thing and you would prefer a C# approach, then the Windows Azure Service Management REST API’s are available for you. Using the REST API’s gives you full control, but at the expense of some added complexity.In this post, I’m going to show how to use a few of these REST API’s in a way that you can re-use – much like you would a client SDK ( hmmm, that would be nice ). I’ll cover enough to demonstrate use of common HTTP verbs such as GET, POST and DELETE and show how to serialize complex structures the API’s use so you don’t have to create or parse XML.Solution OverviewTo demonstrate, I’ve limited the scope of my solution to just three C# libraries (DLL’s), two of which use a small part of the Service Management REST API’s for Cloud Service and Data Center Location operations.ServiceManagement is the base library for the two other libraries, providing basic requirements for any Service Manage REST API.As you can see, I also created Unit Test projects for each of these libraries so you can see how the different libraries might be used in client code. In similar fashion, the ServiceManagement.Tests project is the base for the other two test projects. Later, I will cover how to run the tests.Base Service ManagementThe ServiceManagement project defines a base class, BaseApi, that the other two projects ( and potentially future projects ) derive from. It encapsulates some basic things that all the REST API’s need such as a valid Azure subscription, a base URI for web requests, and an HttpClient that already has the Azure subscription’s management certificate added to the client certificates collection for authentication. It does a few other things, but these are the main parts.public class BaseApi{ private AzureSubscription azureSubscription = null; public BaseApi(AzureSubscription Subscription) { if (Subscription == null) { throw new ArgumentNullException( &quot;Subscription&quot;, &quot;Subscription parameter cannot be null.&quot;); } this.azureSubscription = Subscription; } // Base Uri for all Service Management APIs public virtual Uri ServiceManagementUri { get { return new Uri( string.Format(&quot;https://management.core.windows.net/{0}&quot;, azureSubscription.SubscriptionId)); } } // Returns a WebRequestHandler with the AzureSubscriptions // management certificate already in the client certificates collection. public WebRequestHandler RequestHandler { get { var handler = new WebRequestHandler(); handler.ClientCertificates.Add( this.azureSubscription.ManagementCertificate); return handler; } } // Returns an HttpClient instance using the RequestHandler for this instance. // Sets the base address and common headers for all services APIs. // Sets up the Accept header for xml format. public HttpClient HttpClientInstance { get { var httpClient = new HttpClient(this.RequestHandler); httpClient.BaseAddress = ServiceManagementUri; httpClient.DefaultRequestHeaders.Add(&quot;x-ms-version&quot;, &quot;2013-03-01&quot;); httpClient.DefaultRequestHeaders.Accept.Add( new MediaTypeWithQualityHeaderValue(&quot;application/xml&quot;)); return httpClient; } }}This project also defines an AzureSubscription class that holds the Azure Subscription Id and associated management certificate for the subscription. An instance of this class is required to instantiate BaseApi or any of it’s derived classes. To instantiate AzureSubscription you must provide a Subscription ID and Certificate Thumbprint.public class AzureSubscription{ private string subscriptionId = null; private X509Certificate2 certificate = null; public AzureSubscription(string SubscriptionId, string CertificateThumbprint) { if (string.IsNullOrEmpty(SubscriptionId)) { throw new ArgumentNullException(&quot;SubscriptionId is null or empty.&quot;); } this.subscriptionId = SubscriptionId; this.certificate = GetCertificate(CertificateThumbprint); } public string SubscriptionId { get { return this.subscriptionId; } } public X509Certificate2 ManagementCertificate { get { return certificate; } } // Looks for the certificate by thumbprint in the &quot;My&quot; certificate store. private X509Certificate2 GetCertificate(string thumbprint) { List&amp;lt;StoreLocation&amp;gt; locations = new List&amp;lt;StoreLocation&amp;gt; { StoreLocation.CurrentUser, StoreLocation.LocalMachine }; foreach (var location in locations) { X509Store store = new X509Store(&quot;My&quot;, location); try { store.Open(OpenFlags.ReadOnly | OpenFlags.OpenExistingOnly); X509Certificate2Collection certificates = store.Certificates.Find( X509FindType.FindByThumbprint, thumbprint, false); if (certificates.Count == 1) { return certificates[0]; } } finally { store.Close(); } } throw new ArgumentException(string.Format( &quot;A certificate with thumbprint &#39;{0}&#39; could not be located.&quot;, thumbprint)); }}With this base library in place, I’ll transition now to implementing the libraries that will call the various Service Management REST API’s.Operations on Data Center LocationsThe Operations on Locations is very simple with just one operation, List Locations. It returns a list of data centers available to your subscription and a list of available services for each data center. The request is a simple GET request that returns XML that could be serialized into a complex structure.Data Center Locations ModelTo support serialization into a structure I can code against, I defined three classes according to the REST API documentation for this operation.Locations contains a collection of DataCenterLocation’s and DataCenterLocation contains a collection of available Services at that data center. Pretty straight forward.What you have to be careful about are how the classes are named, the namespace, and the order of properties. If any of this is off then the data won’t serialize into the structure, even though it was returned in the underlying XML. This is where the DataContract and DataMember attributes come in handy. I decorated the classes with the necessary attributes according to the documentation so that the response will serialize correctly.[DataContract( Namespace = &quot;http://schemas.microsoft.com/windowsazure&quot;, Name=&quot;Location&quot;)]public class DataCenterLocation{ [DataMember(Order = 1)] public string Name { get; set; } [DataMember(Order = 2)] public string DisplayName { get; set; } [DataMember(Order = 3)] public Services AvailableServices { get; set; }}[CollectionDataContract( Namespace = &quot;http://schemas.microsoft.com/windowsazure&quot;)]public class Locations : Collection&amp;lt;DataCenterLocation&amp;gt;{}[CollectionDataContract( Namespace = &quot;http://schemas.microsoft.com/windowsazure&quot;, ItemName = &quot;AvailableService&quot;)]public class Services : Collection&amp;lt;string&amp;gt;{}Data Center Locations APINow that the model is defined, the implementation of the LocationsApi is quick and to the point.public class LocationsApi : BaseApi{ public LocationsApi(AzureSubscription Subscription) : base(Subscription) { } public override Uri ServiceManagementUri { get { return new Uri(string.Format(&quot;{0}/locations&quot;, base.ServiceManagementUri)); } } public Locations.Models.Locations List() { // Invoke REST API HttpResponseMessage response = this.HttpClientInstance.GetAsync(&quot;&quot;).Result; response.EnsureSuccessStatusCode(); List&amp;lt;MediaTypeFormatter&amp;gt; formatters = new List&amp;lt;MediaTypeFormatter&amp;gt;(){ new XmlMediaTypeFormatter() }; return response.Content.ReadAsAsync&amp;lt;Locations.Models.Locations&amp;gt;(formatters).Result; }}Although there’s not much to it, I’ll explain the main points. The implementation of LocationsApi derives from BaseApi which I covered earlier. So, it gets basic support needed for Service Management REST API’s. The ServiceManagementUri is overridden to return the correct Uri for operations specific to locations. The one operation defined, which is List, uses the HttpClient instance from the base class to invoke the REST API and return back the response as an instance of Locations using the ReadAsAsync extension method.Using the LocationsApi in Client CodeThis is an example of client code using the LocationsApi to get a list of data center locations.locationsApi = new LocationsApi(this.subscription);var locations = this.locationsApi.List();// Print out the locations collection.foreach (DataCenterLocation loc in locations){ Console.WriteLine(&quot;Name: {0}&quot;, loc.Name); Console.WriteLine(&quot;Display Name: {0}&quot;, loc.DisplayName); Console.WriteLine(&quot;Services:&quot;); foreach (string service in loc.AvailableServices) { Console.WriteLine(&quot;\\t{0}&quot;, service); } Console.WriteLine();}And here is the output for one of the six data centers returned for my subscription.Personally, I like this a lot better than parsing XML.Operations on Cloud ServicesThe Operations on Cloud Services is a little more involved and has several operations. For the sake of this blog, I’m just going to focus on two operations; Create Cloud Service and Delete Cloud Service. I chose these because they provide small examples of API’s using POST and DELETE http verbs.Create Cloud Service ModelThis time, I’m defining a model that I can POST to the REST API’s to create a cloud service. So, continuing in similar fashion, I defined a class, CreateHostedService, to support the minimum requirements needed to create a cloud service. Notice that I said “minimum”. These API’s can get extremely complex quickly. For the sake of demonstration, I’ve limited my code to handle just these minimum requirements. Feel free to contribute to my project on github if you want to build this out further.[DataContract(Namespace = &quot;http://schemas.microsoft.com/windowsazure&quot;)]public class CreateHostedService{ [DataMember(Order = 1, IsRequired=true)] public string ServiceName { get; set; } [DataMember(Order = 2, IsRequired = true)] public string Label { get; set; } [DataMember(Order = 3, IsRequired = true)] public string Location { get; set; }}Cloud Services APIHere again, a pretty trivial implementation with everything else in place.public class CloudServicesApi : BaseApi{ public CloudServicesApi(AzureSubscription Subscription) : base(Subscription) { } public override Uri ServiceManagementUri { get { return new Uri(string.Format(&quot;{0}/services/hostedservices&quot;, base.ServiceManagementUri)); } } public Uri Create(string ServiceName, string Location) { // Create the request body byte[] serviceNameBytes = System.Text.Encoding.UTF8.GetBytes(ServiceName); CreateHostedService requestBody = new CreateHostedService() { ServiceName = ServiceName, Label = Convert.ToBase64String(serviceNameBytes), Location = Location }; // Invoke REST API call HttpResponseMessage response = this.HttpClientInstance.PostAsXmlAsync&amp;lt;CreateHostedService&amp;gt;(&quot;&quot;, requestBody).Result; response.EnsureSuccessStatusCode(); return response.Headers.Location; } public void Delete(string ServiceName) { // Invoke REST API call HttpResponseMessage response = this.HttpClientInstance.DeleteAsync( string.Format(&quot;{0}/{1}&quot;, this.ServiceManagementUri, ServiceName)).Result; response.EnsureSuccessStatusCode(); }}Like before, here are the main observations in this class library. The implementation of CloudServicesApi derives from BaseApi which I covered earlier. So, it gets basic support needed for Service Management REST API’s. The ServiceManagementUri is overridden to return the correct Uri for these operations specific to cloud services. The Create method constructs a CreateHostedService instance, populates it with required data to create the service. Then, it uses the HttpClient instance from the base class to invoke the REST API, passing the CreateHostedService instance in the request body using the PostAsAsync extension method. The Delete method is about as simple as it gets. Just use DeleteAsync to issue an http DELETE to the appropriate Uri for the cloud service to be deleted.Using the CloudServicesApi in Client CodeThis is an example of client code using the CloudServicesApi to create a new cloud service and then delete it.// Set name of new cloud service.var name = Guid.NewGuid().ToString();// Set location of new cloud service to first US location // found available in the current subscription.var locationsApi = new Locations.LocationsApi(this.subscription);var locations = locationsApi.List();string location = null;foreach (DataCenterLocation dcLocation in locations){ if (dcLocation.Name.EndsWith(&quot; US&quot;)) { location = dcLocation.Name; break; }}// Create a cloud service.Console.WriteLine(&quot;Creating cloud service &#39;{0}&#39; in data center &#39;{1}&#39;.&quot;, name, location);cloudServicesApi = new CloudServicesApi(this.subscription);var cloudServiceUri = this.cloudServicesApi.Create(name, location);Console.WriteLine(&quot;Cloud service created.&quot;);Console.WriteLine(cloudServiceUri);// Delete the same cloud service.this.cloudServicesApi.Delete(name);Console.WriteLine(&quot;Cloud service deleted.&quot;);Notice in this code that I’m also making use of the LocationsApi to pick a data center that’s available for my subscription and in the US.And here is the output. I know the name is not very user friendly, but at least it’s unique!So, there you have it. A few examples using Service Management REST API’s from C# using models to serialize and de-serialize complex structures. Extending this to support more complex API’s is largely going to come down to defining the models. The code to call the REST API’s is pretty short when you have a model to pass to and from the library.I’m going to wrap things up now with a quick review of the Test Projects.Running the Test ProjectsIf you know me or have read previous posts, you know I don’t like writing client applications just for the sake of testing other code. So, I’ll try and use test projects when possible to avoid this. Plus, it’s just good development practice to do so.Configure the Unit TestsAll that is needed to configure this for your environment is to replace the subscriptionId and certThumbprint in the BaseApiTest.cs file.As I indicated earlier, all the test projects derive from this BaseApiTest class (just like the REST API’s projects). So, once you set this all the other test projects are good to go.You can find these values in your Windows Azure Portal in the Settings tab. The management certificate must also be installed in your personal certificates store in either CurrentUser or LocalMachine.Run the Unit TestsFirst, build the solution. Next, to run the Unit Tests, select **TEST Windows Test Explorer** from Visual Studio 2013’s menu. This will open the Test Explorer window where you should find all the tests I’ve provided so far. Click the Run All link to run all the tests or right-click on a single test to run just that test. View the Output of the Unit TestsMost of the tests I’ve written just write data to the Console as you saw previously. You can view this output by clicking on the test you’re interested in and looking for the Output link.If you don’t see an Output link for a test, it’s just because I didn’t write anything to the console in the test code.ConclusionIn this post I showed how to program against the Service Management REST API’s using C#. I gave examples of how you can build models to pass data to/from the API’s that will make programming against them in client code simpler and more intuitive.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Securing a WCF Service in an Azure Web Role with HTTP &amp; TCP Endpoints", "url": "/2013/09/18/securing-a-wcf-service-in-an-azure-web-role-with-http-tcp-endpoints-2/", "categories": "", "tags": "", "date": "2013-09-18 16:06:04 -0500", "snippet": "IntroductionIn my last post, I showed how to host a WCF Service in an Azure Web Role with http and tcp endpoints. In this post, I will wrap things up by showing how to secure these endpoints using a self-signed certificate that can be used for TLS/SSL during development and testing.Before I can start to update the application, there are a few necessary steps: Create a Self-Signed Root Authority Certificate (CA). This is generally something you only want to do once. For example, when you’re setting up your development environment for the first time. Once you have a root certificate authority (CA), you can use it to issue additional test certificates signed by this CA. When I create a new development machine, this is one of the first things I typically do. Create a Test Certificate using the Self-Signed Root CA. This is generally something you do for each application (or service in this case). The reason is that you want the subject of the certificate set to the URL of the application so you don’t get certificate trust errors (red bar in IE for example). Upload the Test Certificate to your Cloud Service. This is necessary if you intend to publish your cloud service from Visual Studio. If the SSL Certificate for your https input endpoint doesn’t already exist in your cloud service, then Visual Studio will raise an error instructing you to first upload the certificate (.pfx) file to your cloud service.Much has been written about each of these steps, and you can achieve each by following the instructions here and here. Just reading all of this will take you several minutes and will navigate you through command prompts and Windows Azure Portal experiences to complete the task. Alternatively, you can do it all in about 30 seconds with this handy little (PowerShell script)[https://github.com/rickrain/WCF-In-Azure-WebRole/blob/master/New-CloudServiceSSLCert.ps1] I wrote.The script requires that you have the Azure PowerShell Cmdlets installed and also have a Default Azure Subscription configured for the Azure PowerShell Cmdlets.Now, let’s get started…Run the script to do all that stuff up there for me!The script is thoroughly documented so I’ll leave it to you to review the details on what it does.To run the script, you just need to edit a couple of variables at the very top, specifically the caSubjectName and the serviceName. Optionally, you can change the serviceLocation to an Azure data center location closer to you. Finally, you may also need to adjust the path to makecert.exe for your machine by updating the makeCertPath. I am using the PowerShell ISE to edit and run the script as I write this. Note: You must run PowerShell (or PowerShell ISE) as an Administrator for this script to run.Click the Save button and then the Play button to run the script and you’re on your way.Here is the output when I ran it on my machine. Notice the very last line where it prints out the thumbprint to use to configure the https input endpoint for my Web Role. I’ll come back to this a little later.Although the output is pretty clear what just took place, here is some visual evidence to illustrate what this script just did.It created a self-signed root authority certificate and placed it in the Cert:\\LocalMachine\\Root store.It created a test certificate issued by the self-signed root CA and placed it in the Cert:\\LocalMachine\\My store.It created a new cloud service for my application since one didn’t already exist.It uploaded the certificate that I will use for SSL to the certificates section in Azure for my cloud service.Now that that’s all done, I can move onto the application changes needed to secure these endpoints.Configure Web Role EndpointsThe first thing is to add the test certificate to the Certificates tab in the Web Role properties. I named mine “SSL Cert”.The thumbprint should match the thumbprint that the PowerShell script printed out earlier.Next, change the http endpoint to use https, port 443, and specify the SSL Certificate Name from the previous step.The tcp endpoint doesn’t require any changes here. Instead, the necessary changes are added in the WCF configuration to secure the netTcpBinding. This is because of how transport security in WCF works when using the netTcpBinding. I’ll get to that in the next section.Secure the WCF Endpoints in Web.ConfigThe differences in the web.config for the Web Role between this post and the last one are highlighted and annotated here. There are no code changes needed. Just update the configuration as I’ve shown here.That’s all there is to it. Build and publish the service and you are ready to consume it from a client application.Test The ServiceTo test the service, I’ll go back to using the WCF Test Client because I want to highlight a problem I see many people running into.When you run the WCF Test Client with these secure endpoints, you will see an error when you try and consume the service over the tcp endpoint (http will work just fine). The error message is actually pretty helpful though.It is true that WCF’s transport security and the WCF Test Client are a bit stricter when it comes to insuring proper security is applied. This is a good thing (but can be frustrating at times). It turns out that self-signed certificates created using makecert.exe don’t have a certificate revocation list (CRL). Since there is not one, the WCF Test Client is complaining because by default, it wants to check the CRL before establishing the connection. So, how do you fix this so you can still use the WCF Test Client?In the WCF Test Client, you can edit the config file that the test client generated. Simply right-click on the Config File node and select the option Edit with SvcConfigEditor. Add an Endpoint Behavior ( I named mine NoCRLCheck ). For the behavior, add a clientCredentials element, navigate to the **serviceCertificate authentication** node and change the RevocationMode to NoCheck. Next, wire-up the endpoint behavior to the client tcp endpoint.Save and close the WCF Service Configuration Editor. When you do, the WCF Test Client will detect there was a change and prompt you to reload the service with these new settings. Click on Yes.With this configuration, we’re simply telling the WCF Test Client not to bother checking the CRL for this service. After all, it is just a test client!Now you will be able successfully call the service securely over http and tcp using the WCF Test Client.Remove the Test CertificateWhen you are finished with your cloud service development and want to remove the test certificate from your certificate store, here is some handy PowerShell to do the job. Just change [YOUR SERVICENAME] to match your cloud service name. This will run the script in “What If” mode (not making any changes) so you can see the certificates it will remove. To have it actually remove the certificates remove the –Whatif switch and run it again.Get-ChildItem -Path Cert:LocalMachineMy -Recurse | Where-Object { $_.Subject -eq &quot;CN=[YOUR SERVICENAME].cloudapp.net&quot; } | Remove-Item -Verbose -WhatIfThis does not remove the self-signed CA certificate. Again, you probably want to leave the self-signed CA on your machine so you can run the script again for another cloud service project and generate new test certificates as needed. The script will re-use your self-signed CA certificate if it finds one.My full solution (and this new script) are available here.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Hosting a WCF Service in an Azure Web Role with HTTP &amp; TCP Endpoints", "url": "/2013/08/30/hosting-a-wcf-service-in-an-azure-web-role-with-http-tcp-endpoints/", "categories": "", "tags": "", "date": "2013-08-30 16:06:04 -0500", "snippet": "IntroductionSince IIS 7.5, developers have been able to take advantage of the rich hosting environment IIS provides for their WCF services, and do so using non-http protocols, such as net.tcp, net.pipe (for x-process calls), and net.msmq (for queuing). For many, it was this release that removed the ball-and-chain of writing and maintaining a self-hosted WCF service host just to leverage non-http protocols. Extending this concept to the cloud and hosting your WCF service in an Azure Web Role allows you to continue to benefit from the great hosting features in IIS. It is a very simple task, if you just stick with the defaults (http). Add support for tcp and things can start to get a bit tricky.In this post, I’m going to walk through the steps to host a WCF service in an Azure Web Role that supports both http and tcp protocols. I will also share my approach to web service development along the way.Create a WCF Service LibraryI prefer to start just about any WCF service using the WCF Service Library template.There are a few reasons for this… The project is configured to leverage the WCFServiceHost. This frees me from having to provide a host process of my own. If you’ve done WCF development for a while, you probably remember the days of the console host application you had to write just so you could test your service. Sadly, many of those samples are still around today. I don’t like these because they just take away from what is strategic, which is, developing the service! The project is configured to launch the WCFTestClient. This frees me from having to provide a client application of my own just to test the functionality of my service. Again, not strategic to what I’m trying to do. Finally, and this is the most important, the project makes absolutely no assumption about how the service will be hosted. I can reference this DLL from any host process I choose (IIS, a Windows Service, COM+, etc.) and that host can be on-premise or in the cloud.As a result, I spend my time focusing on just the implementation of my service. Once I’m satisfied with my implementation, I can shift my attention to the tasks needed to host it.For this blog post, I used the WCF Service Library template to create a calculator service. I’m not going to go into the details here because this is a very trivial step and not the focus of this post. It literally took about 60 seconds and I was up and running. If you’re reading this I’m assuming you know the basics of creating a WCF service. My full solution is also available at the end of this post for download.Create a Cloud Service using the WCF Service Web RoleMy calculator service needs a host process and since I’m going to host this service in Azure, I used the WCF Service Web Role template for my new Cloud Service project.Using this template assumes you will be using IIS as your host. This is recommended because of all the features that IIS makes available to you out of the box. Again, what is strategic for me is not creating a service host but instead making my service (from the previous section) available to clients or other applications. In other words, I’m simply leveraging as many features of the platform as possible so I don’t have to maintain unnecessary code.When Visual Studio creates the WCF Service Web Role project, it provides a fully functional service you can use to start your development from. Since I already have a service though I deleted these files from the project.Even if I were planning to start my development from this template, I always delete these files because I don’t like the physical .svc file that has just one line of markup in it. This .svc file (the extension actually) is significant though because it is what IIS uses to activate the service. So, removing it means I need to handle this another way, which I’ll get to shortly.The main issue I have with the .svc file though is this. If you decide to add more services to your Web Role, then you would end up with more .svc files in your virtual directory. Imagine what your virtual directory in IIS would look like if you had hundreds of services. You guessed it – you would have hundreds of .svc files (each with just one line of markup in it).There is a better way to handle activation of your service that will free your virtual directory from the clutter of .svc files. And if your going to run your service on-premise or in an IaaS Virtual Machine in Azure, your IT Professional will appreciate not having all these files in the directory too!To achieve this, I just need to do three simple things: Add a project reference (not a service reference) to my WebRole referencing the WCF Service Library from the previous section. Build the solution so the DLL for my WCF Service Library (referenced in #1) is in the output directory of my Web Role. Add a serviceActivation to the configuration (web.config) for the Web Role. The Relative Address can be anything, just as long as it ends with “.svc”. Again, this is because IIS uses this to broker in the correct handlers for WCF. The Service is the service type from the WCF Service Library. That is, the class that implements the service contract.That’s it. Now, I can build and publish the solution to Azure. To interact with the service, I prefer to continue using the WCFTestClient that I used during the development of the service. In fact, I just keep it on my task bar for easy access to just about any service I want to test. Here is where it is located on a Visual Studio 2013 installation.To consume the service, open the service using the public address of the cloud service and append the relative address from the serviceActivation configured earlier and you’re up and running.Provision the Virtual Machine to Support TCPWhen you host your service in IIS, your service is accessed through the base addresses the environment has provisioned . By default, there is only one base address, and it uses http . So, the base address for your service is http://[yourservicename].cloudapp.net/calc.svc . If you want to support tcp, then you need to provision the environment so that IIS offers a base address for it. When you do, you will automatically have a base address for that protocol, such as net.tcp://[yourservicename].cloudapp.net/calc.svc .To provision the server so that IIS support TCP requires some server configuration changes. There is plenty of documentation showing how to install and configure non-http activation on a server. But, when you’re deploying your service to Azure, you need to automate this.To automate this step, I added a Startup folder to my Web Role project with two files. A Startup.cmd that I’ll run as a startup task when the virtual machine is being provisioned. A PowerShell script, RoleStart.ps1, that I’ll run from the OnStart method in my Web Role.The reason for not putting all this in a single startup task is explained here .Startup.cmdPowershell.exe -command &quot;Set-ExecutionPolicy -ExecutionPolicy Unrestricted&quot; &amp;gt;&amp;gt; startup_log.txtThis will get executed as a startup task. All it does is set the execution policy to Unrestricted so I can run the RoleStart.ps1 later in the OnStart method.RoleStartup.ps1# Install TCP Activation to support netTcpBinding.Import-Module ServerManagerInstall-WindowsFeature -Name AS-TCP-Activation # Add TCP to site bindings.# Assume the fist website is for my WebRole.Import-Module WebAdministration$site = Get-WebSite | Select-Object -First(1)Set-Alias appcmd $env:windir\\system32\\inetsrv\\appcmd.exeappcmd set site &quot;&quot;$site.Name&quot;&quot; &quot;-+bindings.[protocol=&#39;net.tcp&#39;,bindingInformation=&#39;808:*&#39;]&quot; # Enable net.tcp on protocol.$appName = $site.Name + &quot;/&quot;appcmd set app $appName &quot;/enabledProtocols:http,net.tcp&quot;This script provisions the server to install the TCP Activation Windows feature, add TCP site bindings in IIS, and then enable net.tcp on the site. It will get executed from the OnStart method when the Web Role is starting which I’ll get to shortly.A word of caution regarding these two files. They need to be saved using codepage 65001 instead of the default 1252. Otherwise, they will fail because of some encoding discrepancies at the beginning of the file. To do this, select File –&amp;gt; Advanced Save Options if/when you edit these in Visual Studio.The next step is to edit the service definition (.csdef) file. Here, I’ll make two changes:Set the executionContext in for the Runtime to elevated. This is because I’m going to be invoking the RoleStart.ps1 PowerShell script in my OnStart method. Since that script will be making server configuration changes, the Web Role needs to be elevated for that to be successful.Add a Startup Task to invoke the Startup.cmd during the virtual machine provisioning process.WebRole.csFinally, the last part of the provisioning process is to invoke the RoleStart.ps1 PowerShell script in the OnStart method. Here is the snippet of code that starts a new PowerShell process and invokes the script.var startInfo = new ProcessStartInfo(){ FileName = &quot;powershell.exe&quot;, Arguments = @&quot;..\\Startup\\RoleStart.ps1&quot;, RedirectStandardOutput = true, UseShellExecute = false};var writer = new StreamWriter(&quot;out.txt&quot;);var process = Process.Start(startInfo);process.WaitForExit();writer.Write(process.StandardOutput.ReadToEnd());writer.Close();Configure the WCF Service Web Role to use TCPThere are just a few configuration changes that need to be added to the WCF Service to round this out. The first is to add the useRequestHeadersForMetadataAddress Service behavior and defaultPort property setting for net.tcp. The port is 808, which is the port used in the RoleStart.ps1 script to configure site bindings for net.tcp. This will make your service work behind the Azure load balancer.http://support.microsoft.com/kb/971842Next, I’m going to disable security on the netTcpBinding. By default, it uses Transport Security. Since my http endpoint is also not secure (yet), I’m just disabling it for tcp as well. In a follow-up post, I’ll cover the steps to secure these two endpoints.With the netTcpBinding configuration taken care of, the next step is to wire this up to the ProtocolMapping for net.tcp.Add a TCP Endpoint to the Cloud ServiceThe last part is to add a new input endpoint to the cloud service project.Now, build and publish the solution and you will have a WCF Service running in a Web Role accessible through either http or tcp endpoints.Test the Cloud ServiceUsing the WCFTestClient again to test the service, I now have two endpoints; one using http and another using tcp.ConclusionIn this post I showed how to host a WCF Service in an Azure Web Role with support for http and tcp protocols. I also covered some of my personal best practices on how to build your WCF service to accommodate various hosting options.As always, my full source code is available here.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Visual Studio and GitHub: The Basics of Working with Existing Repositories", "url": "/2013/07/27/visual-studio-and-gitub-the-basics-of-working-with-existing-repositories-2/", "categories": "", "tags": "", "date": "2013-07-27 16:06:04 -0500", "snippet": "In this post, I’m going to cover some basics for using Visual Studio’s Git support with Github repositories. Specifically, I’ll cover Pre-Requisites (getting setup). Forking a Repository Cloning a Repository Committing Changes to a Local Repository Pushing Changes to a Remote Repository Submitting Pull RequestsPre-RequisitesI’m going to be use Visual Studio 2013 Preview because it’s new and it comes pre-packaged with Git support. So, no additional downloads are needed. If you want to do the things I’m going to show in this post using Visual Studio 2012, then you will need the Visual Studio Tools for Git.The other pre-requisite is a Github account .Forking a RepositorySuppose you have found a Github repository that interests you and you want to make some contributions. You could ask the repository owner to give you access to the repository so you can make changes. That’s likely to be met with a firm “NO” if you get any response at all. Or, you could essentially copy the repository to a repository under your Github account where you can make all the changes you want to your own copy. That’s what forking a repository basically does. It sets up a copy in your Github account that you can work against. It also provides a way to link back to the repository you forked from so you can offer your great work back to the original repository owners for consideration (see Submitting Pull Requests below).Github provides a repository called “Spoon-Knife” that is available for demonstration purposes, which I will use to demonstrate the concepts in this post. Assume I want to make some contributions to this repository. Once I’m logged into Github and have navigated to the “Spoon-Knife” repository, I can click the Fork button to fork the repository.After a few seconds, I have a copy of this repository sitting in my Github account. Notice the link back to the repository I forked from at the top. Also, notice the HTTPS clone URL in the lower-right corner. This will be needed for the next section.Cloning a RepositoryTo clone a repository is to get a local copy of it on your development machine. This will allow you to commit changes to the local repository in the absence of any connectivity to the remote repository on Github. This is a key difference between distributed and centralized version control systems.From Visual Studio’s Team Explorer, clicking on the Connect button will show the available repositories you can connect to.Since I’m working from a fresh install, I don’t have any repositories yet. I’ll clone the remote repository by clicking the Clone link.Adding the HTTPS clone URL from the remote repository to the address field and clicking the Clone button will bring a copy of the repository local.Now, in the future, when I open Team Explorer, I’ll see this repository in my Local Git Repositories.It’s not really complicated what’s happening here. If you open Windows Explorer and navigate to the folder shown above, you can see it’s simply a copy of the remote repository on Github. If you look closely, you will also see a “.git” folder. That’s where Git stores information about the local repository as you work with it. You should leave it alone. However, I wanted to call it out as an artifact of cloning the remote repository.Committing Changes to a Local RepositoryNow I’m ready to start making some contributions to this repository.Suppose I want to add a new project. I’ll start by creating a new project and setting the Location to the path of my local repository.Clicking OK will create the project in the folder of my local repository. Switching over to Team Explorer and clicking on Changes …… will show all the folders and files for the project I just created as Included Changes. This doesn’t mean that the changes have been committed yet. It just means that Visual Studio and Git are aware of them and tracking the changes.To commit the changes I need to enter a commit message and click the Commit button.Now this is committed to my local repository. Note, this is not committed to my remote repository on Github. I’ll get to that in the next section.It is not necessary that changes/contributions come just from Visual Studio. For example, suppose I want to add some user instructions for the HelloWorld project. I can simply create a file in the folder of the local repository.Now, going back to Team Explorer, I can see in the Untracked Files section that Visual Studio and Git are aware of this new file, but not tracking it. To start tracking this file, right-click on the file and select Add.Now, this file is being tracked as a change to my local repository. However, it has still not been committed to the repository.To commit this change, I’ll add a comment and click the Commit button just as I demonstrated earlier.So far, I’ve made two commits to my local repository. I can view these commits and drill into the details for each by clicking on the Commits link.My two commits are in the Outgoing Commits section. From here, I can double-click into these commits to review the details for each.Before I move onto the next section, I want to revisit the Untracked Files from earlier.Here, the tools are rightfully assuming you don’t want to add these files to version control. Generally, you only want source code in the repository and not the compiled artifacts that land in directories like bin\\Debug and obj\\Debug. Nor do you want your Visual Studio user options file committed to the repository. You can tell Git to ignore these by right-clicking on the folder and selecting ignore. This will result in a .gitignore file being added to your local repository so these will be ignored going forward.You also have the option of deleting the Untracked Files. This will physically delete the files from the repository folder on disk. However, next time you build the application, they will be recreated and show back up as Untracked Files.Pushing Changes to a Remote RepositoryEventually you will need to get your changes in the local repository up to your remote repository on Github. That’s what a push does. When you initiate a push, your local repository is checked against the remote repository to see if any changes were added that you don’t have locally. If so, then you will need to resolve those conflicts locally before you can push to the remote repository. Since my remote repository is mine and only mine, I won’t have any conflicts. But, imagine if you were on a team with other developers, all committing changes to the same remote repository. In this case, your local repository is likely to get out of sync with the remote repository. In which case, you will need to resolve these conflicts before pushing to the remote repository.Using Team Explorer, you can Fetch or Pull commits from the remote repository. A Fetch will return back a list of commits without applying the changes to your local repository. This gives you an opportunity to review changes from other users before proceeding. A Pull will apply the changes in the remote repository to your local repository, potentially prompting you to resolve any conflicts.Once your local repository is in sync with the remote repository, clicking the Push link under Outgoing Commits will push your commits up to the remote repository. When the push is completed, Incoming Commits and Outgoing Commits will be empty. In other words, your local repository and remote repository are in sync.Now, if I go back to the remote repository on Github, I can see these commits have been applied.Near the top of the page is a commits link. Drilling into this will show details for the commits. In the Commits page, you can further drill into the changes to see what files were added/modified.Submitting Pull RequestsUp until now, all the changes I’ve made have been to my local and remote repository that I forked from “octocat/Spoon-Knife” repository. Suppose now I want to offer up my great work to the owners of the “octocat/Spoon-Knife” repository. The way this is done is through a Pull Request. A pull request is a request to the original repository owner to pull my changes into the original repository. If accepted, then my changes become part of the original repository I forked from. Later, if I or someone else were to fork the “octocat/Spoon-Knife” repository, my contributions would be there.To submit a pull request, I’ll click the Pull Requests link on the right-hand side of the screen.This takes me to a screen where I can view all my pull requests. Since I’ve not submitted one yet, I don’t have any to view. On this screen is a New pull request button I can click on to submit a new one.This takes me to a screen that shows all the details for the pull request I’m about to submit.Once I confirm this is what I want to do, then I can click on the Click to create a pull request for this comparison link. This will take me to a screen where I can give a title and some brief comments for the pull request.Clicking the Send pull request button will submit my pull request and take me to the original “octocat/Spoon-Knife” repository. That’s it – done! What generally will follow from here is some back-and-forth collaboration, code reviews, changes, bug fixes, etc.Considerations Before Submitting Pull RequestsIf you’re a new contributor to a repository, then you may want to spend some time reviewing the pull requests of the original repository you forked from. You can do this by going to the repository on Github.On the right-hand side of the screen is a Pull Requests link. Clicking this will take me to a screen where I can see the 583 other pull requests that people have submitted. In a real development repository, this is a good way to see who’s actively making contributions and also to observe things such as the kinds of pull requests the repository owners are accepting. This gives some insight into coding standards that may be expected. For example, if you UPPERCASE all your variable names while the repository owners prefer camelCase, then you can expect comments from the owners to change your casing before they will accept it.ConclusionThere are many other things I’ve not touched on in this blog post, such as branching, reverting commits, etc. Perhaps I’ll talk about that in another post. This post is long enough as it is though and covers the basics – which was my goal.There is plenty of good documentation to learn about Git, Github, and key differences between centralized and distributed version control systems. Here is a handy link that links to some additional reading on the subject.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Implementing Polyglot Persistence – Part 5", "url": "/2013/07/14/implementing-polyglot-persistence-part-5/", "categories": "", "tags": "", "date": "2013-07-14 16:06:04 -0500", "snippet": "In this post I will complete the implementation of my MongoDB repository by implementing the Review operations previously defined in my IProductRepository interface. This will also be the post where I visit some of the Create, Update, and Delete API’s of the MongoDB C# Drivers. Everything up to this point has been variations of Read API’s.// Review OperationsReview GetReview(string reviewId);List&amp;lt;Review&amp;gt; GetReviews(string prodId, int pageIndex, int pageSize);string AddReview(string prodId, Review review);bool UpdateReview(string reviewId, Review review);void DeleteReview(string reviewId);However, before I get into the implementation of these methods, I need to revisit my MongoHelper class to support a new collection in MongoDB called “Reviews”, which is the name of the collection where I’ll be storing product reviews.Update MongoHelper to Support my “Reviews” Collection in MongoDBIn part 3 I first introduced my MongoHelper class and talked about class maps. For the reviews collection, I’ll need a similar class map, but this time only to indicate that the “Id” field in my Review class will be represented as an ObjectId. To achieve this, I added this small piece of code to my RegisterClassMaps method.BsonClassMap.RegisterClassMap&amp;lt;Review&amp;gt;(cm =&amp;gt;{ cm.AutoMap(); cm.IdMemberMap.SetRepresentation(BsonType.ObjectId);});While I was in this file, I also added a new read-only property to return a reference to the Reviews collection. The reviewsCollectionName is a string set from an AppSetting in web.config (just like I did for the products collections).public static MongoCollection&amp;lt;Review&amp;gt; ReviewsCollection { get { return mongoDatabase.GetCollection&amp;lt;Review&amp;gt;(reviewsCollectionName); }}With those few changes in place, I can now proceed to the implementation of the Review operations.The GetReview ImplementationThis method is among the simplest in all the methods I’ll cover in this post and the use case is very straight forward – you just want to retrieve a single review for a product. The way I’ve setup my controller, a review is accessed by including the “Id” of the review in the URL. For example,Recall that my class map defines the Id property as an ObjectId. So, to retrieve this Id, I just need to look for it in the collection using the FindOneById method.public ProductService.Model.Review GetReview(string reviewId){ ObjectId objectId; if (ObjectId.TryParse(reviewId, out objectId)) return MongoHelper.ReviewsCollection.FindOneById(objectId); else return null;}The GetReviews ImplementationThis method returns a list of reviews for a given product Id. Since there could be a large number of reviews, this method also supports some basic pagination. But, to simply return all reviews for a product Product Id, this URL would work.In this implementation, I’m building a query that will run on the server, searching the Reviews collection for all reviews with a ProdId matching the product Id passed in on the URL. Then, I simply pass that query into the Find method with the necessary pagination (skip, take).public List&amp;lt;ProductService.Model.Review&amp;gt; GetReviews(string prodId, int pageIndex, int pageSize){ ObjectId objectId; if (ObjectId.TryParse(prodId, out objectId)) { var query = Query&amp;lt;Review&amp;gt;.EQ(r =&amp;gt; r.ProdId, objectId.ToString()); return MongoHelper.ReviewsCollection.Find(query). Skip(pageIndex * pageSize).Take(pageSize).ToList&amp;lt;Review&amp;gt;(); } else return null;}The AddReview ImplementationThis is the first method where we actually add something to a collection. The use case here is a user has purchased a product and wants to write a review for it. For this, you need to POST data to the server using this URL and JSON in the Request Body of the message.You may notice in the JSON that I didn’t specify things like ProdId, Id, and ReviewDate. All of which are properties of the Review class. I chose to set these explicitly in the method. For ProdId, I use the product Id from the URL. For ReviewDate, I just timestamp the document with the current time. And, for Id, I’m generating an new ObjectId before inserting.public string AddReview(string prodId, ProductService.Model.Review review){ review.Id = ObjectId.GenerateNewId().ToString(); review.ProdId = prodId; review.ReviewDate = DateTime.UtcNow; MongoHelper.ReviewsCollection.Insert(review); return review.Id;}The UpdateReview ImplementationTo update an existing review, this translates to a PUT verb on my controller with similar JSON in the Request Body for the data I want to update.In the implementation, I’m applying updates to the properties of the existing review. I’m creating a query to find the review Id passed in. Next, I’m telling MongoDB to update the ReviewDate, Rating, and Comments fields in the review. The Combine method is particularly handy when you need to make updates to multiple fields in the document.public bool UpdateReview(string reviewId, ProductService.Model.Review review){ ObjectId objectId; if (ObjectId.TryParse(reviewId, out objectId)) { var query = Query&amp;lt;Review&amp;gt;.EQ(r =&amp;gt; r.Id, objectId.ToString()); var updates = Update&amp;lt;Review&amp;gt;.Combine( Update&amp;lt;Review&amp;gt;.Set(r =&amp;gt; r.ReviewDate, DateTime.UtcNow), Update&amp;lt;Review&amp;gt;.Set(r =&amp;gt; r.Rating, review.Rating), Update&amp;lt;Review&amp;gt;.Set(r =&amp;gt; r.Comments, review.Comments)); var result = MongoHelper.ReviewsCollection.Update(query, updates); return result.Ok; } else return false;}The DeleteReview ImplementationFinally, to remove a review document from the reviews collection, the DELETE verb is set with the URL indicating the id of the review to delete.Once the reviewId is parsed into an ObjectId, it’s a simple matter of finding the document and calling Remove.public void DeleteReview(string reviewId){ ObjectId objectId; if (ObjectId.TryParse(reviewId, out objectId)) { var query = Query&amp;lt;Review&amp;gt;.EQ(r =&amp;gt; r.Id, objectId.ToString()); MongoHelper.ReviewsCollection.Remove(query); }}ConclusionThis wraps up my journey through the document database category and MongoDB-as-a-servicefrom Mongolab. The service from Mongolab is very powerful and easy to use. The C# Drivers are a gem and the more I use them the more I appreciate everything they do.My solution is available here for anyone interested in reviewing the full end-to-end implementation.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Implementing Polyglot Persistence – Part 4", "url": "/2013/06/30/implementing-polyglot-persistence-part-4/", "categories": "", "tags": "", "date": "2013-06-30 16:06:04 -0500", "snippet": "This post continues the journey of implementing a MongoDB repository, this time focusing on the search implementation.Implementing the Product Repository (continued…)I structured my API for search to allow me to pass in the property I want to search on and a search value for that property. The goal here wasn’t to be a rich searching engine but instead to give some common scenarios and see how they could be implemented. A few examples of search URL’s I wanted to support are…// Find products whose Name field contains “sql” GET /api/product/search?property=name&amp;amp;value=sql// Find products whose Price field equals 19.99GET /api/product/search?property=price&amp;amp;value=19.99// Find products in the &quot;clothing&quot; categoryGET /api/product/search?property=categories&amp;amp;value=clothingI also wanted to be able to do searches on properties/fields that are unknown to the base Product model.// Find products with a “Fabric” property equal to “cotton” GET /api/product/search?property=Fabric&amp;amp;value=cotton// Find products with a &quot;Voltage&quot; property containing &quot;18&quot;GET /api/product/search?property=Voltage&amp;amp;value=18Finally, I want all the searches to be case-insensitive, matching substrings (not just full string matches).What I ended up with was a generic, yet powerful, search method. When I first started this, I thought I would have a search method using LINQ queries against my base Product model and another method for everything else. And that was what my first pass at this actually was. More on that later. However, after experimenting more with the API’s, I was able to get all my search requirements down to one short method that I’m mostly satisfied with.When searching on properties that are in in my base Product model, I was able to be very precise about the search implementation. For example, since I know about the properties in my model, I can correct casing of the property name before issuing the query. Or, if the property is a complex type (array, nested document, etc.), then I can adjust the query for that particular type.As I expanded my search capabilities beyond my model to any property that any product might have, things got a little thorny. For example, if I want to search on the “Voltage” property (a property not in my model), then the URL needs to specify the exact casing of that property (“Voltage” not “voltage”). And, who’s to say that some product in the collection may actually have a “voltage” property now, or in the future. In which case, a search on “Voltage” and a search on “voltage” would yield two different results. Not good! Another issue is searching properties unknown in my model that have complex structures. A lack of a model for these additional properties makes searching on them extremely challenging. In my implementation, I treat anything I don’t know about as a simple string search.The point of all this is to support a point I made in an earlier post. Which is, just because you can store any random document into a collection, doesn’t mean you always should. Properties in your documents should be consistently named (and cased) throughout the collection. Try to think ahead about how you intend to use and query the documents in the collection and structure your model (schema) to support those needs. That was the basis for my Product model (although very small). It enables me to have some consistency throughout the collection. Without it, searching the collection would otherwise have been very challenging.Now, on to the code…The Search ImplementationIn the constructor for my repository, I added this line of code to reflect over my base Product model, giving me a list of PropertyInfo objects that I can leverage in my search method. This is just the way I chose to do this. If you use the LINQ query support then this is not necessary.// Get a list of properties I know about (from the model).knownProperties = typeof(Product).GetProperties().ToList();My search method starts off like this, taking advantage of the knownProperties list. All I’m doing here is properly casing the srchProperty when the property specified is one I know about.// If this is a property/field I know about, then I can be more // precise about how I construct the query.var knownProperty = knownProperties.Find(p =&amp;gt; p.Name.ToLower() == srchProperty.ToLower());// Set the name of the property to exactly match casing.srchProperty = (knownProperty != null) ? knownProperty.Name : srchProperty;Next, I setup my default query. This query does a simple string search on the srchProperty to see if the value contains the srchValue. It also ignores casing inconsistencies.// Query for searching a property&#39;s value. // This is the default query unless otherwise specified.IMongoQuery query = Query.Matches( srchProperty, new BsonRegularExpression( new Regex(srchValue, RegexOptions.IgnoreCase)));If the search is on a property that is in my model, then I can leverage what I know about the property to make the search more intelligent. My Product model contains 3 different types of properties (string, double, and List&amp;lt;string&amp;gt;). Since my default query (above) already handles string, I just need to handle the other two types in my model.The first type I handle is the double, which is what my Product.Price is. Since the parameter is passed in from the URL as a string (again, that’s my default), I simply convert it to a double and overwrite query with a precise query for double.Next is the List&amp;lt;string&amp;gt; type, which is what Product.Categories is. These 2 lines of code overwrite the query to search the list of categories for each product, matching any product whose category(s) contain the srchValue passed in. In other words, I can search for “book”, “Book”, or even “OOK” and I’ll get back the same list of products. Again, having a model you can reference makes implementing the query very easy and more powerful.if (knownProperty != null){ if (knownProperty.PropertyType == typeof(double)) { double srchDouble; if (double.TryParse(srchValue, out srchDouble)) query = Query.EQ(knownProperty.Name, srchDouble); } else if (knownProperty.PropertyType == typeof(List&amp;lt;string&amp;gt;)) { var rx = new Regex(srchValue, RegexOptions.IgnoreCase); query = Query.All(knownProperty.Name, new List&amp;lt;BsonValue&amp;gt;() { BsonValue.Create(rx) }); }}For searches on properties I don’t know about, I default to the string query (above). However, in this case, I need to tell MongoDB to also check if the property exists because now I’m searching on properties that not all documents in the collection will have.// A query to check if the property exists in the collection.var qPropExists = Query.Exists(srchProperty);// Pull the two query conditions together into one query.query = Query.And(qPropExists, query);Finally, I issue the query and return the results.return MongoHelper.ProductsCollection.FindAs&amp;lt;Product&amp;gt;(query). Skip(pageIndex * pageSize).Take(pageSize).ToList&amp;lt;Product&amp;gt;();Query Builder or LINQWhy did I choose this approach over using LINQ for queries against my Product model? I wanted to support queries against my model and queries on properties that are not part of my model in as few lines of code as possible. By taking this approach, I was able to have just one line issue the query to MongoDB. The only difference is how the query is constructed.There was also a “gotcha” I ran into using LINQ that I was able to easily resolve using the Query builder approach. Recall that I wanted my searches to be case-insensitive. At first I thought this would be very easy using LINQ. After all, this line of code compiles just fine!productQuery = MongoHelper.ProductsCollection.AsQueryable&amp;lt;Product&amp;gt;(). Where(p =&amp;gt; p.Categories.Contains(srchValue, StringComparer.CurrentCultureIgnoreCase));However, when I ran it, I got this runtime exception.In fact, this condition (and potentially others) is documented here.and here…Bummer! This was the only condition like this I discovered. Otherwise, the LINQ support was fantastic.Anyway, there were a couple of easy ways to resolve this. I chose to do so using the Query builder approach, incorporating a simple regular expression into my query (see above). Not only was I able to achieve a case-insensitive search on the Categories property, but I was also able to easily support substrings of the category being searched for – something my LINQ query wouldn’t have done even if it was supported.ConclusionUsing Fiddler (or a browser since these are GET operations), it is easy to search the products catalog. My full implementation is available here for anyone interested in trying it out.MongoDB supports very powerful queries across collections, regardless of the schema (or lack of) from one document to the next. This post showed how you can use the MongoDB C# drivers to query the database. More importantly, I hope it illustrates how having some degree of schema in your application model will help query the collection effectively.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Implementing Polyglot Persistence – Part 3", "url": "/2013/06/25/implementing-polyglot-persistence-part-3/", "categories": "", "tags": "", "date": "2013-06-25 16:06:04 -0500", "snippet": "In this post I will start creating a Mongolab repository to perform CRUD operations against the MongoDB database I created in Part 2. As a starting point, I will use the solution that I introduced in Part 1, adding a new repository for Mongolab.Get the MongoDB Client LibraryTo start, I’ll need some client libraries to program against my MongoDB. I’ll be using the C# drivers that 10gen supports, which can be found here. This is an outstanding resource for documentation and samples on how to use the drivers.Using the “Manage Nuget Packages…” feature in Visual Studio 2012 and searching for “mongocsharpdriver” makes installing the drivers for my Mongolab project a one-click task.Handling Multiple Product SchemasMongoDB is a “schemaless” database. While the database may not be enforcing schema, it’s still helpful to have some basic schema in place (in the application). Recall from Part 1 that I created a very basic model (schema) for products and reviews in ProductService.Model. However, in Part 2 where I created my database, I was not restricted to the model and MongoDB was perfectly happy to take whatever I gave it.In MongoDB, you can reference the collection as a generic collection of BsonDocument types. Some examples of how to use this document type are in the C# Driver Tutorial. This exhibits a great deal of flexibility when working with the collection. With that flexibility, however, comes challenges in how you’re able to query the collection. For example, if you’re not able to assume any model (or schema) at all, then field names for the entities obviously can be different or non-existent. Even the casing of field names starts to become an issue (“Price” does not equal “price”).Having a basic model for products and insuring items in the collection adhere to that basic model I’ve found makes things much simpler. Yet, I still have the power of schema flexibility in the database. To handle the various properties that a product might have, I extended my ProductService.Model.Product to include a new property called “ExtraElements”. I extended this internal to my repository since this is a capability the client library supports and not relevant to other potential repository implementations using the model.Now, anytime I refer to the “products” collection in my database, I do so using this new *ProductDocument** class as the type. I wrote a little helper class to return an instance of the collection for me using this type.public static MongoCollection&amp;lt;ProductDocument&amp;gt; ProductsCollection { get { return mongoDatabase.GetCollection&amp;lt;ProductDocument&amp;gt;(productsCollectionName); }}Because I have two classes representing products (Product and ProductDocument), I need to give some hints to the client library so it can properly serialize these. I also need to tell it how to handle Id’s (more on that later). This is accomplished using the BsonClassMap, as shown here.public static void RegisterClassMaps(){ BsonClassMap.RegisterClassMap&amp;lt;Product&amp;gt;(cm =&amp;gt; { cm.AutoMap(); cm.SetIgnoreExtraElements(true); cm.IdMemberMap.SetRepresentation(BsonType.ObjectId); }); BsonClassMap.RegisterClassMap&amp;lt;ProductDocument&amp;gt;(cm =&amp;gt; { cm.AutoMap(); cm.SetExtraElementsMember(cm.GetMemberMap(c =&amp;gt; c.ExtraElements)); });}In the first class map, I’m telling it that when serializing a product as a ProductService.Model.Product, ignore any extra properties that may exist for the product. I’m also telling it that the “Id” field should be represented as an ObjectId.In the second class map, I’m telling it that when serializing a product as a ProductService.Repository.Mongolab.ProductDocument, store any extra data it finds for the product in the “ExtraElements” property I added above.The Id FieldWhen I inserted the JSON documents into my products collection, a “_id” field was created automatically for each product. I could have generated the field myself and populated it differently, but I chose to just let the database do it for me when I added these documents in Part 2.The “_id” field is actually an ObjectId, which is a structure of 12 bytes (no less and no more) and not to be confused for a Guid (although similar). Its value takes into account things like time, process id, machine and an incrementing counter.My ProductService.Model.Product already has an “Id” field and the client library automatically maps any field named “Id” to the “_id” field in the database. So, my “Id” is already handled and queries for products by Id will just work.Implementing the RepositoryWith everything described above in place, the implementation of the repository methods is very simple. Here I’ll show a few of the methods and the output using Fiddler to generate requests.The GetProduct ImplementationThis method returns the base Product representation of a product. Recall from above that my collection is actually a collection of ProductDocument. Using the FindOneByIdAs&amp;lt;&amp;gt; method, I can tell Mongo to return the document as a Product instance (ie: no product details).public ProductService.Model.Product GetProduct(string prodId){ var id = ObjectId.Parse(prodId); if (id != null) return MongoHelper.ProductsCollection.FindOneByIdAs&amp;lt;Product&amp;gt;(id); else return null;}The GetProductDetails ImplementationThis will return the product and any additional fields stored for the product.One side effect of this is the “Extra Elements” field wrapping the additional product details. Ideally, I’d like for the additional product details to appear as they do in my MongoDB without being wrapped by the “Extra Elements” field. Perhaps in a future post I’ll fix this with some custom serialization. For now, I’m happy to accept this little nuanceThe GetCategories ImplementationThe use case for this method is one where you need a list of all available categories of products in the collection. For example, to populate a list of categories on a web page so users can browse products by category.public List&amp;lt;string&amp;gt; GetCategories(){ var categories = MongoHelper.ProductsCollection.Distinct&amp;lt;string&amp;gt;(&quot;Categories&quot;); if (categories != null) return categories.ToList(); else return null;}ConclusionI’ll get to the rest of the methods in the repository in Part 4. Things get a little more interesting when implementing search. Although, as you can probably imagine, the client libraries simplify this considerably.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Implementing Polyglot Persistence – Part 2", "url": "/2013/06/14/implementing-polyglot-persistence-part-2/", "categories": "", "tags": "", "date": "2013-06-14 16:06:04 -0500", "snippet": "In part 1 of this series, I created a product catalog repository interface and a simple in-memory repository. In this post I am going to start the implementation of a repository going against a MongoDB database.There are a couple of options to get a MongoDB database setup. You could stand up your own VM(s) using this image from VM Depot provided by Cognosys. It will provide a MongoDB instance running on ubuntu. Use Mongolab, which essentially is MongoDB-as-a-Service.For this implementation, I’m going with the latter.In this post, I’ll share my experience creating the database and populating it with some data.Create a MongoDB DatabaseIf you don’t already have a Mongolab account, you can get one here.To create a database, click the link to create a new database and fill out the page. For example:After a few seconds, I was presented with this page, providing a link to my new database “rickraincommerce“.Populate the DatabaseNow that I have a database I can work with, I need to add some data. Clicking on my database takes me to a page that displays my database connection string. It also has tabs to manage collections (think of tables if you come from a RDBMS world), users, backups, statistics, and tools. A pretty handy page.You can use this web interface to create a collection and add documents to it. You could write a simple little client to add documents. Use the MongoDB shell. There are many ways to skin this cat. As I was playing around with this, I came up with an approach using the Mongolab REST API’s. That’s right! They provide REST API’s too!!!Being a big fan of REST API’s, I chose to try out this approach to populate my database with a “products” collection.Get a Mongolab REST API KeyAll the Mongolab REST API’s require an API key. You can get yours by clicking on your “user” link in the upper-right corner of your Mongolab screen.This will take you to a screen that will show you your API key. There’s also a button to regenerate your key ( a good practice is to roll your keys regularly ). This screen also provides the base URL for the API’s.Get Some Product Data to Add to the DatabaseI already have some test data in this repository I created in Part 1 of this series. So, I just decided to run this solution and open up Fiddler to get the data. All my test products have a product Id between 101 and 108, so this search query against my in-memory repository will give me all my data at once. NOTE: If you want to skip to the next section, a copy of my test data in JSON format is available here.Copy just the JSON content from the response ( don’t copy the headers ) and save this off somewhere ( notepad for example ). A small (optional) change I made to the data is I deleted the “Id” field from each product. It’s perfectly fine to leave it, but Mongolab is going to create a new “_id” for each product anyway when I add it so I’m going to use that for the Id going forward.Add Product Data to MongoDB Using Mongolab REST APIUsing the Mongolab REST API URL, https://api.mongolab.com/api/1/databases?apiKey=&amp;lt;your-api-key&amp;gt;, a quick POST from Fiddler will take care of this.Going back to the Mongolab collections, I see my products collection was created and has 8 documents (my 8 test products) in it. Clicking on the products will drill into the document for each product.Looking at just the first few products I can verify the data was POSTed correctly. I also see the “_id” field that MongoDB created for each of these products. I’ll talk more about this field in Part-3.Is it Really Running in Azure?Pulling the server name from the connection string, I ran nslookup on it just to see how it resolves. Sure enough, it’s got a *.cloudapp.net URL, where all cloud services in Azure exist.ConclusionThat’s it! Now I’ve got a MongoDB instance with some data in it. In Part-3 of this series I’ll start writing the code to interact with it.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Setting up a Neo4j Server running on Ubuntu in Windows Azure", "url": "/2013/05/27/setting-up-a-neo4j-server-running-on-ubuntu-in-windows-azure/", "categories": "", "tags": "", "date": "2013-05-27 16:06:04 -0500", "snippet": "The VM Depot by Microsoft Open Technologies is a great community resource for finding pre-configured images of various operating systems and software that you can deploy on Azure. You can also create and share your own images there.This week I was in need of a graph database running Neo4j. So, I went to the VM Depot and found this image: Neo4j Community 1.8 on Ubuntu 12.04 LTS. I’ve not used any flavor of Unix since college and setting up Neo4j is not something I’ve done before. So, this was pretty much a complete green field experience for me. This blog captures the steps necessary to set this up based on my experience.Import Your Publish-Settings from AzureGet your Publish-Settings data for your subscription imported if you haven’t already. This link will get you going.https://windows.azure.com/download/publishprofile.aspxNext, open a Windows Azure Command Prompt.At the Azure SDK Command Prompt, run the following command to import your publish-settings data.azure account import &amp;lt;publishsettings file&amp;gt;Create the Virtual MachineOpen a browser and go to the VM Depot. A search for “neo” returns a couple of options. I chose this one from Cognosys.Click the Deployment Script link and choose the region you want to create the virtual machine in. This will create the script you need to deploy this image. Replacing the DNS name, user, and password with your preferences should be all you need. I added the subscription parameter to my script since I have multiple subscriptions. If you just have one then you don’t need it. This step only took about 30 seconds.Go into your Azure portal and you should see your new image there. It took a minute or two for mine to finish starting up before it reached the “running” status.Next, add a public endpoint for port 7474. This is explained in the documentation for this image (although easy to miss if you’re not paying close attention).You can give the endpoint any name you like but the protocol needs to be TCP and the port 7474.Now that the virtual machine is up and running, I can connect to it for some final configuration settings.Configure the Virtual Machine / Start Neo4jAt the time of this writing, this image does not automatically start the Neo4j server. Starting the server I found to be a pretty simple task. However, when Windows Azure decides to recycle your virtual machine (and it will), you will have to connect back to your virtual machine and restart Neo4j. To mitigate this disruption that my application will experience when this virtual machine is restarted, I configured it to start Neo4j automatically (see below).PuTTY is the tool I used to SSH into the virtual machine. It can be downloaded from here.Start PuTTY and enter the host name (ie: &amp;lt;vm-dns-name&amp;gt;.cloudapp.net) of the virtual machine in Host Name field. Make sure the Port is set to 22, which is the SSH port of the virtual machine.Click Open to open a session to the virtual machine. For the login name and password, use the user and password specified in the deployment script (above). After successful login, you land at a prompt as shown here.Start the Neo4j server…Configure Virtual Machine to Start Neo4j on Startup…There is a file named rc.local that my research suggests is the place to put commands to run when a Unix server is started. This file is located in the etc directory. I used the vi editor to edit this file.In the vi editor, I added the full path to Neo4j and the start command.That’s it! Close/Exit the SSH session.Test Things OutTo test this setting, I went back to the Windows Azure portal and restarted my virtual machine. Shortly after the machine restarted, I was able to open a browser and access the Neo4j admin portal.Referenceshttp://ss64.com/vi.htmlPlease enable JavaScript to view the comments powered by Disqus." }, { "title": "Implementing Polyglot Persistence – Part 1", "url": "/2013/05/22/implementing-polyglot-persistence-part-1/", "categories": "", "tags": "", "date": "2013-05-22 16:06:04 -0500", "snippet": "NOTE: This post was recovered from archives and unfortunately some of the images were lost. I’ve restored as much as I can. However, due to the age of this post I won’t be re-producing the missing images.In my previous post I talked about what polyglot persistence could look like. In this post I’ll start digging into some of the implementation details. I’ve decided to make this a series of posts, hence the “Part 1” suffix in the title.In this post, I’m going to build out the web service layer for the product catalog service using a repository pattern. There’s plenty written about the repository pattern so I won’t go into those details here. This first repository implementation will be a simple in-memory repository to define the model classes, service interface, and test out the web service layer, In Part 2 I’ll build a repository against a document database. Disclaimer: This is not a feature rich implementation of a product catalog service. It’s an example of some operations you might expect from such a service that provide some interesting use cases for kicking the tires of a document database.The ModelFor product catalog service, I’ve got two classes comprising the model; Product and Review.(missing image #1 here)The Product class defines some common properties that you might expect to see for any product of any category. Note that I’m not making any effort here to further define models for various product categories.(missing image #2 here)The Review class is used to represent a review of a particular product.(missing image #3 here)The Product Service InterfaceFor the web service API, I defined the following operations to interact with the model above. These operations will cover the various CRUD operations, plus give some interesting use cases for querying the repository and supporting basic pagination.(missing image #4 here)The ASP.NET WebApi ControllerThe ASP.NET WebApi Controller supports these operations that rely on a repository implementation to carry out the work.GET api/product/categoriesGET api/product/{prodId}GET api/product/{prodId}/detailsPOST api/product/{prodId}/review/GET api/product/{prodId}/reviewGET api/product/{prodId}/review/{reviewId}PUT api/product/{prodId}/review/{reviewId}DELETE api/product/{prodId}/review/{reviewId}GET api/product/search/category?value=clothingThe Http Routes to support this line-up of operations is shown here.(missing image #5 here)Testing Things OutThe ASP.NET Web Api Controller is currently using an in-memory implementation of the repository described above. It’s quick, easy, and doesn’t require any database configuration to run it. It’s got a few products and reviews prepopulated too so I can begin testing my service layer.(missing image #6 here)Using Fiddler, I can interact easily with the service to make sure things are working as expected.(missing image #9 here)I won’t go through all the operations here. If you want to explore this solution further you can download it from here.Now that this is working I can focus on adding a new repository implementation using a document database, which I’ll do in Part 2.Please enable JavaScript to view the comments powered by Disqus." }, { "title": "Polyglot Persistence in Windows Azure", "url": "/2013/04/16/polyglot-persistence-in-windows-azure/", "categories": "", "tags": "", "date": "2013-04-16 16:06:04 -0500", "snippet": "NOTE: This post was recovered from archives and unfortunately some of the images were lost. I’ve restored as much as I can. However, due to the age of this post I won’t be re-producing the missing images.I recently read NoSQL Distilled by Pramod J. Sadalage and Martin Fowler. In the book, an argument is put forth that we are entering into an era of polyglot persistence, where applications use different storage technologies to handle different storage needs. The de-facto standard of using a relational database management system (RDBMS) for all storage needs is being challenged by some interesting NoSQL alternatives. In this book, discussion is given to five categories of databases; RDBMS, Key-Value, Document, Column, and Graph. I recommend the book if you want to gain a deeper understanding of these database categories. If you just want a quick primer in less than 1 hour, check out this video.So, what could polyglot persistence look like? A canonical example is an online retail application as shown here.(missing image #1 here)In this example, we have a web-front-end (WFE) making use of different database categories for different needs. For example, When a user adds an item to his/her shopping cart, that shopping cart data may be stored using a key-value database. Key-Value databases provide fast access to simple, unstructured structured data. The key could be the user’s e-mail address with the value being individual shopping cart items. Product data can vary greatly from one product to the next. Just drill into the product details of your favorite online retailer and you will likely see different properties for different categories of products. In a RDBMS implementation, you can probably imagine schemas that have columns like “customfield1”, “customfield2”, and so on. Or all-encompassing schemas where each row has a lot of null values in it. Document database don’t enforce schema like a RDBMS does, and therefore a perfect fit for product data. You just store the product information as a document. Recommendation data such as “users who bought X also bought Y” is handled exceptionally well by graph databases. Here, nodes (users) can have any number of relationships (bought X, bought Y) and these relationships can evolve without requiring any schema changes. Furthermore, the nodes and, relationships between nodes, can each have properties associated with them to further enhance node traversal. The ACID characteristics of RDBMS is still a great fit for data such as order data, where the processing of payment and updating of inventory can easily be done in a manner that insures data integrity.Something to observe here is that while we’re making use of different databases for different needs, we’ve also introduced a massive amount of complexity. The WFE needs to know how to do CRUD against four different databases now.As a result of this, another pattern is emerging whereby database implementations are wrapped by web services. Consider now a cloud architecture such as this, where the underlying database (Key-Value, Document, Graph, and RDBMS) is wrapped by a web service and the WFE simply retrieves the URI for these services from its web.config file.This is a good idea when you’re using multiple database technologies as it eliminates direct dependencies to the databases that the WFE would otherwise have to take. With this design, the WFE just consumes the web services using standard web protocols (HTTP, JSON), hiding the complexities of the underlying database. This also offers the web service flexibility to change the database it uses in the future without introducing breaking changes.By deploying this as a Windows Azure Cloud Service, you can begin to take advantage of the elastic scale of cloud computing. For example, the WFE and each underlying web service can be a separate role. Need more instances of the WFE? Fewer instances of the Shopping Cart Service? No problem – scale each as needed.This gives you an idea of what polyglot persistence could look like in a Windows Azure Cloud Service. In future posts, I’ll be exploring some of the implementation details for each of these services.Please enable JavaScript to view the comments powered by Disqus." } ]
